{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath('..//../techminer/'))\n",
    "from docs_normalizer import DocNormalizer\n",
    "import pandas as pd\n",
    "from techminer import RecordsDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf = RecordsDataFrame(\n",
    "    pd.read_json(\n",
    "        'step-07.json', \n",
    "        orient='records', \n",
    "        lines=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_normalizer = DocNormalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 326 stopwords\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DocNormalizer(words_keep=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_normalizer.fit(rdf['Title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop words loaded to clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['itself',\n",
       " \"'re\",\n",
       " 'does',\n",
       " 'full',\n",
       " 'eight',\n",
       " 'although',\n",
       " 'as',\n",
       " 'whether',\n",
       " 'could',\n",
       " 'hereby']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_normalizer.stopwords_[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document cleansing - Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing documents\n"
     ]
    }
   ],
   "source": [
    "docs_normalized = doc_normalizer.transform(rdf['Title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fisrt document before the process of normalization\n",
      "\n",
      "Improving DWT-RNN model via B-spline wavelet multiresolution to forecast a high-frequency time series \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Fisrt document before the process of normalization\\n')\n",
    "print(rdf.loc[0,'Title'], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fisrt document after the process of normalization\n",
      "\n",
      "improve dwtrnn model via bspline wavelet multiresolution to forecast a highfrequency time series \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Fisrt document after the process of normalization\\n')\n",
    "print(docs_normalized[0], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the whole corpus of titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(rdf.loc[:,'Title'].values)\n",
    "vocabulary = [word for doc in corpus for word in doc.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the corpus: 1654\n",
      "Number of words in the vocabulary of the corpus: 579\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of words in the corpus: {len(vocabulary)}')\n",
    "print(f'Number of words in the vocabulary of the corpus: {len(set(vocabulary))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_cleaned = [word for doc in docs_normalized for word in doc.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the corpus: 1652\n",
      "Number of words in the vocabulary of the corpus: 443\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of words in the corpus: {len(vocabulary_cleaned)}')\n",
    "print(f'Number of words in the vocabulary of the corpus: {len(set(vocabulary_cleaned))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction of 0.12% from the total number of words in the corpus\n",
      "Reduction of 23.5% from the uncleaned vocabulary\n"
     ]
    }
   ],
   "source": [
    "print(f'Reduction of {round(1 - len(vocabulary_cleaned)/len(vocabulary),4)*100}% from the total number of words in the corpus')\n",
    "print(f'Reduction of {round(1 - len(set(vocabulary_cleaned))/len(set(vocabulary)),3)*100}% from the uncleaned vocabulary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_normalizer = DocNormalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 326 stopwords\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DocNormalizer(words_keep=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_normalizer.fit(rdf['Abstract'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document cleansing - Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing documents\n"
     ]
    }
   ],
   "source": [
    "docs_normalized = doc_normalizer.transform(rdf['Abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fisrt document before the process of normalization\n",
      "\n",
      "The importance of an interference-less machine learning scheme in time series prediction is crucial, as an oversight can have a negative cumulative effect, especially when predicting many steps ahead of the currently available data. The on-going research on noise elimination in time series forecasting has led to a successful approach of decomposing the data sequence into component trends to identify noise-inducing information. The empirical mode decomposition method separates the time series/signal into a set of intrinsic mode functions ranging from high to low frequencies, which can be summed up to reconstruct the original data. The usual assumption that random noises are only contained in the high-frequency component has been shown not to be the case, as observed in our previous findings. The results from that experiment reveal that noise can be present in a low frequency component, and this motivates the newly-proposed algorithm. Additionally, to prevent the erosion of periodic trends and patterns within the series, we perform the learning of local and global trends separately in a hierarchical manner which succeeds in detecting and eliminating short/long term noise. The algorithm is tested on four datasets from financial market data and physical science data. The simulation results are compared with the conventional and state-of-the-art approaches for time series machine learning, such as the non-linear autoregressive neural network and the long short-term memory recurrent neural network, respectively. Statistically significant performance gains are recorded when the meta-learning algorithm for noise reduction is used in combination with these artificial neural networks. For time series data which cannot be decomposed into meaningful trends, applying the moving average method to create meta-information for guiding the learning process is still better than the traditional approach. Therefore, this new approach is applicable to the forecasting of time series with a low signal to noise ratio, with a potential to scale adequately in a multi-cluster system due to the parallelized nature of the algorithm.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Fisrt document before the process of normalization\\n')\n",
    "print(rdf.iloc[0,0], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fisrt document after the process of normalization\n",
      "\n",
      "the importance of an interferenceless machine learn scheme in time series prediction be crucial as an oversight can have a negative cumulative effect especially when predict many step ahead of the currently available datum the ongoing research on noise elimination in time series forecasting have lead to a successful approach of decompose the data sequence into component trend to identify noiseinduce information the empirical mode decomposition method separate the time series signal into a set of intrinsic mode function range from high to low frequency which can be sum up to reconstruct the original datum the usual assumption that random noise be only contain in the highfrequency component have be show not to be the case as observe in previous finding the result from that experiment reveal that noise can be present in a low frequency component and this motivate the newlyproposed algorithm additionally to prevent the erosion of periodic trend and pattern within the series perform the learning of local and global trend separately in a hierarchical manner which succeed in detect and eliminate short long term noise the algorithm be test on four dataset from financial market datum and physical science datum the simulation result be compare with the conventional and stateoftheart approach for time series machine learn such as the nonlinear autoregressive neural network and the long shortterm memory recurrent neural network respectively statistically significant performance gain be record when the metalearning algorithm for noise reduction be use in combination with these artificial neural network for time series datum which can not be decompose into meaningful trend apply the move average method to create metainformation for guide the learning process be still well than the traditional approach therefore this new approach be applicable to the forecasting of time series with a low signal to noise ratio with a potential to scale adequately in a multicluster system due to the parallelize nature of the algorithm \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Fisrt document after the process of normalization\\n')\n",
    "print(docs_normalized[0], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the whole corpus of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(rdf.iloc[:,0].values)\n",
    "vocabulary = [word for doc in corpus for word in doc.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the corpus: 25473\n",
      "Number of words in the vocabulary of the corpus: 4920\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of words in the corpus: {len(vocabulary)}')\n",
    "print(f'Number of words in the vocabulary of the corpus: {len(set(vocabulary))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_cleaned = [word for doc in docs_normalized for word in doc.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the corpus: 24900\n",
      "Number of words in the vocabulary of the corpus: 2746\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of words in the corpus: {len(vocabulary_cleaned)}')\n",
    "print(f'Number of words in the vocabulary of the corpus: {len(set(vocabulary_cleaned))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction of 2.25% from the total number of words in the corpus\n",
      "Reduction of 44.2% from the uncleaned vocabulary\n"
     ]
    }
   ],
   "source": [
    "print(f'Reduction of {round(1 - len(vocabulary_cleaned)/len(vocabulary),4)*100}% from the total number of words in the corpus')\n",
    "print(f'Reduction of {round(1 - len(set(vocabulary_cleaned))/len(set(vocabulary)),3)*100}% from the uncleaned vocabulary')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
