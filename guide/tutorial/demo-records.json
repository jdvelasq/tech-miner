{"Title":"Towards high-capacity fibre-optic communications at the speed of light in vacuum","keywords (cleaned)":"Algorithmic trading;Experimental demonstrations;Fibre-optic communication;Signal transmission;State-of-the-art technology;Transmission bandwidth;Wavelength division multiplexed;Wide bandwidth;Bandwidth;Data communication systems;Fibers;Glass fibers;Light transmission;Light velocity;Optical fibers;Supercomputers;Vacuum;Wave transmission","Abstract":"Wide-bandwidth signal transmission with low latency is emerging as a key requirement in a number of applications, including the development of future exaflop-scale supercomputers, financial algorithmic trading and cloud computing. Optical fibres provide unsurpassed transmission bandwidth, but light propagates 31% slower in a silica glass fibre than in vacuum, thus compromising latency. Air guidance in hollow-core fibres can reduce fibre latency very significantly. However, state-of-the-art technology cannot achieve the combined values of loss, bandwidth and mode-coupling characteristics required for high-capacity data transmission. Here, we report a fundamentally improved hollow-core photonic-bandgap fibre that provides a record combination of low loss (3.5 dB km -1) and wide bandwidth (160 nm), and use it to transmit 37 \u00d7 40 Gbit s -1 channels at a 1.54 \u03bcs km -1 faster speed than in a conventional fibre. This represents the first experimental demonstration of fibre-based wavelength division multiplexed data transmission at close to (99.7%) the speed of light in vacuum. ","SELECTED":null}
{"Title":"Low-latency trading","keywords (cleaned)":"High-frequency trading;Limit order markets;Liquidity;Market quality;NASDAQ;Order placement strategies","Abstract":"We define low-latency activity as strategies that respond to market events in the millisecond environment, the hallmark of proprietary trading by high-frequency traders though it could include other algorithmic activity as well. We propose a new measure of low-latency activity to investigate the impact of high-frequency trading on the market environment. Our measure is highly correlated with NASDAQ-constructed estimates of high-frequency trading, but it can be computed from widely-available message data. We use this measure to study how low-latency activity affects market quality both during normal market conditions and during a period of declining prices and heightened economic uncertainty. Our analysis suggests that increased low-latency activity improves traditional market quality measures-decreasing spreads, increasing displayed depth in the limit order book, and lowering short-term volatility. Our findings suggest that given the current market structure for U.S. equities, increased low-latency activity need not work to the detriment of long-term investors. ","SELECTED":null}
{"Title":"Rise of the machines: Algorithmic trading in the foreign exchange market","keywords (cleaned)":"foreign exchange|price discovery|information|triangular arbitrage|computers|Algorithmic trading|r|high-frequency data|market quality|speed|arbitrage|R|time series|algorithmic trading|trading|market|strategies|adverse selection|Algorithm|costs|liquidity|algorithm|return|foreign exchange market|edi|efficiency|price efficiency|trade|high-frequency","Abstract":"We study the impact of algorithmic trading (AT) in the foreign exchange market using a long time series of high-frequency data that identify computer-generated trading activity. We find that AT causes an improvement in two measures of price efficiency: the frequency of triangular arbitrage opportunities and the autocorrelation of high-frequency returns. We show that the reduction in arbitrage opportunities is associated primarily with computers taking liquidity. This result is consistent with the view that AT improves informational efficiency by speeding up price discovery, but that it may also impose higher adverse selection costs on slower traders. In contrast, the reduction in the autocorrelation of returns owes more to the algorithmic provision of liquidity. We also find evidence consistent with the strategies of algorithmic traders being highly correlated. This correlation, however, does not appear to cause a degradation in market quality, at least not on average. ","SELECTED":null}
{"Title":"The high-frequency trading arms race: Frequent batch auctions as a market design response","keywords (cleaned)":"competition (economics);Financial markets;market system;price dynamics;Time series;trade","Abstract":"The high-frequency trading arms race is a symptom of flawed market design. Instead of the continuous limit order book market design that is currently predominant, we argue that financial exchanges should use frequent batch auctions: uniform price double auctions conducted, for example, every tenth of a second. That is, time should be treated as discrete instead of continuous, and orders should be processed in a batch auction instead of serially. Our argument has three parts. First, we use millisecond-level direct-feed data from exchanges to document a series of stylized facts about how the continuous market works at high-frequency time horizons: (i) correlations completely break down; which (ii) leads to obvious mechanical arbitrage opportunities; and (iii) competition has not affected the size or frequency of the arbitrage opportunities, it has only raised the bar for how fast one has to be to capture them. Second, we introduce a simple theory model which is motivated by and helps explain the empirical facts. The key insight is that obvious mechanical arbitrage opportunities, like those observed in the data, are built into the market design-continuous-time serial-processing implies that even symmetrically observed public information creates arbitrage rents. These rents harm liquidity provision and induce a never-ending socially wasteful arms race for speed. Last, we show that frequent batch auctions directly address the flaws of the continuous limit order book. Discrete time reduces the value of tiny speed advantages, and the auction transforms competition on speed into competition on price. Consequently, frequent batch auctions eliminate the mechanical arbitrage rents, enhance liquidity for investors, and stop the high-frequency trading arms race. ","SELECTED":null}
{"Title":"The diversity of high-frequency traders","keywords (cleaned)":"High-frequency trading;Market making;Market quality;Volatility","Abstract":"The regulatory debate concerning high-frequency trading (HFT) emphasizes the importance of distinguishing different HFT strategies and their influence on market quality. Using data from NASDAQ-OMX Stockholm, we compare market-making HFTs to opportunistic HFTs. We find that market makers constitute the lion's share of HFT trading volume (63-72%) and limit order traffic (81-86%). Furthermore, market makers have higher order-to-trade ratios and lower latency than opportunistic HFTs. In a natural experiment based on tick size changes, we find that the activity of market-making HFTs mitigates intraday price volatility. ","SELECTED":null}
{"Title":"Individual investors and financial disclosure","keywords (cleaned)":"Financial disclosure;Individual characteristics;Individual investors;Information disadvantage","Abstract":"Using detailed data of individual investors, this study shows that, on average, individuals invest more in firms with clear and concise financial disclosures. The results indicate this relation is less pronounced for high frequency trading and financially-literate individuals. The study also shows that individuals' returns are increasing with clearer and more concise disclosures, implying such disclosures reduce individuals' relative information disadvantage. Together, the findings suggest improved corporate disclosure practices benefit individual investors, in particular buy-and-hold investors. ","SELECTED":null}
{"Title":"High-frequency financial econometrics","keywords (cleaned)":"r|econometrics|research|financial econometrics|market microstructure|microstructure noise|High-frequency trading|high-frequency trading|microstructure|expo|trading|trading strategies|jump process|market|arch|estimation|strategies|High-frequency|algorithm|stock|financial data|trade|volatility|high-frequency","Abstract":"High-frequency trading is an algorithm-based computerized trading practice that allows firms to trade stocks in milliseconds. Over the last fifteen years, the use of statistical and econometric methods for analyzing high-frequency financial data has grown exponentially. This growth has been driven by the increasing availability of such data, the technological advancements that make high-frequency trading strategies possible, and the need of practitioners to analyze these data. This comprehensive book introduces readers to these emerging methods and tools of analysis. Yacine A\u00eft-Sahalia and Jean Jacod cover the mathematical foundations of stochastic processes, describe the primary characteristics of high-frequency financial data, and present the asymptotic concepts that their analysis relies on. A\u00eft-Sahalia and Jacod also deal with estimation of the volatility portion of the model, including methods that are robust to market microstructure noise, and address estimation and testing questions involving the jump part of the model. As they demonstrate, the practical importance and relevance of jumps in financial data are universally recognized, but only recently have econometric methods become available to rigorously analyze jump processes. A\u00eft-Sahalia and Jacod approach high-frequency econometrics with a distinct focus on the financial side of matters while maintaining technical rigor, which makes this book invaluable to researchers and practitioners alike. ","SELECTED":null}
{"Title":"Design and analysis of a social botnet","keywords (cleaned)":"Profitability;Automated social engineering;Social engineering;Social network security;Online systems;Commerce;Online privacy;Social network security;Social networking (online);Botnets;Behavioral research;Social networking (online);Socialbots;Social networking (online);Network security","Abstract":"Online Social Networks (OSNs) have attracted millions of active users and have become an integral part of today's web ecosystem. Unfortunately, in the wrong hands, OSNs can be used to harvest private user data, distribute malware, control botnets, perform surveillance, spread misinformation, and even influence algorithmic trading. Usually, an adversary starts off by running an infiltration campaign using hijacked or adversary-owned OSN accounts, with an objective to connect with a large number of users in the targeted OSN. In this article, we evaluate how vulnerable OSNs are to a large-scale infiltration campaign run by socialbots: bots that control OSN accounts and mimic the actions of real users. We adopted the design of a traditional web-based botnet and built a prototype of a Socialbot Network (SbN): a group of coordinated programmable socialbots. We operated our prototype on Facebook for 8 weeks, and collected data about user behavior in response to a large-scale infiltration campaign. Our results show that (1) by exploiting known social behaviors of users, OSNs such as Facebook can be infiltrated with a success rate of up to 80%, (2) subject to user profile privacy settings, a successful infiltration can result in privacy breaches where even more private user data are exposed, (3) given the economics of today's underground markets, running a large-scale infiltration campaign might be profitable but is still not particularly attractive as a sustainable and independent business, (4) the security of socially-aware systems that use or integrate OSN platforms can be at risk, given the infiltration capability of an adversary in OSNs, and (5) defending against malicious socialbots raises a set of challenges that relate to web automation, online-offline identity binding, and usable security. ","SELECTED":null}
{"Title":"The Flash Crash: High-Frequency Trading in an Electronic Market","keywords (cleaned)":"r|Trade|High-Frequency|Trading|intraday|stock index futures|electronic market|High-Frequency Traders|futures market|trading|index|market|design|Market|financial markets|stock|prices|edi|Flash Crash|High-Frequency Trading|Electronic Market|financial market|futures","Abstract":"We study intraday market intermediation in an electronic market before and during a period of large and temporary selling pressure. On May 6, 2010, U.S. financial markets experienced a systemic intraday event\u2014the Flash Crash\u2014where a large automated selling program was rapidly executed in the E-mini S&P 500 stock index futures market. Using audit trail transaction-level data for the E-mini on May 6 and the previous three days, we find that the trading pattern of the most active nondesignated intraday intermediaries (classified as High-Frequency Traders) did not change when prices fell during the Flash Crash. ","SELECTED":null}
{"Title":"What's not there: Odd lots and market data","keywords (cleaned)":"algorithm|gate|price discovery|stock|trading|informed traders|r|market|informed trader|edi|order imbalance|trade|high-frequency trading|market data|sentiment|high-frequency","Abstract":"We investigate odd-lot trades in equity markets. Odd lots are increasingly used in algorithmic and high-frequency trading, but are not reported to the consolidated tape or in databases such as TAQ. In our sample, the median number of odd-lot trades is 24% but in some stocks odd lots are 60% or more of trading. Odd-lot trades contribute 35% of price discovery, consistent with informed traders using odd lots to avoid detection. Omitting odd-lot trades leads to inaccuracies in order imbalance measures and makes sentiment measures unreliable. Excluding odd lots from the consolidated tape raises important regulatory issues. ","SELECTED":null}
{"Title":"High-frequency financial data modeling using Hawkes processes","keywords (cleaned)":"Hawkes processes;High-frequency data;Peaks-over-threshold;Self-exciting process;Value-at-risk","Abstract":"Intraday Value-at-Risk (VaR) is one of the risk measures used by market participants involved in high-frequency trading. High-frequency log-returns feature important kurtosis (fat tails) and volatility clustering (extreme log-returns appear in clusters) that VaR models should take into account. We propose a marked point process model for the excesses of the time series over a high threshold that combines Hawkes processes for the exceedances with a generalized Pareto distribution model for the marks (exceedance sizes). The conditional approach features intraday clustering of extremes and is used to calculate instantaneous conditional VaR. The models are backtested on real data and compared to a competitor approach that proposes a nonparametric extension of the classical peaks-over-threshold method. Maximum likelihood estimation is computationally intensive; we use a differential evolution genetic algorithm to find adequate starting values for the optimization process. ","SELECTED":null}
{"Title":"The Penn-Lehman Automated Trading Project","keywords (cleaned)":"Automated markets;Financial markets;Algorithms;Automatic programming;Competition;Computer program listings;Computer simulation;Computer software;Finance;Mathematical models;Real time systems;Strategic planning;Electronic commerce","Abstract":"The various aspects of Penn-Lehman Automated Trading (PLAT) project are discussed. The project uses a Penn Exchange Simulator (PXS), to develop algorithms and strategies for automatic stock trading. PXS automatically computes client profit and losses, volumes traded and simulator and external prices. PXS merges limit order data from two sources: actively connected trading clients and limit orders. The simulator runs both in live and historical modes.","SELECTED":null}
{"Title":"Keep calm and react with foresight: Strategies for low-latency and energy-efficient elastic data stream processing","keywords (cleaned)":"Parallel programming;Queueing theory;Voltage scaling;Data handling;Optimization;Electronic trading;Data stream processing;High-frequency trading;Data communication systems;Multicore programming;DVFS;Optimal applications;Energy efficiency;Latency constraints;State-of-the-art techniques;Model predictive control;Data stream processing;Multicore programming;Degree of flexibility;Computer programming;Dynamic frequency scaling;Model predictive control;Dynamic voltage and frequency scaling;Elasticity;Energy utilization;Program processors","Abstract":"This paper addresses the problem of designing scaling strategies for elastic data stream processing. Elasticity allows applications to rapidly change their configuration on-the-fly (e.g., the amount of used resources) in response to dynamic workload fluctuations. In this work we face this problem by adopting the Model Predictive Control technique, a control-theoretic method aimed at finding the optimal application configuration along a limited prediction hori-zon in the future by solving an online optimization problem. Our control strategies are designed to address latency constraints, using Queueing Theory models, and energy consumption by changing the number of used cores and the CPU frequency through the Dynamic Voltage and Frequency Scaling (DVFS) support available in the modern multicore CPUs. The proactive capabilities, in addition to the latency-and energy-awareness, represent the novel features of our approach. To validate our methodology, we develop a thorough set of experiments on a high-frequency trading application. The re-sults demonstrate the high-degree of flexibility and configurability of our approach, and show the effectiveness of our elastic scaling strategies compared with existing state-of-the-art techniques used in similar scenarios. ","SELECTED":null}
{"Title":"Efficient event processing through reconfigurable hardware for algorithmic trading","keywords (cleaned)":"Algorithmic trading;Boolean expressions;Event-processing engine;Financial data;Financial strategies;High frequency HF;Low-latency;Processing platform;Publish\/Subscribe system;Toronto;Algorithms;Commerce;Digital signal processing;Hardware;Quantum chemistry;Reconfigurable hardware","Abstract":"In this demo, we present fpga-ToPSS (Toronto Publish\/Subscribe System Family), an efficient event processing platform for highfrequency and low-latency algorithmic trading. Our event processing platform is built over reconfigurable hardware-FPGAs-to achieve line-rate processing. Furthermore, our event processing engine supports Boolean expression matching with an expressive predicate language that models complex financial strategies to autonomously buy and sell stocks based on real-time financial data. ","SELECTED":null}
{"Title":"Application of evolutionary computation for rule discovery in stock algorithmic trading: A literature review","keywords (cleaned)":"Calculations;Electronic trading;Rule discovery;Commerce;Data mining;Artificial intelligence;Algorithmic trading;Genetic algorithms;Blending;Stock trading;Classification framework;Evolutionary algorithms;Literature reviews;Algorithms;Stock trading rule;Costs;Literature reviews;Evolutionary computation;Genetic programming","Abstract":"Despite the wide application of evolutionary computation (EC) techniques to rule discovery in stock algorithmic trading (AT), a comprehensive literature review on this topic is unavailable. Therefore, this paper aims to provide the first systematic literature review on the state-of-the-art application of EC techniques for rule discovery in stock AT. Out of 650 articles published before 2013 (inclusive), 51 relevant articles from 24 journals were confirmed. These papers were reviewed and grouped into three analytical method categories (fundamental analysis, technical analysis, and blending analysis) and three EC technique categories (evolutionary algorithm, swarm intelligence, and hybrid EC techniques). A significant bias toward the applications of genetic algorithm-based (GA) and genetic programming-based (GP) techniques in technical trading rule discovery is observed. Other EC techniques and fundamental analysis lack sufficient study. Furthermore, we summarize the information on the evaluation scheme of selected papers and particularly analyze the researches which compare their models with buy and hold strategy (B&H). We observe an interesting phenomenon where most of the existing techniques perform effectively in the downtrend and poorly in the uptrend, and considering the distribution of research in the classification framework, we suggest that this phenomenon can be attributed to the inclination of factor selections and problem in transaction cost selections. We also observe the significant influence of the transaction cost change on the margins of excess return. Other influenced factors are also presented in detail. The absence of ways for market trend prediction and the selection of transaction cost are two major limitations of the studies reviewed. In addition, the combination of trading rule discovery techniques and portfolio selection is a major research gap. Our review reveals the research focus and gaps in applying EC techniques for rule discovery in stock AT and suggests a roadmap for future research. ","SELECTED":null}
{"Title":"Optimal high-frequency trading with limit and market orders","keywords (cleaned)":"Applied mathematical finance;Market microstructure;Portfolio optimization;Quantitative finance techniques;Stochastic control;Trading strategies","Abstract":"We propose a framework for studying optimal market-making policies in a limit order book (LOB). The bid-ask spread of the LOB is modeled by a tick-valued continuous-time Markov chain. We consider a small agent who continuously submits limit buy\/sell orders at best bid\/ask quotes, and may also set limit orders at best bid (resp. ask) plus (resp. minus) a tick for obtaining execution order priority, which is a crucial issue in high-frequency trading. The agent faces an execution risk since her limit orders are executed only when they meet counterpart market orders. She is also subject to inventory risk due to price volatility when holding the risky asset. The agent can then also choose to trade with market orders, and therefore obtain immediate execution, but at a less favorable price. The objective of the market maker is to maximize her expected utility from revenue over a short-term horizon by a trade-off between limit and market orders, while controlling her inventory position. This is formulated as a mixed regime switching regular\/impulse control problem that we characterize in terms of a quasi-variational system by dynamic programming methods. Calibration procedures are derived for estimating the transition matrix and intensity parameters for the spread and for Cox processes modelling the execution of limit orders. We provide an explicit backward splitting scheme for solving the problem and show how it can be reduced to a system of simple equations involving only the inventory and spread variables. Several computational tests are performed both on simulated and real data, and illustrate the impact and profit when considering execution priority in limit orders and market orders. ","SELECTED":null}
{"Title":"Esc: Towards an elastic stream computing platform for the cloud","keywords (cleaned)":"Directed acyclic graphs;Event Processing;Online data;High frequency HF;Batch data processing;Computer programming;Data flow;Online processing;Computational demands;Computational capacity;Cloud computing;Stream computing;Data handling;Programming models;Adaptability;Fault tolerance;Data mining;Data stream","Abstract":"Today, most tools for processing big data are batch-oriented. However, many scenarios require continuous, online processing of data streams and events. We present ESC, a new stream computing engine. It is designed for computations with real-time demands, such as online data mining. It offers a simple programming model in which programs are specified by directed acyclic graphs (DAGs). The DAG defines the data flow of a program, vertices represent operations applied to the data. The data which are streaming through the graph are expressed as key\/value pairs. ESC allows programmers to focus on the problem at hand and deals with distribution and fault tolerance. Furthermore, it is able to adapt to changing computational demands. In the cloud, ESC can dynamically attach and release machines to adjust the computational capacities to the current needs. This is crucial for stream computing since the amount of data fed into the system is not under the platform's control. We substantiate the concepts we propose in this paper with an evaluation based on a high-frequency trading scenario. ","SELECTED":null}
{"Title":"A 105Gb\/s 300GHz CMOS transmitter","keywords (cleaned)":"CMOS integrated circuits;Electronic trading;Fibers;Light velocity;Optical fiber fabrication;Optical fibers;Optical transmitters;Transmitters;Wireless telecommunication systems;Atmospheric transmissions;CMOS transmitters;Fiber optic links;Fiber-optic technology;High-frequency trading;Real time response;Transmitters (Tx);Wireless technologies;Optical fibers","Abstract":"'High speed' in communications often means 'high data-rate' and fiber-optic technologies have long been ahead of wireless technologies in that regard. However, an often overlooked definite advantage of wireless links over fiber-optic links is that waves travel at the speed of light c, which is about 50% faster than in optical fibers as shown in Fig. 17.9.1 (top left). This 'minimum latency' is crucial for applications requiring real-time responses over a long distance, including high-frequency trading [1]. Further opportunities and new applications might be created if the absolute minimum latency and fiber-optic data-rates are put together. (Sub-)THz frequencies have an extremely broad atmospheric transmission window with manageable losses as shown in Fig. 17.9.1 (top right) and will be ideal for building light-speed links supporting fiber-optic data-rates. This paper presents a 105Gb\/s 300GHz transmitter (TX) fabricated using a 40nm CMOS process. ","SELECTED":null}
{"Title":"Automatic code generation for real-time convex optimization","keywords (cleaned)":"Automatic programming;Automation;Codes (symbols);Convex optimization;Decision making;Digital storage;Embedded systems;Optimization;Program compilers;Signal processing;Automatic code generations;Human decision making;Mathematical optimizations;Optimization algorithms;Optimization problems;Real-time embedded systems;Real-time estimation;Storage structures;Real time systems","Abstract":"This chapter concerns the use of convex optimization in real-time embedded systems, in areas such as signal processing, automatic control, real-time estimation, real-time resource allocation and decision making, and fast automated trading. By \u201cembedded\u201d we mean that the optimization algorithm is part of a larger, fully automated system, that executes automatically with newly arriving data or changing conditions, and without any human intervention or action. By \u201creal-time\u201d we mean that the optimization algorithm executes much faster than a typical or generic method with a human in the loop, in times measured in milliseconds or microseconds for small and medium size problems, and (a few) seconds for larger problems. In real-time embedded convex optimization the same optimization problem is solved many times, with different data, often with a hard real-time deadline. In this chapter we propose an automatic code generation system for real-time embedded convex optimization. Such a system scans a description of the problem family, and performs much of the analysis and optimization of the algorithm, such as choosing variable orderings used with sparse factorizations and determining storage structures, at code generation time. Compiling the generated source code yields an extremely efficient custom solver for the problem family. We describe a preliminary implementation, built on the Python-based modeling framework CVXMOD, and give some timing results for several examples. Introduction Advisory optimization Mathematical optimization is traditionally thought of as an aid to human decision making. ","SELECTED":null}
{"Title":"Automated trading with performance weighted random forests and seasonality","keywords (cleaned)":"Profitability;Economics;Random forests;Expert systems;Decision trees;Commerce;Algorithmic trading;Forecasting;Regression techniques;Automated trading systems;Learning systems;Empirical regularities;Stock price prediction;Machine learning;Ensemble learning;Machine learning techniques","Abstract":"Seasonality effects and empirical regularities in financial data have been well documented in the financial economics literature for over seven decades. This paper proposes an expert system that uses novel machine learning techniques to predict the price return over these seasonal events, and then uses these predictions to develop a profitable trading strategy. While simple approaches to trading these regularities can prove profitable, such trading leads to potential large drawdowns (peak-to-trough decline of an investment measured as a percentage between the peak and the trough) in profit. In this paper, we introduce an automated trading system based on performance weighted ensembles of random forests that improves the profitability and stability of trading seasonality events. An analysis of various regression techniques is performed as well as an exploration of the merits of various techniques for expert weighting. The performance of the models is analysed using a large sample of stocks from the DAX. The results show that recency-weighted ensembles of random forests produce superior results in terms of both profitability and prediction accuracy compared with other ensemble techniques. It is also found that using seasonality effects produces superior results than not having them modelled explicitly. ","SELECTED":null}
{"Title":"Latency arbitrage, market fragmentation, and efficiency: A two-market model","keywords (cleaned)":"High-frequency trading;Financial data processing;Zero intelligences;Electronic commerce;Allocative efficiency;Information transfers;Discrete event simulation;Arbitrage opportunity;Market fragmentation;Agent-based approach;Regulation nms","Abstract":"We study the effect of latency arbitrage on allocative efficiency and liquidity in fragmented financial markets. We propose a simple model of latency arbitrage in which a single security is traded on two exchanges, with aggregate information available to regular traders only after some delay. An infinitely fast arbitrageur profits from market fragmentation by reaping the surplus when the two markets diverge due to this latency in cross-market communication. We develop a discrete-event simulation system to capture this processing and information transfer delay, and using an agent-based approach, we simulate the interactions between highfrequency and zero-intelligence trading agents at the millisecond level. We then evaluate allocative efficiency and market liquidity arising from the simulated order streams, and we find that market fragmentation and the presence of a latency arbitrageur reduces total surplus and negatively impacts liquidity. By replacing continuous-time markets with periodic call markets, we eliminate latency arbitrage opportunities and achieve further efficiency gains through the aggregation of orders over short time periods. Copyright ","SELECTED":null}
{"Title":"The Economics of High-Frequency Trading: Taking Stock","keywords (cleaned)":"Electronic market;High-frequency trading;Microstructure","Abstract":"I review the recent high-frequency trader (HFT) literature to single out the economic channels by which HFTs affect market quality. I first group the various theoretical studies according to common denominators and discuss the economic costs and benefits they identify. For each group, I then review the empirical literature that speaks to either the models' assumptions or their predictions. This enables me to come to a data-weighted judgement on the economic value of HFTs. Copyright ","SELECTED":null}
{"Title":"DBToaster: Higher-order delta processing for dynamic, frequently fresh views","keywords (cleaned)":"Costs;Information management;Maintenance;Optimization;Semantics;Algorithmic trading;Cost-based optimization;Dynamic data management;Finite differencing;Incremental maintenance;Optimizing compilation;Real-time analytics;Scientific data analysis;Search engines","Abstract":"Applications ranging from algorithmic trading to scientific data analysis require realtime analytics based on views over databases that change at very high rates. Such views have to be kept fresh at low maintenance cost and latencies. At the same time, these views have to support classical SQL, rather than window semantics, to enable applications that combine current with aged or historical data. In this paper, we present viewlet transforms, a recursive finite differencing technique applied to queries. The viewlet transform materializes a query and a set of its higher-order deltas as views. These views support each other's incremental maintenance, leading to a reduced overall view maintenance cost. The viewlet transform of a query admits efficient evaluation, the elimination of certain expensive query operations, and aggressive parallelization. We develop viewlet transforms into a workable query execution technique, present a heuristic and cost-based optimization framework, and report on experiments with a prototype dynamic data management system that combines viewlet transforms with an optimizing compilation technique. The system supports tens of thousands of complete view refreshes a second for a wide range of queries. ","SELECTED":null}
{"Title":"Proactive elasticity and energy awareness in data stream processing","keywords (cleaned)":"Quality of service;Multi-core systems;Dynamic frequency scaling;Model predictive control;Frequency scaling;Dynamic voltage and frequency scaling;Energy awareness;Elasticity;Voltage scaling;Specific properties;Data handling;Frequency-scaling;QoS requirements;Electronic trading;Data stream processing;Program processors;Power management;High-frequency trading;Data communication systems","Abstract":"Data stream processing applications have a long running nature (24 hr\/7 d) with workload conditions that may exhibit wide variations at run-time. Elasticity is the term coined to describe the capability of applications to change dynamically their resource usage in response to workload fluctuations. This paper focuses on strategies for elastic data stream processing targeting multicore systems. The key idea is to exploit Model Predictive Control, a control-theoretic method that takes into account the system behavior over a future time horizon in order to decide the best reconfiguration to execute. We design a set of energy-aware proactive strategies, optimized for throughput and latency QoS requirements, which regulate the number of used cores and the CPU frequency through the Dynamic Voltage and Frequency Scaling (DVFS) support offered by modern multicore CPUs. We evaluate our strategies in a high-frequency trading application fed by synthetic and real-world workload traces. We introduce specific properties to effectively compare different elastic approaches, and the results show that our strategies are able to achieve the best outcome. ","SELECTED":null}
{"Title":"High frequency trading acceleration using FPGAs","keywords (cleaned)":"Ethers;FIX;FPGA;Low latency;Commerce;Electronic data interchange;FAST;High frequency;Ethernet;High frequency trading;Field programmable gate arrays (FPGA);Internet protocols;UDP","Abstract":"This paper presents the design of an application specific hardware for accelerating High Frequency Trading applications. It is optimized to achieve the lowest possible latency for interpreting market data feeds and hence enable minimal round-trip times for executing electronic stock trades. The implementation described in this work enables hardware decoding of Ethernet, IP and UDP as well as of the FAST protocol which is a common protocol to transmit market feeds. For this purpose, we developed a microcode engine with a corresponding instruction set as well as a compiler which enables the flexibility to support a wide range of applied trading protocols. The complete system has been implemented in RTL code and evaluated on an FPGA. Our approach shows a 4x latency reduction in comparison to the conventional Software based approach. ","SELECTED":null}
{"Title":"Automated trading with boosting and expert weighting","keywords (cleaned)":"Algorithmic trading;Automated trading;Boosting;Machine learning","Abstract":"We propose a multi-stock automated trading system that relies on a layered structure consisting of a machine learning algorithm, an online learning utility, and a risk management overlay. Alternating decision tree (ADT), which is implemented with Logitboost, was chosen as the underlying algorithm. One of the strengths of our approach is that the algorithm is able to select the best combination of rules derived from well-known technical analysis indicators and is also able to select the best parameters of the technical indicators. Additionally, the online learning layer combines the output of several ADTs and suggests a short or long position. Finally, the risk management layer can validate the trading signal when it exceeds a specified non-zero threshold and limit the application of our trading strategy when it is not profitable. We test the expert weighting algorithm with data of 100 randomly selected companies of the S & P 500 index during the period 2003-2005. We find that this algorithm generates abnormal returns during the test period. Our experiments show that the boosting approach is able to improve the predictive capacity when indicators are combined and aggregated as a single predictor. Even more, the combination of indicators of different stocks demonstrated to be adequate in order to reduce the use of computational resources, and still maintain an adequate predictive capacity. ","SELECTED":null}
{"Title":"Scale-up strategies for processing high-rate data streams in system S","keywords (cleaned)":"Algorithmic trading;Application domains;Architectural pattern;Code Generation;Data stream;Environmental Monitoring;Financial trading;High rate;Programming models;Runtime support;Scale-up;Stream processing;Computer hardware description languages;Linguistics;Middleware;Data processing","Abstract":"High performance stream processing is critical in sense-and-respond application domains - from environmental monitoring to algorithmic trading. In this paper, we focus on language and runtime support for improving the performance of sense-and-respond applications in processing data from highrate streams. The central tenet of this work is the definition of a streaming architectural pattern for these application domains and the programming model and the code generation framework to support it. Using IBM Research's System S middleware and the SPADE language, we demonstrate how to scale up a financial trading application. ","SELECTED":null}
{"Title":"Speed, algorithmic trading, and market quality around macroeconomic news announcements","keywords (cleaned)":"Event-based trading;High-frequency trading;Latency costs;Macroeconomic news;Market activity","Abstract":"This paper documents that speed is crucially important for high-frequency trading strategies based on U.S. macroeconomic news releases. Using order-level data on the highly liquid S&P 500 ETF traded on NASDAQ from January 6, 2009 to December 12, 2011, we find that a delay of 300. ms or more significantly reduces returns of news-based trading strategies. This reduction is greater for high impact news and on days with high volatility. In addition, we assess the effect of algorithmic trading on market quality around macroeconomic news. In the minute following a macroeconomic news arrival, algorithmic activity increases trading volume and depth at the best quotes, but also increases volatility and leads to a drop in overall depth. Quoted half-spreads decrease (increase) when we measure algorithmic trading over the full (top of the) order book. ","SELECTED":null}
{"Title":"Functional data analysis for volatility","keywords (cleaned)":"Functional principal component;Diffusion model;Data reduction;Financial data processing;Commerce;Principal Components;Economic analysis;High frequency;Forecasting;Principal component analysis;Trajectories of volatility;High frequency trading;Functional regression;Volatility process;Market returns;prediction","Abstract":"We introduce a functional volatility process for modeling volatility trajectories for high frequency observations in financial markets and describe functional representations and data-based recovery of the process from repeated observations. A study of its asymptotic properties, as the frequency of observed trades increases, is complemented by simulations and an application to the analysis of intra-day volatility patterns of the S&P 500 index. The proposed volatility model is found to be useful to identify recurring patterns of volatility and for successful prediction of future volatility, through the application of functional regression and prediction techniques. ","SELECTED":null}
{"Title":"Liquidity Cycles and Make\/Take Fees in Electronic Markets","keywords (cleaned)":"liquidity|algorithm|Electronic Markets|durations|trading|algorithmic trading|Liquidity|r|Trade|ace|Market|speed|profit|trade|Electronic Market","Abstract":"We develop a model in which the speed of reaction to trading opportunities is endogenous. Traders face a trade-off between the benefit of being first to seize a profit opportunity and the cost of attention required to be first to seize this opportunity. The model provides an explanation for maker\/taker pricing, and has implications for the effects of algorithmic trading on liquidity, volume, and welfare. Liquidity suppliers' and liquidity demanders' trading intensities reinforce each other, highlighting a new form of liquidity externalities. Data on durations between trades and quotes could be used to identify these externalities. ","SELECTED":null}
{"Title":"Intra-daily volume modeling and prediction for algorithmic trading","keywords (cleaned)":"Forecasting;GMM;Multiplicative error models;Trading volumes;Ultra-high-frequency data;VWAP","Abstract":"The explosion of algorithmic trading has been one of the most pro-minent recent trends in the financial industry. Algorithmic trading consists of automated trading strategies that attempt to minimize transaction costs by optimally placing orders. The key ingredient of many of these strategies are intra-daily volume proportions forecasts. This work proposes a dynamic model for intra-daily volumes that captures salient features of the series such as time series dependence, intra-daily periodicity and volume asymmetry. Moreover, we intro-duce loss functions for the evaluation of proportion forecasts which retains both an operational and information theoretic interpretation. An empirical application on a set of widely traded index Exchange Traded Funds shows that the proposed methodology is able to signif-icantly outperform common forecasting methods and delivers more precise predictions for Volume Weighted Average Price trading. ","SELECTED":null}
{"Title":"Every microsecond counts: Tracking fine-grain latencies with a lossy difference aggregator","keywords (cleaned)":"NetFlows;Internet telephony;Packet sampling;Compact data structure;Convolutional codes;Data structures;Active probing;Application specific integrated circuits;High-performance computing;Loss rates;End-to-end latency;Orders of magnitude;Standard deviation;Passive measurements;Passive measurements;Interactive video;Hardware implementations;Theoretical result;Video conferencing;Relative errors;Network applications;Automated trading;Hardware","Abstract":"Many network applications have stringent end-to-end latency requirements, including VoIP and interactive video conferencing, automated trading, and high-performance computing - where even microsecond variations may be intolerable. The resulting fine-grain measurement demands cannot be met effectively by existing technologies, such as SNMP, NetFlow, or active probing. We propose instrumenting routers with a hash-based primitive that we call a Lossy Difference Aggregator (LDA) to measure latencies down to tens of microseconds and losses as infrequent as one in a million. Such measurement can be viewed abstractly as what we refer to as a coordinated streaming problem, which is fundamentally harder than standard streaming problems due to the need to coordinate values between nodes. We describe a compact data structure that efficiently computes the average and standard deviation of latency and loss rate in a coordinated streaming environment. Our theoretical results translate to an efficient hardware implementation at 40 Gbps using less than 1% of a typical 65-nm 400-MHz networking ASIC. When compared to Poisson-spaced active probing with similar overheads, our LDA mechanism delivers orders of magnitude smaller relative error; active probing requires 50-60 times as much bandwidth to deliver similar levels of accuracy. Copyright 2009 ACM.","SELECTED":null}
{"Title":"Bidding algorithms for simultaneous auctions","keywords (cleaned)":"Data structures;Intelligent agents;System optimizations;Resource allocation;Software agents;Automated trading agent;Bidding algorithms;Algorithms","Abstract":"This paper introduces RoxyBot, one of the top-scoring agents in the First International Trading Agent Competition. A TAG agent simulates one vision of future travel agents: it represents a set of clients in simultaneous auctions, trading complementary (e.g., airline tickets and hotel reservations) and substitutable (e.g., symphony and theater tickets) goods. RoxyBot faced two key technical challenges in TAG: (i) allocation-assigning purchased goods to clients at the end of a game instance so as to maximize total client utility, and (ii) completion-determining the optimal quantity of each resource to buy and sell given client preferences, current holdings, and market prices. For the dimensions of TAG, an optimal solution to the allocation problem is tractable, and RoxyBot uses a search algorithm based on Az.ast to produce optimal allocations. An optimal solution to the completion problem is also tractable, but in the interest of minimizing bidding cycle time, RoxyBot solves the completion problem using beam search, producing approximately optimal completions. RoxyBot's completer relies on an innovative data structure called a priceline.","SELECTED":null}
{"Title":"Social signals and algorithmic trading of Bitcoin","keywords (cleaned)":"Algorithmic trading;Bitcoin;Computational social science;Polarization;prediction;sentiment","Abstract":"The availability of data on digital traces is growing to unprecedented sizes, but inferring actionable knowledge from large-scale data is far from being trivial. This is especially important for computational finance, where digital traces of human behaviour offer a great potential to drive trading strategies. We contribute to this by providing a consistent approach that integrates various datasources in the design of algorithmic traders. This allows us to derive insights into the principles behind the profitability of our trading strategies. We illustrate our approach through the analysis of Bitcoin, a cryptocurrency known for its large price fluctuations. In our analysis, we include economic signals of volume and price of exchange for USD, adoption of the Bitcoin technology and transaction volume of Bitcoin. We add social signals related to information search, word of mouth volume, emotional valence and opinion polarization as expressed in tweets related to Bitcoin for more than 3 years. Our analysis reveals that increases in opinion polarization and exchange volume precede rising Bitcoin prices, and that emotional valence precedes opinion polarization and rising exchange volumes. We apply these insights to design algorithmic trading strategies for Bitcoin, reaching very high profits in less than a year. We verify this high profitability with robust statistical methods that take into account risk and trading costs, confirming the long-standing hypothesis that trading-based social media sentiment has the potential to yield positive returns on investment. ","SELECTED":null}
{"Title":"Information transmission between financial markets in Chicago and New York","keywords (cleaned)":"Market behavior;Market pricing;Market structure","Abstract":"High-frequency trading has led to widespread efforts to reduce information propagation delays between physically distant exchanges. Using relativistically correct millisecond-resolution tick data, we document a three millisecond decrease in one-way communication time between the Chicago and New York areas that occurred from April 27, 2010 to August 17, 2012. We attribute the first segment of this decline to the introduction of a latency-optimized fiber optic connection in late 2010. A second phase of latency decrease can be attributed to line-of-sight microwave networks, operating primarily in the 6-11 GHz region of the spectrum, licensed during 2011 and 2012. Using publicly available information, we estimate these networks' latencies, costs, and bandwidths. ","SELECTED":null}
{"Title":"Modelling Asset Prices for Algorithmic and High-Frequency Trading","keywords (cleaned)":"Algorithmic trading;durations;Hidden Markov models;High-frequency traders","Abstract":"Algorithmic trading (AT) and high-frequency (HF) trading, which are responsible for over 70% of US stocks trading volume, have greatly changed the microstructure dynamics of tick-by-tick stock data. In this article, we employ a hidden Markov model to examine how the intraday dynamics of the stock market have changed and how to use this information to develop trading strategies at high frequencies. In particular, we show how to employ our model to submit limit orders to profit from the bid-ask spread, and we also provide evidence of how HF traders may profit from liquidity incentives (liquidity rebates). We use data from February 2001 and February 2008 to show that while in 2001 the intraday states with the shortest average durations (waiting time between trades) were also the ones with very few trades, in 2008 the vast majority of trades took place in the states with the shortest average durations. Moreover, in 2008, the states with the shortest durations have the smallest price impact as measured by the volatility of price innovations. ","SELECTED":null}
{"Title":"Processing high data rate streams in System S","keywords (cleaned)":"Code Generation;Split\/aggregate\/join architectural pattern;High data rate;Algorithmic trading;Real-world;Data handling;Data stream processing;Scale-up strategies;Data communication systems;Environmental Monitoring;Scale-up;Runtime support;Architectural pattern;Middleware;System S;High rate;Underlying systems;Computer hardware description languages;Workload balancing;Financial trading;Programming models;Stream processing","Abstract":"High-performance stream processing is critical in many sense-and-respond application domainsfrom environmental monitoring to algorithmic trading. In this paper, we focus on language and runtime support for improving the performance of sense-and-respond applications in processing data from high-rate live streams. The central tenets of this work are the programming model, the workload splitting mechanisms, the code generation framework, and the underlying System S middleware and Spade programming model. We demonstrate considerable scalability behavior coupled with low processing latency in a real-world financial trading application. ","SELECTED":null}
{"Title":"DBToaster: Higher-order delta processing for dynamic, frequently fresh views","keywords (cleaned)":"Real-time analytics;Database queries;Compilation techniques;Semantics;Scientific data analysis;Incremental maintenance;Incremental view maintenance;Materialized views;Maintenance;Compilation;Materialized views","Abstract":"Applications ranging from algorithmic trading to scientific data analysis require real-time analytics based on views over databases receiving thousands of updates each second. Such views have to be kept fresh at millisecond latencies. At the same time, these views have to support classical SQL, rather than window semantics, to enable applications that combine current with aged or historical data. In this article, we present the DBToaster system, which keeps materialized views of standard SQL queries continuously fresh as data changes very rapidly. This is achieved by a combination of aggressive compilation techniques and DBToaster's original recursive finite differencing technique which materializes a query and a set of its higher-order deltas as views. These views support each other's incremental maintenance, leading to a reduced overall view maintenance cost. DBToaster supports tens of thousands of complete view refreshes per second for a wide range of queries. ","SELECTED":null}
{"Title":"How slow is the NBBO? A comparison with direct exchange feeds","keywords (cleaned)":"High-frequency trading;Market data;Transparency","Abstract":"This paper provides evidence on the benefits of faster proprietary data feeds from stock exchanges over the regulated \"public\" consolidated data feeds. We measure and compare the National Best Bid and Offer (NBBO) prices in each data feed at the same data center. Price dislocations between the NBBOs occur several times a second in very active stocks and typically last one to two milliseconds. The short duration of dislocations makes their costs small for investors who trade infrequently, while the frequency of the dislocations makes them costly for frequent traders. Higher security price and days with high trading volume and volatility are associated with dislocations. ","SELECTED":null}
{"Title":"Flexible least squares for temporal data mining and statistical arbitrage","keywords (cleaned)":"Financial markets;Temporal data mining;Time-varying regression;Electronic trading;Commerce;Filtration;Statistical arbitrage;Algorithmic trading system;Flexible least squares;Data mining","Abstract":"A number of recent emerging applications call for studying data streams, potentially infinite flows of information updated in real-time. When multiple co-evolving data streams are observed, an important task is to determine how these streams depend on each other, accounting for dynamic dependence patterns without imposing any restrictive probabilistic law governing this dependence. In this paper we argue that flexible least squares (FLS), a penalized version of ordinary least squares that accommodates for time-varying regression coefficients, can be deployed successfully in this context. Our motivating application is statistical arbitrage, an investment strategy that exploits patterns detected in financial data streams. We demonstrate that FLS is algebraically equivalent to the well-known Kalman filter equations, and take advantage of this equivalence to gain a better understanding of FLS and suggest a more efficient algorithm. Promising experimental results obtained from a FLS-based algorithmic trading system for the S&P 500 Futures Index are reported. ","SELECTED":null}
{"Title":"E-commerce and firm bargaining power shift in grocery marketing channels: A case of wholesalers\u2019 structured document exchanges","keywords (cleaned)":"R|ann|trading|information|Electronic commerce|commerce|r|market|marketing|electronic data interchange|EDI","Abstract":"Electronic commerce is transforming interorganizational relationships in marketing channels. Reports frequently note that \u2018power\u2019 goes to those who gain relevant trading information in marketing channels. The paper then asks whether and how structured or \u2018automated\u2019 trading information exchanges (e.g. electronic data interchange (EDI) links) between suppliers and wholesalers impact on their bargaining power in grocery marketing channels. This paper starts with the view that automated information exchanges favour the suppliers because they obtain trading information and gain marketing flexibility more than the wholesalers do. Focusing on the perspective of grocery wholesalers, the paper examines the relationships between (1) EDI use, (2) suppliers\u2019 incentives for EDI, (3) wholesalers\u2019 perceived bargaining power and (4) trust and cooperation between wholesalers and suppliers. Based on exploratory survey data from 33 grocery wholesalers, the paper finds that automated information exchanges may lower wholesalers' perceived bargaining power. It also shows that an appropriate level of incentives from suppliers tends to compensate for the power loss. This results in higher trust and cooperation in their trading relationships. ","SELECTED":null}
{"Title":"Stock price prediction via discovering multi-frequency trading patterns","keywords (cleaned)":"Trading patterns;Financial markets;High-frequency trading;Mathematical transformations;Competitive performance;Commerce;Discrete Fourier transforms;Inverse problems;Forecasting;Short term prediction;Inverse Fourier transforms;State frequency memory;Corporate performance;Multi-frequency trading patterns;Long short-term memory;Electronic trading;State-of-the-art methods;Costs;Stock price prediction;Data mining","Abstract":"Stock prices are formed based on short and\/or long-term commercial and trading activities that reflect different frequencies of trading patterns. However, these patterns are often elusive as they are affected by many uncertain political-economic factors in the real world, such as corporate performances, government policies, and even breaking news circulated across markets. Moreover, time series of stock prices are non-stationary and non-linear, making the prediction of future price trends much challenging. To address them, we propose a novel State Frequency Memory (SFM) recurrent network to capture the multi-frequency trading patterns from past market data to make long and short term predictions over time. Inspired by Discrete Fourier Transform (DFT), the SFM decomposes the hidden states of memory cells into multiple frequency components, each of which models a particular frequency of latent trading pattern underlying the fluctuation of stock price. Then the future stock prices are predicted as a nonlinear mapping of the combination of these components in an Inverse Fourier Transform (IFT) fashion. Modeling multi-frequency trading patterns can enable more accurate predictions for various time ranges: while a shortterm prediction usually depends on high frequency trading patterns, a long-term prediction should focus more on the low frequency trading patterns targeting at long-term return. Unfortunately, no existing model explicitly distinguishes between various frequencies of trading patterns to make dynamic predictions in literature. The experiments on the real market data also demonstrate more competitive performance by the SFM as compared with the state-of-the-art methods. ","SELECTED":null}
{"Title":"A model for unpacking big data analytics in high-frequency trading","keywords (cleaned)":"Asymmetry;Big data;HFT;latency;Strategies","Abstract":"This study develops a conceptual model of the 7\u00a0V\u2032s of big data analytics to gain a deeper understanding of the strategies and practices of high-frequency trading (HFT) in financial markets. HFT is computerized trading using proprietary algorithms. Empirical data collected from HFT firms and regulators in the US and UK reveals competitive asymmetries between HFTs and low-frequency traders (LFTs) operating more traditional forms of market trading. These findings show that HFT gains extensive market advantages over LFT due to significant investment in advanced technological architecture. Regulators are challenged to keep pace with HFT as different priorities to the 7\u00a0V\u2032s are given in pursuit of a short term market strategy. This research has implications for regulators, financial practitioners and investors as the technological arms race is fundamentally changing the nature of global financial markets. ","SELECTED":null}
{"Title":"Multi-query stream processing on FPGAs","keywords (cleaned)":"Algorithmic trading;Event streams;High frequency HF;Real-time data;Stream processing;Targeted advertising;Field programmable gate arrays (FPGA);Logic design;Query processing","Abstract":"We present an efficient multi-query event stream platform to support query processing over high-frequency event streams. Our platform is built over reconfigurable hardware - FPGAs - to achieve line-rate multi-query processing by exploiting unprecedented degrees of parallelism and potential for pipelining, only available through custom-built, application-specific and low-level logic design. Moreover, a multi-query event stream processing engine is at the core of a wide range of applications including real-time data analytics, algorithmic trading, targeted advertisement, and (complex) event processing. ","SELECTED":null}
{"Title":"Robust technical trading strategies using GP for algorithmic portfolio selection","keywords (cleaned)":"Financial markets;Investments;Financial data processing;Generalization capability;Portfolio selection;Technical indicator;Commerce;Electronic trading;Trading rules;Algorithmic trading;Genetic algorithms;Investment strategy;Random sampling method;Portfolio managements;Finance;Portfolio managements;Algorithms;Trading rules;Genetic programming","Abstract":"This paper presents a Robust Genetic Programming approach for discovering profitable trading rules which are used to manage a portfolio of stocks from the Spanish market. The investigated method is used to determine potential buy and sell conditions for stocks, aiming to yield robust solutions able to withstand extreme market conditions, while producing high returns at a minimal risk. One of the biggest challenges GP evolved solutions face is over-fitting. GP trading rules need to have similar performance when tested with new data in order to be deployed in a real situation. We explore a random sampling method (RSFGP) which instead of calculating the fitness over the whole dataset, calculates it on randomly selected segments. This method shows improved robustness and out-of-sample results compared to standard genetic programming (SGP) and a volatility adjusted fitness (VAFGP). Trading strategies (TS) are evolved using financial metrics like the volatility, CAPM alpha and beta, and the Sharpe ratio alongside other Technical Indicators (TI) to find the best investment strategy. These strategies are evaluated using 21 of the most liquid stocks of the Spanish market. The achieved results clearly outperform Buy&Hold, SGP and VAFGP. Additionally, the solutions obtained with the training data during the experiments clearly show during testing robustness to step market declines as seen during the European sovereign debt crisis experienced recently in Spain. In this paper the solutions learned were able to operate for prolonged periods, which demonstrated the validity and robustness of the rules learned, which are able to operate continuously and with minimal human intervention. To sum up, the developed method is able to evolve TSs suitable for all market conditions with promising results, which suggests great potential in the method generalization capabilities. The use of financial metrics alongside popular TI enables the system to increase the stock return while proving resilient through time. The RSFGP system is able to cope with different types of markets achieving a portfolio return of 31.81% for the testing period 2009-2013 in the Spanish market, having the IBEX35 index returned 2.67%. ","SELECTED":null}
{"Title":"Does high frequency trading affect technical analysis and market efficiency? And if so, how?","keywords (cleaned)":"Exchange rates;Genetic programming;Technical trading rules","Abstract":"In this paper we investigate how high frequency trading affects technical analysis and market efficiency in the foreign exchange (FX) market by using a special adaptive form of the Strongly Typed Genetic Programming (STGP)-based learning algorithm. We use this approach for real one-minute high frequency data of the most traded currency pairs worldwide: EUR\/USD, USD\/JPY, GBP\/USD, AUD\/USD, USD\/CHF, and USD\/CAD. The STGP performance is compared with that of parametric and non-parametric models and validated by two formal empirical tests. We perform in-sample and out-of-sample comparisons between all models on the basis of forecast performance and investment return. Furthermore, our paper shows the relative strength of these models with respect to the actual trading profit generated by their forecasts. Empirical experiments suggest that the STGP forecasting technique significantly outperforms the traditional econometric models. We find evidence that the excess returns are both statistically and economically significant, even when appropriate transaction costs are taken into account. We also find evidence that HFT has a beneficial role in the price discovery process. ","SELECTED":null}
{"Title":"Algorithmic trading review","keywords (cleaned)":"Algorithmic trading;Arrival time;Data challenges;Dow Jones Industrial averages;Economic consequences;Market values;Research challenges;Ultra-high frequency;Algorithms;Data handling;Depreciation;Industrial research;Commerce","Abstract":"The competitive nature of AT, the scarcity of expertise, and the vast profits potential, makes for a secretive community where implementation details are difficult to find. AT presents huge research challenges, especially given the economic consequences of getting it wrong, such as the May 6, 2010 Flash Crash in which the Dow Jones Industrial Average plunged 9% wiping $600 billion off the market value and the Knight Capital loss of $440 million on August 1, 2012, due to erratic behavior of its trading algorithms. Current research challenges include: Data challenges cover the quantity\/quality of the data, processing data at ultra-high frequency and increasingly incorporating new types of data such as social media and news. Dealers generally execute their orders through a shared centralized order book that lists the buy and sell orders for a specific security ranked by price and order arrival time.","SELECTED":null}
{"Title":"Portfolio of automated trading systems: Complexity and learning set size issues","keywords (cleaned)":"Multi agent systems;Sample sizes;Financial data processing;artificial neural network;statistical model;Sample sizes;Optimization;Neural Networks (Computer);Portfolios;Multiagent systems;Efficient-market hypothesis;Experiments;procedures;Markowitz;information processing;Commerce;Models, Economic;Artificial intelligence;Investments;complexity;Automatic Data Processing;Artificial intelligence;regularization","Abstract":"In this paper, we consider using profit\/loss histories of multiple automated trading systems (ATSs) as N input variables in portfolio management. By means of multivariate statistical analysis and simulation studies, we analyze the influences of sample size (L) and input dimensionality on the accuracy of determining the portfolio weights. We find that degradation in portfolio performance due to inexact estimation of N means and N(N - 1)\/2 correlations is proportional to N\/L; however, estimation of N variances does not worsen the result. To reduce unhelpful sample size\/dimensionality effects, we perform a clustering of N time series and split them into a small number of blocks. Each block is composed of mutually correlated ATSs. It generates an expert trading agent based on a nontrainable 1\/ N portfolio rule. To increase the diversity of the expert agents, we use training sets of different lengths for clustering. In the output of the portfolio management system, the regularized mean-variance framework-based fusion agent is developed in each walk-forward step of an out-of-sample portfolio validation experiment. Experiments with the real financial data (2003-2012) confirm the effectiveness of the suggested approach. ","SELECTED":null}
{"Title":"High-frequency trading strategy based on deep neural networks","keywords (cleaned)":"Computational finance;High frequency HF;On currents;Training and testing;Deep neural networks;Commerce;Intelligent computing;Electronic trading;Standard deviation;Costs;High-frequency trading;Directional accuracy","Abstract":"This paper presents a high-frequency strategy based on Deep Neural Networks (DNNs). The DNN was trained on current time (hour and minute), and n-lagged one-minute pseudo-returns, price standard deviations and trend indicators in order to forecast the next one-minute average price. The DNN predictions are used to build a high-frequency trading strategy that buys (sells) when the next predicted average price is above (below) the last closing price. The data used for training and testing are the AAPL tick-by-tick transactions from September to November of 2008. The best-found DNN has a 66% of directional accuracy. This strategy yields an 81% successful trades during testing period. ","SELECTED":null}
{"Title":"Towards highly parallel event processing through reconfigurable hardware","keywords (cleaned)":"Algorithmic trading;Application requirements;Boolean expressions;Degree of parallelism;Design tradeoff;Event matching;Event-processing engine;High frequency HF;Low-latency;Processing platform;Publish\/Subscribe system;Re-configurable;Real-time data;Toronto;Algorithmic languages;Computer hardware;Data reduction;Digital signal processing;Field programmable gate arrays (FPGA);Information management;Quantum chemistry;Reconfigurable hardware","Abstract":"We present fpga-ToPSS (Toronto Publish\/Subscribe System), an efficient event processing platform to support high-frequency and low-latency event matching. fpga-ToPSS is built over reconfigurable hardware-FPGAs-to achieve line-rate processing by exploring various degrees of parallelism. Furthermore, each of our proposed FPGA-based designs is geared towards a unique application requirement, such as flexibility, adaptability, scalability, or pure performance, such that each solution is specifically optimized to attain a high level of parallelism. Therefore, each solution is formulated as a design trade-off between the degree of parallelism versus the desired application requirement. Moreover, our event processing engine supports Boolean expression matching with an expressive predicate language applicable to a wide range of applications including real-time data analysis, algorithmic trading, targeted advertisement, and (complex) event processing. Copyright ","SELECTED":null}
{"Title":"Enhancing risk-adjusted performance of stock market intraday trading with Neuro-Fuzzy systems","keywords (cleaned)":"artificial neural network;intermethod comparison;Fuzzy systems;mathematical computing;Neural networks;High-frequency trading;Dynamic moving average;article;DENFIS;mathematical analysis;adaptive neuro fuzzy system;Moving averages;ANN;commercial phenomena;computer prediction;Mathematical models;ANFIS;trade;Commerce;validation process;priority journal;Risk assessment;dynamic evolving fuzzy system;Algorithms;Fuzzy systems;Stock market;controlled study;analytical error;Profitability;ANFIS ensemble","Abstract":"Whilst the interest of many former studies on the application of AI in finance is solely on predicting market movements, trading practitioners are predominantly concerned about risk-adjusted performance. This paper provides new insights into improving the time-varying risk-adjusted performance of trading systems controlled by Artificial Neural Networks (ANNs), Adaptive Neuro-Fuzzy Systems (ANFIS) or Dynamic Evolving Neuro Fuzzy Systems (DENFIS). Contrary to most former studies which focus on daily predictions, we compare these models in an intraday stock trading scenario using high-frequency data. Firstly, we propose a dynamic extension of the popular moving average rule and enhance it with a model validation methodology using heat maps to analyse favourable profitability in specific holding time and signal regions. Secondly, we study the effect of realistic constraints such as transaction costs and intraday trading hours, which many existing approaches in the literature ignore. Thirdly, unlike most former studies that only aim to minimise statistical error measures, we compare this approach with financially more relevant risk-adjusted objective functions. To this end, we also consider an innovative ANFIS ensemble architecture which on an intraday level dynamically selects between different risk-adjusted models. Our study shows that accounting for transaction costs and the use of risk-return objective functions provide better results in out-of-sample tests. Overall, the ANN model is identified as a viable model, however ANFIS shows more stable time-varying performance across multiple market regimes. Moreover, we find that combining multiple risk-adjusted objective functions using an ANFIS ensemble yields promising results. ","SELECTED":null}
{"Title":"Implementing a high-volume, low-latency market data processing system on commodity hardware using IBM middleware","keywords (cleaned)":"Algorithmic trading;Implementation;Wireless sensor networks;Data volume;Safety factor;Market-maker;Market data processing;Options market;Commerce;Data processing;Middleware;Commodity hardware;Market data;Transport technology;Stock market;Low latency;Infiniband;Risk analysis;Risk assessment;IBM middleware;Market surveillance;Hardware","Abstract":"A stock market data processing system that can handle high data volumes at low latencies is critical to market makers. Such systems play a critical role in algorithmic trading, risk analysis, market surveillance, and many other related areas. We show that such a system can be built with general-purpose middleware and run on commodity hardware. The middleware we use is IBM System S, which has been augmented with transport technology from IBM WebSphere MQ Low Latency Messaging. Using eight commodity x86 blades connected with Ethernet and Infiniband, this system can achieve 80 \u03bcsec average latency at 3 times the February 2008 options market data rate and 206 \u03bcsec average latency at 15 times the February 2008 rate. Copyright ","SELECTED":null}
{"Title":"High Frequency Trading in the Korean Index Futures Market","keywords (cleaned)":"HFT|High Frequency|price discovery|high frequency trading (HFT)|r|transaction cost|Futures Market|data set|Trading|gate|market quality|profitability|futures market|trading|index|high frequency|market|Futures|Market|HFTs|costs|liquidity|Index|transaction costs|high frequency trading|profit|trade|futures|High Frequency Trading","Abstract":"We investigate the trading behavior of high frequency trading (HFT), the impact of HFT on market quality, its role in the price discovery process, and its profitability, using a very detailed data set of the KOSPI 200 index futures market. We find that high frequency traders (HFTs) do not provide liquidity in the futures market, nor does HFT have any role in enhancing market quality. Indeed, HFT is detrimental to the price discovery process. This finding is contrary to those in the existing literature on HFT in equity markets. We also find that profitable opportunities for HFTs are rare after transaction costs are considered, with the notable exception that foreign HFTs can earn a profit in the index futures market. ","SELECTED":null}
{"Title":"A Markov-switching multifractal inter-trade duration model, with application to US equities","keywords (cleaned)":"Regime-switching models;High-frequency trading;Fractals;Market microstructure;Liquidity;Commerce;Long memory;Time deformation;High-frequency trading data;Regime-switching models;Market microstructure;Point process","Abstract":"We propose and illustrate a Markov-switching multifractal duration (MSMD) model for analysis of inter-trade durations in financial markets. We establish several of its key properties with emphasis on high persistence and long memory. Empirical exploration suggests MSMD's superiority relative to leading competitors. ","SELECTED":null}
{"Title":"Modelling trades-through in a limit order book using Hawkes processes","keywords (cleaned)":"Hawkes processes;High-frequency trading;Limit order book;Microstructure;Trades-through","Abstract":"The authors model trades-through, i.e. transactions that reach at least the second level of limit orders in an order book. Using tick-by-tick data on Euronext-traded stocks, they show that a simple bivariate Hawkes process fits nicely their empirical observations of tradesthrough. The authors show that the cross-influence of bid and ask trades-through is weak. ","SELECTED":null}
{"Title":"A material political economy: Automated Trading Desk and price prediction in high-frequency trading","keywords (cleaned)":"Economics;HFT;Marketing;Models, Econometric;political economy;statistical model;United States;fixed-role markets;Commerce;commercial phenomena;financial management;prediction;Marketing;South Carolina;History, 20th Century;High-frequency trading;financial management;Economics;history;Automated Trading Desk","Abstract":"This article contains the first detailed historical study of one of the new high-frequency trading (HFT) firms that have transformed many of the world\u2019s financial markets. The study, of Automated Trading Desk (ATD), one of the earliest and most important such firms, focuses on how ATD\u2019s algorithms predicted share price changes. The article argues that political-economic struggles are integral to the existence of some of the \u2018pockets\u2019 of predictable structure in the otherwise random movements of prices, to the availability of the data that allow algorithms to identify these pockets, and to the capacity of algorithms to use these predictions to trade profitably. The article also examines the role of HFT algorithms such as ATD\u2019s in the epochal, fiercely contested shift in US share trading from \u2018fixed-role\u2019 markets towards \u2018all-to-all\u2019 markets. ","SELECTED":null}
{"Title":"OR forum-the cost of latency in high-frequency trading","keywords (cleaned)":"Closed-form expression;Electronic market;High-frequency trading;Human time scale;Institutional investors;Orders of magnitude;Theoretical modeling;Trade execution;Commerce;Costs;Economics;Investments;Time measurement;Cost benefit analysis","Abstract":"Modern electronic markets have been characterized by a relentless drive toward faster decision making. Significant technological investments have led to dramatic improvements in latency, the delay between a trading decision and the resulting trade execution. We describe a theoretical model for the quantitative valuation of latency. Our model measures the trading frictions created by the presence of latency, by considering the optimal execution problem of a representative investor. Via a dynamic programming analysis, our model provides a closed-form expression for the cost of latency in terms of well-known parameters of the underlying asset. We implement our model by estimating the latency cost incurred by trading on a human time scale. Examining NYSE common stocks from 1995 to 2005 shows that median latency cost across our sample roughly tripled during this time period. Furthermore, using the same data set, we compute a measure of implied latency and conclude that the median implied latency decreased by approximately two orders of magnitude. Empirically calibrated, our model suggests that the reduction in cost achieved by going from trading on a human time scale to a low latency time scale is comparable with other execution costs faced by the most cost efficient institutional investors, and it is consistent with the rents that are extracted by ultra-low latency agents, such as providers of automated execution services or high frequency traders. ","SELECTED":null}
{"Title":"Ethics, Finance, and Automation: A Preliminary Survey of Problems in High Frequency Trading","keywords (cleaned)":"quality control;information processing;article;business ethics;business ethics;Commerce;Automation;commercial phenomena;Engines;ethics;ethics;Quality management;Humans;quality control;Automation;Data Collection;Humans;High-frequency trading","Abstract":"All of finance is now automated, most notably high frequency trading. This paper examines the ethical implications of this fact. As automation is an interdisciplinary endeavor, we argue that the interfaces between the respective disciplines can lead to conflicting ethical perspectives; we also argue that existing disciplinary standards do not pay enough attention to the ethical problems automation generates. Conflicting perspectives undermine the protection those who rely on trading should have. Ethics in finance can be expanded to include organizational and industry-wide responsibilities to external market participants and society. As a starting point, quality management techniques can provide a foundation for a new cross-disciplinary ethical standard in the age of automation. ","SELECTED":null}
{"Title":"How high frequency trading affects a market index","keywords (cleaned)":"article;commercial phenomena;Investments;Commerce;Investments","Abstract":"The relationship between a market index and its constituent stocks is complicated. While an index is a weighted average of its constituent stocks, when the investigated time scale is one day or longer the index has been found to have a stronger effect on the stocks than vice versa. We explore how this interaction changes in short time scales using high frequency data. Using a correlation-based analysis approach, we find that in short time scales stocks have a stronger influence on the index. These findings have implications for high frequency trading and suggest that the price of an index should be published on shorter time scales, as close as possible to those of the actual transaction time scale.","SELECTED":null}
{"Title":"Model calibration and automated trading agent for Euro futures","keywords (cleaned)":"Agent based economics;Algorithmic trading;Automated trading;Boosting;Machine learning;Trading agent","Abstract":"We explored the application of a machine learning method, Logitboost, to automatically calibrate a trading model using different versions of the same technical analysis indicators. This approach takes advantage of boosting's feature selection capability to select an optimal combination of technical indicators and design a new set of trading rules. We tested this approach with high-frequency data of the Dow Jones EURO STOXX 50 Index Futures (FESX) and the DAX Futures (FDAX) for March 2009. Our method was implemented with different learning algorithms and outperformed a combination of the same group of technical analysis indicators using the parameters typically recommended by practitioners. We incorporated this method of model calibration in a trading agent that relies on a layered structure consisting of the machine learning algorithm described above, an online learning utility, a trading strategy, and a risk management overlay. The online learning layer combines the output of several experts and suggests a short or long position. If the expected position is positive (negative), the trading agent sends a buy (sell) limit order at prices slightly lower (higher) than the bid price at the top of the buy (sell) order book less (plus) transaction costs. If the order is not 100% filled within a fixed period (i.e. 1 minute) of being issued, the existent limit orders are cancelled, and limit orders are reissued according to the new experts' forecast. As part of its risk management capability, the trading agent eliminates any weak trading signal. The trading agent algorithm generated positive returns for the two major European index futures (FESX and FDAX) and outperformed a buy-and-hold strategy. ","SELECTED":null}
{"Title":"Censored exploration and the dark pool problem","keywords (cleaned)":"Algorithmic trading;Censored data;Experimental evaluation;Market impacts;Near-optimal algorithms;Stock exchange;Commerce;Lakes;Optimization;Reinforcement;Learning algorithms","Abstract":"Dark pools are a recent type of stock exchange in which information about outstanding orders is deliberately hidden in order to minimize the market impact of large-volume trades. The success and proliferation of dark pools have created challenging and interesting problems in algorithmic trading-in particular, the problem of optimizing the allocation of a large trade over multiple competing dark pools. In this work, we formalize this optimization as a problem of multi-venue exploration from censored data, and provide a provably efficient and near-optimal algorithm for its solution. Our algorithm and its analysis have much in common with well-studied algorithms for managing the exploration-exploitation trade-off in reinforcement learning. We also provide an extensive experimental evaluation of our algorithm using dark pool execution data from a large brokerage. ","SELECTED":null}
{"Title":"Algorithmic trading patterns in xetra orders","keywords (cleaned)":"Algorithmic trading;Cancellations;Market microstructure;Order lifetime;Xetra","Abstract":"Computerized trading controlled by algorithms - \"Algorithmic Trading\" - has become a fashionable term in investment banking. We investigate a set of Xetra order data to find traces of algorithmic trading by studying the lifetimes of cancelled orders. Even though it is widely agreed that an algorithm must randomize its order activities to avoid exploitation by other traders, we still find systematic patterns in the submission and cancellation of certain Xetra orders, indicating the activity of algorithmic trading. The trading patterns observed might be interpreted as fishing for profitable roundtrips.","SELECTED":null}
{"Title":"Sequence classification of the limit order book using recurrent neural networks","keywords (cleaned)":"Classification (of information);Financial markets;Financial data processing;Linear Kalman filters;Non-linear relationships;Sequence classification;Commerce;Short sequences;Forecasting;Neural networks;Correlation theory;Recurrent neural networks;Recurrent neural network (RNNs);Financial time series;Electronic trading;Costs;Limit order book;High-frequency trading;Futures market","Abstract":"Recurrent neural networks (RNNs) are types of artificial neural networks (ANNs) that are well suited to forecasting and sequence classification. They have been applied extensively to forecasting univariate financial time series, however their application to high frequency trading has not been previously considered. This paper solves a sequence classification problem in which a short sequence of observations of limit order book depths and market orders is used to predict a next event price-flip. The capability to adjust quotes according to this prediction reduces the likelihood of adverse price selection. Our results demonstrate the ability of the RNN to capture the non-linear relationship between the near-term price-flips and a spatio-temporal representation of the limit order book. The RNN compares favorably with other classifiers, including a linear Kalman filter, using S&P500 E-mini futures level II data over the month of August 2016. Further results assess the effect of retraining the RNN daily and the sensitivity of the performance to trade latency. ","SELECTED":null}
{"Title":"What is an algorithm? Financial regulation in the era of high-frequency trading","keywords (cleaned)":"Algorithms;regulatory framework;Algorithms;financial policy;compliance;financial regulation;stakeholder;High-frequency trading;enforcement;research work;Financial markets","Abstract":"In response to the flash crashes and market manipulations blamed on high-frequency trading (HFT), algorithms have been brought inside the regulatory perimeter. This paper focuses on the most ambitious regulation directed at the practice: the algorithm-tagging rule in the German High-Frequency Trading Act. Fifteen interviews with stakeholders in the Act\u2019s implementation serve to reconstruct how regulators defined an algorithm and help pose the question of to what extent regulatory definitions and data need accurately to represent financial practices to be useful. Although tentative in its findings, the research suggests that the algorithm-tagging rule may be providing valuable signals in the noise to trade surveillance officers and having virtuous effects on the cultures of trading firms. The conclusion argues that sociologists of finance should adopt a more balanced approach when evaluating regulatory technologies and heed MacKenzie\u2019s 2005 call to open up their black boxes. ","SELECTED":null}
{"Title":"Big data in finance","keywords (cleaned)":"Commerce;Computer architecture;Data visualization;Electronic trading;Finance;Information management;Purification;Risk management;Algorithmic trading;Emerging technologies;Financial practitioners;High-throughput data;Market opportunities;Massively parallel processing;Real-time data streams;Technological adaptation;Big data","Abstract":"Quantitative finance is an area in which data is the vital actionable information in all aspects. Leading finance institutions and firms are adopting advanced Big Data technologies towards gaining actionable insights from massive market data, standardizing financial data from a variety of sources, reducing the response time to real-time data streams, improving the scalability of algorithms and software stacks on novel architectures. Today, these major profits are driving the pioneers of the financial practitioners to develop and deploy the big data solutions in financial products, ranging from front-office algorithmic trading to back-office data management and analytics. Not only the collection and purification of multi-source data, the effective visualization of high-throughput data streams and rapid programmability on massively parallel processing architectures are widely used to facilitate the algorithmic trading and research. Big data analytics can help reveal more hidden market opportunities through analyzing high-volume structured data and social news, in contrast to the underperformers that are incapable of adopting novel techniques. Being able to process massive complex events in ultra-fast speed removes the roadblock for promptly capturing market trends and timely managing risks. These key trends in capital markets and extensive examples in quantitative finance are systematically highlighted in this chapter. The insufficiency of technological adaptation and the gap between research and practice are also presented. To clarify matters, the three natures of Big Data, volume, velocity and variety are used as a prism through which to understand the pitfalls and opportunities of emerged and emerging technologies towards financial services. ","SELECTED":null}
{"Title":"Forecasting trends of high-frequency KOSPI200 index data using learning classifiers","keywords (cleaned)":"Profitability;Financial forecasting;High frequency HF;Time series;Statistical learning;Commerce;Lead-lag;Forecasting;Finance;Learning algorithms;Market lead-lag relationship;Binary classification;High-frequency trading","Abstract":"Recently many statistical learning techniques have been applied to the prediction of financial variables. The aim of this paper is to conduct a comprehensive study of the applications of statistical learning techniques to predict the trend of the return of high-frequency Korea composite stock price index (KOSPI) 200 index data using the information from the one-minute time series of spot index, futures index, and foreign exchange rate. Through experiments, it is observed that the spot index change is better predictable with high-frequency time series data and the futures index information significantly improves the prediction accuracy of the return trends of the spot index for high-frequency index data, while the information of exchange rate does not. Also, dimension reduction process before training helps to increase the accuracy and dramatically for some classifiers. In addition, the trained classifiers with which a virtual trading strategy is applied to, noticeable better profits can be achieved than just a buy-and-hold-like strategy. ","SELECTED":null}
{"Title":"Adaptive neuro fuzzy inference systems for high frequency financial trading and forecasting","keywords (cleaned)":"Financial data;Financial data processing;Time series;Fuzzy systems;Forecasting;Very high frequency;Financial time series;Efficient market hypothesis;Financial forecasting;Human expert;Pattern recognition systems;Financial markets;Fuzzy reasoning;High frequency;Trading systems;Financial prediction;Fuzzy inference;Neuro-fuzzy inference systems;Expert systems;Short time intervals;Commerce;Fuzzy neural networks;Finance;Physical limitations;Adaptive neuro-fuzzy inference system;High-frequency finance;Computer aided engineering;Financial trading;High frequency trading;Automated trading;Neuro-fuzzy inference systems","Abstract":"The prediction of financial time series is a very complicated process. An initial look at financial time series gives the impression that they are random in nature. If true, this would make the forecast, and therefore the trading, of such series exceptionally difficult. The efficient market hypothesis states that the current price contains all available information in the market. This leads to the predictability of most financial time series as being a rather controversial issue. Experts have been forecasting and trading financial markets for decades, using their knowledge and expertise in recognizing patterns and interpreting current financial data. This paper extends the Adaptive Neuro-Fuzzy Inference System to create an expert system that is capable of using fuzzy reasoning combined with the pattern recognition capability of neural networks to be used in financial forecasting and trading. The novelty of the approach lies in its application to the field of high frequency finance. Such an approach has not been used so far with high frequency trading or as a part of an automated trading strategy. This has produced an expert trading system which overcomes the physical limitations of human experts and traders in taking multiple decisions at extremely short time intervals. This means the system can perform predictions and trading decisions at a very high frequency using intra-day data. ","SELECTED":null}
{"Title":"Evolving trading strategies using directional changes","keywords (cleaned)":"Economics;Genetic algorithms;Financial markets;Price fluctuation;Financial forecasting;Technical analysis;Commerce;Algorithmic trading;Genetic algorithms;Time measurement;Forecasting methods;Trading strategies;Electronic trading;Directional changes;Its efficiencies;Profitability","Abstract":"The majority of forecasting methods use a physical time scale for studying price fluctuations of financial markets, making the flow of physical time discontinuous. Therefore, using a physical time scale may expose companies to risks, due to ignorance of some significant activities. In this paper, an alternative and original approach is explored to capture important activities in the market. The main idea is to use an event-based time scale based on a new way of summarising data, called Directional Changes. Combined with a genetic algorithm, the proposed approach aims to find a trading strategy that maximises profitability in foreign exchange markets. In order to evaluate its efficiency and robustness, we run rigorous experiments on 255 datasets from six different currency pairs, consisting of intra-day data from the foreign exchange spot market. The results from these experiments indicate that our proposed approach is able to generate new and profitable trading strategies, significantly outperforming other traditional types of trading strategies, such as technical analysis and buy and hold. ","SELECTED":null}
{"Title":"Organizational ignorance: an ethnographic study of high-frequency trading","keywords (cleaned)":"Algorithms;black box;political economy;imitation;organizational structure;ignorance;social studies of finance;strategic approach;trade;High-frequency trading;Economic analysis;Financial markets","Abstract":"This paper provides an analysis of strategic uses of ignorance or not-knowing in one of the most secretive industries within the financial sector. The focus of the paper is on the relation between imitation and ignorance within the organizational structure of high-frequency trading (HFT) firms. In social studies of finance (SSF) literature imitation is considered a strategic act, i.e. imitation is a term applied when traders copy the strategies of other traders. I wish to turn this relation between ignorance and imitation on its head and consider ignorance itself as a strategic unknown and investigate the kinds of imitations that might be produced from structures of not-knowing (i.e. structures intended to divide, obscure and protect knowledge). This point is illustrated through ethnographic studies and interviews within five HFT firms. The data show how a black-box structure of ignorance is replicated within the organizational setting of these firms and re-enacted by the traders. Towards the end of the paper the politics of the relationship between imitation and ignorance is discussed. ","SELECTED":null}
{"Title":"Cultures of high-frequency trading: mapping the landscape of algorithmic developments in contemporary financial markets","keywords (cleaned)":"Algorithms;Algorithms;cultures;economic sociology;social studies of finance;knowledge;market development;trade;High-frequency trading;Financial markets;cultures","Abstract":"As part of ongoing work to lay a foundation for social studies of high-frequency trading (HFT), this paper introduces the culture(s) of HFT as a sociological problem relating to knowledge and practice. HFT is often discussed as a purely technological development, where all that matters is the speed of allocating, processing and transmitting data. Indeed, the speed at which trades are executed and data transmitted is accelerating, and it is fair to say that algorithms are now the primary interacting agents operating in the financial markets. However, we contend that HFT is first and foremost a cultural phenomenon. More specifically, both individuals and collective agents \u2013 such as algorithms \u2013 might be considered cultural entities, charged with very different ways of processing information, making sense of it and turning it into knowledge and practice. This raises issues relating to situated knowledge, distributed cognition and action, the assignment of responsibility when regulating high-speed algorithms, their history, organizational structure and, perhaps more fundamentally, their representation. ","SELECTED":null}
{"Title":"High frequency trading, liquidity, and execution cost","keywords (cleaned)":"Discrete optimization;High frequency trading;Liquidity;Optimal execution;Price impact","Abstract":"We build a model under the framework of discrete optimization to explain how high frequency trading (HFT) can be applied to supply liquidity and reduce execution cost. We derive the analytical properties of our model in finding the optimal solution to minimize the overall execution cost of HFT. We show that the execution cost can be reduced after increasing trading frequency (i.e., the higher the trading frequency, the lower the execution cost) with a simulation study. In addition, we conduct an empirical investigation with tick level data from US equity market through January 2008 to October 2010 to verify our conclusion drawn from the simulation study. Based on the simulation and empirical results we collected, we show that the HFT can reduce the execution cost when supplying liquidity. ","SELECTED":null}
{"Title":"Forecasting high-frequency futures returns using online langevin dynamics","keywords (cleaned)":"Nonlinear filtering;High frequency HF;Langevin dynamics;tracking;Variable rate;Algorithmic trading;Futures trading;particle filter;particle filter;quantitative finance;Transaction cost;Futures contract;Surface discharges;High frequency;Online learning;Jump diffusion models;Online learning;Commerce;Closed form;Transition density;Algorithms;High-frequency finance;Asset pricing;Costs;Sharpe ratio","Abstract":"Forecasting the returns of assets at high frequency is the key challenge for high-frequency algorithmic trading strategies. In this paper, we propose a jump-diffusion model for asset price movements that models price and its trend and allows a momentum strategy to be developed. Conditional on jump times, we derive closed-form transition densities for this model. We show how this allows us to extract a trend from high-frequency finance data by using a Rao-Blackwellized variable rate particle filter to filter incoming price data. Our results show that even in the presence of transaction costs our algorithm can achieve a Sharpe ratio above 1 when applied across a portfolio of 75 futures contracts at high frequency. ","SELECTED":null}
{"Title":"High-frequency trading model for a complex trading hierarchy","keywords (cleaned)":"Fractionally integrated processes;High-frequency data;Subordinated processes","Abstract":"Financial markets exhibit a complex hierarchy among different processes, e.g. a trading time marks the initiation of a trade, and a trade triggers a price change. High-frequency trading data arrive at random times. By combining stochastic and agent-based approaches, we develop a model for trading time, trading volume, and price changes. We generate intertrade time (time between successive trades) \u0394 ti, and the number of shares traded q(\u0394 ti) as two independent but power-law autocorrelated processes, where \u0394 ti is subordinated to q(\u0394 ti), and \u0394 ti is more strongly correlated than q(\u0394 ti). These two power-law autocorrelated processes are responsible for the emergence of strong power-law correlations in (a) the total number of shares traded N(\u0394T) and (b) the share volume Q \u0394T calculated as the sum of the number of shares q i traded in a fixed time interval \u0394T. We find that even though q(\u0394 ti) is weakly power-law correlated, due to strong power-law correlations in \u0394 ti, the (integrated) share volume Q(\u0394T) \u2261\u03a3 \u0394T i=1q(\u0394t i) exhibits strong long-range power-law correlations. We propose that intertrade times and bid-ask price changes share the same volatility mechanism, yielding the power-law autocorrelations in absolute values of price change and power-law tails in the distribution of price changes. The model generates the log-linear functional relationship between the average bid-ask spread \u3008S\u3009 \u0394T and the number of trade occurrences N \u0394T, and between \u3008S\u3009 \u0394T and Q \u0394T. We find that both results agree with empirical findings. ","SELECTED":null}
{"Title":"Acceleration of market value-at-risk estimation","keywords (cleaned)":"Computer hardware;D.1.3 [concurrent programming]: [parallel programming];Delta functions;Commerce;Probabilistic algorithm;Computer graphics equipment;Parallel programming;J.4 [social and behavioral sciences]: [economics];Risk perception;Concurrent programming;Behavioral science;Monte Carlo methods;Probability and statistics;MONTE CARLO;G.3 [probability and statistics]: [probabilistic algorithms (including Monte Carlo)];Estimation;Program processors;Value engineering","Abstract":"The proliferation of algorithmic trading, derivative usage and highly leveraged hedge funds necessitates the acceleration of market Value-at-Risk (VaR) estimation to measure the severity of portfolios losses. This paper demonstrates how solely relying on advances in computer hardware to accelerate market VaR estimation overlooks significant opportunities for acceleration. We use a simulation based delta-gamma Value-at-Risk (VaR) estimate and compute the loss function using basic linear algebra subroutines (BLAS). Our NVIDIA GeForce GTX280 graphics processing unit (GPU) based baseline implementation is a straight-forward port from the CPU implementation and only had a 8.21x speed advantage over a quadcore Intel Core2 Q9300 central processing unit (CPU) based implementation. We demonstrate three approaches to gain additional speedup over the baseline GPU implemention. Firstly, we reformulate the loss function to reduce the amount of necessary computation and achieved a 60.3x speedup. Secondly, we selected functionally equivalent distribution conversion modules to give the best convergence rate - providing an additional 2x speedup. Thirdly, we merged data-parallel computational kernels to remove redundant load store operations leading to an additional 1.85x speedup. Overall, we have achieved a speedup of 148x against the baseline GPU implementation, reducing the time of a VaR estimation with a standard error of 0.1% from minutes to less than one second. Copyright 2009 ACM.","SELECTED":null}
{"Title":"Financial time-series data analysis using deep convolutional neural networks","keywords (cleaned)":"Data visualization;Convolution;Intelligent robots;Financial data processing;Technical indicator;Artificial intelligence;Algorithmic trading;Simulation applications;Neural networks;Data handling;Trend prediction;Financial time series;Electronic trading;Feature representation;Decision support systems;Cloud computing;Learning frameworks;Learning algorithms;Deep learning;Network function virtualization;Commerce;Convolutional neural network;Machine learning;Finance;Learning systems;Visual languages;Data visualization;Trend prediction;Convolutional neural network;Deep neural networks;Big data;Time series analysis;Natural language processing systems","Abstract":"A novel financial time-series analysis method based on deep learning technique is proposed in this paper. In recent years, the explosive growth of deep learning researches have led to several successful applications in various artificial intelligence and multimedia fields, such as visual recognition, robot vision, and natural language processing. In this paper, we focus on the time-series data processing and prediction in financial markets. Traditional feature extraction approaches in intelligent trading decision support system are used to applying several technical indicators and expert rules to extract numerical features. The major contribution of this paper is to improve the algorithmic trading framework with the proposed planar feature representation methods and deep convolutional neural networks (CNN). The proposed system is implemented and benchmarked in the historical datasets of Taiwan Stock Index Futures. The experimental results show that the deep learning technique is effective in our trading simulation application, and may have greater potentialities to model the noisy financial data and complex social science problems. In the future, we expected that the proposed methods and deep learning framework could be applied to more innovative applications in the next financial technology (FinTech) generation. ","SELECTED":null}
{"Title":"Incorporating order-flow into optimal execution","keywords (cleaned)":"Acquisition;Algorithmic trading;High frequency trading;Liquidity;Order-flow;Price impact","Abstract":"We provide an explicit closed-form strategy for an investor who executes a large order when market order-flow from all agents, including the investor\u2019s own trades, has a permanent price impact. The strategy is found in closed-form when the permanent and temporary price impacts are linear in the market\u2019s and investor\u2019s rates of trading. We do this under very general assumptions about the stochastic process followed by the order-flow of the market. The optimal strategy consists of an Almgren\u2013Chriss execution strategy adjusted by a weighted-average of the future expected net order-flow (given by the difference of the market\u2019s rate of buy and sell market orders) over the execution trading horizon and proportional to the ratio of permanent to temporary linear impacts. We use historical data to calibrate the model to Nasdaq traded stocks and use simulations to show how the strategy performs. ","SELECTED":null}
{"Title":"MAPLE: A scalable architecture for maintaining packet latency measurements","keywords (cleaned)":"bloom filter;latency;High-performance computing;approximation;Packet latencies;Scalable architectures;Algorithmic trading;Computer software selection and evaluation;Network architecture;Sub-populations;Data structures;Computer architecture;Network Monitoring;Network operator;bloom filter","Abstract":"Latency has become an important metric for network monitoring since the emergence of new latency-sensitive applications (e.g., algorithmic trading and high-performance computing). To satisfy the need, researchers have proposed new architectures such as LDA and RLI that can provide fine-grained latency measurements. However, these architectures are fundamentally ossified in their design as they are designed to provide only a specific pre-configured aggregate measurement - -either average latency across all packets (LDA) or per-flow latency measurements (RLI). Network operators, however, need latency measurements at both finer (e.g., packet) as well as flexible (e.g., flow subsets) levels of granularity. To bridge this gap, we propose an architecture called MAPLE that essentially stores packet-level latencies in routers and allows network operators to query the latency of arbitrary traffic sub-populations. MAPLE is built using scalable data structures with small storage needs (uses only 12.8 bits\/packet), and uses a novel mechanism to reduce the query bandwidth significantly (by a factor of 17 compared to the naive method of sending packet queries individually). ","SELECTED":null}
{"Title":"Analysis of the intraday effects of economic releases on the currency market","keywords (cleaned)":"Economic release;Foreign exchange;High frequency;Volatility estimation;Wavelet","Abstract":"Using four years of second-by-second executed trade data, we study the intraday effects of a representative group of scheduled economic releases on three exchange rates: EUR\/USD, JPY\/USD, and GBP\/USD. Using wavelets to analyze volatility behavior, we empirically show that intraday volatility clusters increase as we approach the time of the releases, and decay exponentially after the releases. Moreover, we compare our results with the results of a poll that we conducted of economists and traders. Finally, we propose a wavelet volatility estimator which is not only more efficient than a range estimator that is commonly used in empirical studies, but also captures the market dynamics as accurately as a range estimator. Our approach has practical value in high-frequency algorithmic trading, as well as electronic market making. ","SELECTED":null}
{"Title":"Big Data and algorithmic governance: the case of financial practices","keywords (cleaned)":"Algorithms;credit scoring;governance approach;bank capital adequacy;capital;Algorithms;actor network theory;banking;High-frequency trading;Data sets;Big data;trade","Abstract":"Big Data and algorithmic governance are transforming traditional institutions and media of transnational governance in manners that hold important implications for power, accountability and effectiveness. Drawing on actor-network theory, this paper contrasts utopian or dystopian views on the increasing presence of Big Data in contemporary financial practices. We scrutinise the emerging impacts of Big Data in the public governance of private banks in the Basel III arrangements, private governance of individual actors in credit scoring and anarchic competitive governance of markets in high-frequency trading. Our findings reveal varied and emergent forms of governance through, with and by algorithms. ","SELECTED":null}
{"Title":"Backlash Agent: A trading strategy based on Directional Change","keywords (cleaned)":"Electronic trading;Time interval;Commerce;Price changes;Artificial intelligence;Algorithmic trading;Bid and ask;Trading strategies;Directional changes;FX trading;Directional changes;Costs;Profitability;Price movement","Abstract":"Directional Change (DC) is a technique to summarize price movements in a financial market. According to the DC concept, data is sampled only when the magnitude of price change is significant according to the investor. Unlike with time series, DC samples data at irregular time intervals. In this paper, we propose a contrarian trading strategy that is based on the DC concept. We examine the profitability of our trading strategy using three currency pairs: EUR\/CHF, GBP\/CHF and EUR\/USD. The results show that our proposed trading strategy is profitable with Alpha over than 10 in some cases. However, counting the bid and ask prices can decrease considerably the profits under particular settings. ","SELECTED":null}
{"Title":"Dynamic mode decomposition for financial trading strategies","keywords (cleaned)":"Dynamic mode decomposition;Dynamical systems;Equation-free;Financial trading;Koopman operator","Abstract":"We demonstrate the application of an algorithmic trading strategy based upon the recently developed dynamic mode decomposition on portfolios of financial data. The method is capable of characterizing complex dynamical systems, in this case financial market dynamics, in an equation-free manner by decomposing the state of the system into low-rank terms whose temporal coefficients in time are known. By extracting key temporal coherent structures (portfolios) in its sampling window, it provides a regression to a best fit linear dynamical system, allowing for a predictive assessment of the market dynamics and informing an investment strategy. The data-driven analytics capitalizes on stock market patterns, either real or perceived, to inform buy\/sell\/hold investment decisions. Critical to the method is an associated learning algorithm that optimizes the sampling and prediction windows of the algorithm by discovering trading hot-spots. The underlying mathematical structure of the algorithms is rooted in methods from nonlinear dynamical systems and shows that the decomposition is an effective mathematical tool for data-driven discovery of market patterns. ","SELECTED":null}
{"Title":"The synchronized and long-lasting structural change on commodity markets: Evidence from high frequency data","keywords (cleaned)":"Commodity;Cross-Market Linkages;Financialization;High frequency;Structural change","Abstract":"This paper analyses the co-movements between the US stock market and several commodity futures between 1998 and 2011. It computes dynamic conditional correlations at (i) 1-hour, (II) 5-minute, (iii) 10-second, and (iv) 1-second frequencies and documents a synchronized structural break, characterized by correlations that have significantly departed from zero to positive territories, since late September 2008. Our results support the idea that high frequency trading and algorithmic strategies have an effect on the behaviour of commodity prices.","SELECTED":null}
{"Title":"Self-evolving trading strategy integrating Internet of Things and big data","keywords (cleaned)":"Big data;Commerce;Financial markets;Hidden Markov models;Internet of things;Learning algorithms;Learning systems;Neural networks;Algorithm design and analysis;Algorithmic trading;BP neural networks;Commodity futures;Evaluation indicators;Predictive models;Price forecasting;Trading strategies;Electronic trading","Abstract":"In the era of Internet of Things (IoT) and big data, data has increased dramatically. Computers have been used in various fields. Algorithmic trading is beginning to develop rapidly in the trading market, more and more algorithms begin to be used in the transaction market. As a form of machine learning, neural network can fully reveal the complex trading market. Based on the characteristics of commodity futures market, this paper chooses back propagation neural network to establish price forecasting model. And then, according to the rules of futures market, a self-evolving commodity futures trading strategy is proposed. We also use the data of the Shanghai Futures Exchange and the Dalian Futures Exchange to back-testing the strategy. Finally, we compare the proposed strategies and traditional strategies, and illustrate the evolution of our strategy. Experiments show that our strategies are superior to other compared strategies in the proposed evaluation indicator. Our strategy has a good performance both in yield and risk. It also proves the feasibility of the model and the strategy. The research of this paper is significant to the research of the futures market, and it also provides a new idea for the application of machine learning in algorithmic trading. ","SELECTED":null}
{"Title":"Frequency effects on predictability of stock returns","keywords (cleaned)":"Frequency effect;Stock returns","Abstract":"We propose that predictability is linked with profitability in a complex manner. We look at ways to measure predictability of price changes using information theoretic approach and employ them on historical data for NYSE 100 stocks. This allows us to determine whether frequency of sampling price changes affects the predictability of those. We also study relations between price changes predictability and the deviation of the price formation processes from iid as well as the stock's sector. We also briefly comment on the complicated relationship between predictability of price changes and the profitability of algorithmic trading. ","SELECTED":null}
{"Title":"Reflecting on the VPIN dispute","keywords (cleaned)":"Flash crash;High-frequency trading;Order flow toxicity;Order imbalance;PIN;VIX;Volatility forecasting;VPIN","Abstract":"In Andersen and Bondarenko (2014), using tick data for S&P 500 futures, we establish that the VPIN metric of Easley, L\u00f3pez de Prado, and O'Hara (ELO), by construction, will be correlated with trading volume and return volatility (innovations). Whether VPIN is more strongly correlated with volume or volatility depends on the exact implementation. Hence, it is crucial for the interpretation of VPIN as a harbinger of market turbulence or as a predictor of short-term volatility to control for current volume and volatility. Doing so, we find no evidence of incremental predictive power of VPIN for future volatility. Likewise, VPIN does not attain unusual extremes prior to the flash crash. Moreover, the properties of VPIN are strongly dependent on the underlying trade classification. In particular, using more standard classification techniques, VPIN behaves in the exact opposite manner of what is portrayed in ELO (2011a, 2012a). At a minimum, ELO should rationalize this systematic reversal as the classification becomes more closely aligned with individual transactions.ELO (2014) dispute our findings. This note reviews the econometric methodology and the market microstructure arguments behind our conclusions and responds to a number of inaccurate assertions. In addition, we summarize fresh empirical evidence that corroborates the hypothesis that VPIN is largely driven, and significantly distorted, by the volume and volatility innovations. Furthermore, we note there is compelling new evidence that transaction-based classification schemes are more accurate than the bulk volume strategies advocated by ELO for constructing VPIN. In fact, using perfect classification leads to diametrically opposite results relative to ELO (2011a, 2012a). ","SELECTED":null}
{"Title":"Informational linkages between dark and lit trading venues","keywords (cleaned)":"Algorithmic trading;Crossing networks;Information transmission;Price discovery","Abstract":"We examine the linkages between dark and lit venues using a proprietary data set. We find that algorithmic trades for less liquid stocks are correlated with higher spreads and price impact, as well as contemporaneous trading on the lit venues. Also, signed trades for these stocks predict future returns over the next 15-120 minutes. Trades for liquid stocks, trades by the dark venue brokerage desk, and trades of large blocks transmit less information to lit venues. The results suggest informed agents split orders using algorithms across dark and lit trading venues, with lit orders providing some price discovery. ","SELECTED":null}
{"Title":"Internal IT load profile variability","keywords (cleaned)":"Daily fluctuations;Data centers;High-frequency trading;Internet activity;Internet traffic;Load profiles;Return on investments;Trading Supported;Commerce;Profitability;Internet","Abstract":"There is a significant opportunity for engineers to deliver great value and great return on investment by right sizing for the IT load in data centers even though it is a challenging task. The physical manifestation of the Internet is server-based one with most of those servers residing in data centers. Some common drops in activity occur when there is a major event that diverts attention away from the Internet such as big sporting events like the Superbowl, World Cup Final and the Olympics. A daily fluctuation in Internet activity commonly occurs with the stock market, with some data centers supporting high-frequency trading, others provide domestic exchanges only. Depending on the type of trading supported, the data center may only see high Internet traffic for eight hours of any given day.","SELECTED":null}
{"Title":"The information content of a limit order book: The case of an FX market","keywords (cleaned)":"Algorithmic trading;High-frequency data;Limit order book;Profitability","Abstract":"In this paper we examine the question of whether knowledge of the information contained in a limit order book helps to provide economic value in a simple trading scheme. Given the greater information content of the order book, over simple price information, it might naturally be expected that the order book would dominate. Using Dollar Sterling tick data, we find that despite the in-sample statistical significance of variables describing the structure of the limit order book in explaining tick-by-tick returns, they do not consistently add significant economic value out-of-sample. We show this using a simple linear model to determine trading activity, as well as a model-free genetic algorithm based on price, order flow, and order book information. We also find that the profitability of all trading rules based on genetic algorithms dropped substantially in 2008 compared to 2003 data. ","SELECTED":null}
{"Title":"Scaling properties and universality of first-passage-time probabilities in financial markets","keywords (cleaned)":"Algorithmic trading;Correlated dynamics;Financial markets;First-passage;Futures market;High frequency data;Large datasets;Markovian dynamics;Non-Gaussian;Phenomenological description;Risk controls;Scaling properties;Single curves;Standard deviation;Student distribution;Survival probabilities;Tick-by-tick data;Time event;Time properties;Weibull;Commerce;Dynamics;Finance;Probability;Weibull distribution","Abstract":"Financial markets provide an ideal frame for the study of crossing or first-passage time events of non-Gaussian correlated dynamics, mainly because large data sets are available. Tick-by-tick data of six futures markets are herein considered, resulting in fat-tailed first-passage time probabilities. The scaling of the return with its standard deviation collapses the probabilities of all markets examined-and also for different time horizons-into single curves, suggesting that first-passage statistics is market independent (at least for high-frequency data). On the other hand, a very closely related quantity, the survival probability, shows, away from the center and tails of the distribution, a hyperbolic t -1 \/2 decay typical of a Markovian dynamics, albeit the existence of memory in markets. Modifications of the Weibull and Student distributions are good candidates for the phenomenological description of first-passage time properties under certain regimes. The scaling strategies shown may be useful for risk control and algorithmic trading. ","SELECTED":null}
{"Title":"Learning to trade with incremental support vector regression experts","keywords (cleaned)":"Computational finance;Subspace tracking;Commerce;Education;Artificial intelligence;Algorithmic trading;Incremental support vector regression;Vectors;Regression analysis;Algorithms;Ensemble learning","Abstract":"Support vector regression (SVR) is an established non-linear regression technique that has been applied successfully to a variety of predictive problems arising in computational finance, such as forecasting asset returns and volatilities. In real-time applications with streaming data two major issues that need particular care are the inefficiency of batch-mode learning, and the arduous task of training the learning machine in presence of non-stationary behavior. We tackle these issues in the context of algorithmic trading, where sequential decisions need to be made quickly as new data points arrive, and where the data generating process may change continuously with time. We propose a master algorithm that evolves a pool of on-line SVR experts and learns to trade by dynamically weighting the experts' opinions. We report on risk-adjusted returns generated by the hybrid algorithm for two large exchange-traded funds, the iShare S&P 500 and Dow Jones EuroStoxx 50. ","SELECTED":null}
{"Title":"Streamsight - A visualization tool for large-scale streaming applications","keywords (cleaned)":"Dynamic natures;Visualization;Performance optimizations;Large-scale streaming;Environment monitoring;Visualizing topologies;Dynamic visualization;Distributed processing;Sheer complexity;Scientific applications;Planning;Performance counters;Distributed parameter networks;Visualization tools;Computing paradigms;Fraud detections;Execution process;Trend analysis;Business intelligences;Financial markets;Streaming applications;Adaptive behaviors;Non-trivial;Applications;Data-flow graphs;Capacity planning;Data flow analysis;Distributed environments;Dynamic behaviors;Topology;Stream processing visualization;Trading strategies","Abstract":"Stream processing is becoming a new and important computing paradigm. Innovative streaming applications are being developed in areas ranging from scientific applications (e.g., environment monitoring), to business intelligence (e.g., fraud detection and trend analysis), to financial markets (e.g., algorithmic trading strategies). Developing, understanding, debugging, and optimizing streaming applications is non-trivial because of the adaptive and dynamic nature of these applications. The sheer complexity and the distributed character of a large number of cooperating components hosted on a distributed environment further complicate matters. In this paper we describe Streamsight, a new visualization tool built to examine, monitor, and help understand the dynamic behavior of streaming applications. Previously developed stream processing visualization tools focus solely on composition of dataflow graphs. Streamsight's novelty hinges on a wide range of capabilities, including the ability to manage the dynamics of large and evolving topologies comprising multiple streaming applications with thousands of nodes and interconnections. From rendering live performance counters using different perspectives to allowing recordings and replays of the execution process, Streamsight provides the mechanisms that permit a better understanding of the evolving and adaptive behavior of streaming applications. These capabilities are used for debugging purposes, for performance optimization, and management of resources, including capacity planning. More than 50 developers, both inside and outside IBM, have been using Streamsight. ","SELECTED":null}
{"Title":"Does algorithmic trading reduce information acquisition?","keywords (cleaned)":"algorithm|amount of information|trading|algorithmic trading|information acquisition|prices|stock|r|acquisition|efficiency|disclosure|information|market|price efficiency|asset prices|standard deviation|market data","Abstract":"I demonstrate an important tension between acquiring information and incorporating it into asset prices. As a salient case, I analyze algorithmic trading (AT), which is typically associated with improved price efficiency. Using a new measure of the information content of prices and a comprehensive panel of 54,879 stock-quarters of Securities and Exchange Commission (SEC) market data, I establish instead that the amount of information in prices decreases by 9% to 13% per standard deviation of AT activity and up to a month before scheduled disclosures. AT thus may reduce price informativeness despite its importance for translating available information into prices. ","SELECTED":null}
{"Title":"Patterns for Distributed Real-Time Stream Processing","keywords (cleaned)":"Twitter;Big data;Stream processing;Commerce;Time factors;Big data;Real-time patterns;Social networking (online);Storms;Big data applications;Stream processing;Computational model;Quality of service;Real time;Real time systems","Abstract":"In recent years, big data systems have become an active area of research and development. Stream processing is one of the potential application scenarios of big data systems where the goal is to process a continuous, high velocity flow of information items. High frequency trading (HFT) in stock markets or trending topic detection in Twitter are some examples of stream processing applications. In some cases (like, for instance, in HFT), these applications have end-to-end quality-of-service requirements and may benefit from the usage of real-time techniques. Taking this into account, the present article analyzes, from the point of view of real-time systems, a set of patterns that can be used when implementing a stream processing application. For each pattern, we discuss its advantages and disadvantages, as well as its impact in application performance, measured as response time, maximum input frequency and changes in utilization demands due to the pattern. ","SELECTED":null}
{"Title":"Robust Median Reversion Strategy for Online Portfolio Selection","keywords (cleaned)":"Electronic trading;L 1 -median;Portfolio selection;Mean reversion;Commerce;Artificial intelligence;Mean reversion;Online learning;Learning systems;Online learning;L <sub>1<\/sub> -median;robust median reversion;Data mining","Abstract":"Online portfolio selection has attracted increasing attention from data mining and machine learning communities in recent years. An important theory in financial markets is mean reversion, which plays a critical role in some state-of-the-art portfolio selection strategies. Although existing mean reversion strategies have been shown to achieve good empirical performance on certain datasets, they seldom carefully deal with noise and outliers in the data, leading to suboptimal portfolios, and consequently yielding poor performance in practice. In this paper, we propose to exploit the reversion phenomenon by using robust L 1 -median estimators, and design a novel online portfolio selection strategy named 'Robust Median Reversion' (RMR), which constructs optimal portfolios based on the improved reversion estimator. We examine the performance of the proposed algorithms on various real markets with extensive experiments. Empirical results show that RMR can overcome the drawbacks of existing mean reversion algorithms and achieve significantly better results. Finally, RMR runs in linear time, and thus is suitable for large-scale real-time algorithmic trading applications. ","SELECTED":null}
{"Title":"A Primer for Financial Engineering: Financial Signal Processing and Electronic Trading","keywords (cleaned)":"Bridges;Commerce;MATLAB;Microstructure;Risk analysis;Risk assessment;Risk management;Signal processing;Engineering perspective;Engineering principles;Financial engineering;High-frequency trading;Market microstructure;Mathematical Finance;Risk analysis and management;Visual representations;Finance","Abstract":"This book bridges the fields of finance, mathematical finance and engineering, and is suitable for engineers and computer scientists who are looking to apply engineering principles to financial markets. The book builds from the fundamentals, with the help of simple examples, clearly explaining the concepts to the level needed by an engineer, while showing their practical significance. Topics covered include an in depth examination of market microstructure and trading, a detailed explanation of High Frequency Trading and the 2010 Flash Crash, risk analysis and management, popular trading strategies and their characteristics, and High Performance DSP and Financial Computing. The book has many examples to explain financial concepts, and the presentation is enhanced with the visual representation of relevant market data. It provides relevant MATLAB codes for readers to further their study. \u2022 Provides engineering perspective to financial problems \u2022 In depth coverage of market microstructure \u2022 Detailed explanation of High Frequency Trading and 2010 Flash Crash \u2022 Explores risk analysis and management \u2022 Covers high performance DSP & financial computing. ","SELECTED":null}
{"Title":"Gaussian process-based algorithmic trading strategy identification","keywords (cleaned)":"Algorithmic trading;Behavioural finance;Gaussian Processes;High-frequency trading;Inverse reinforcement learning;Markov decision process;Support vector machines","Abstract":"Many market participants now employ algorithmic trading, commonly defined as the use of computer algorithms, to automatically make certain trading decisions, submit orders and manage those orders after submission. Identifying and understanding the impact of algorithmic trading on financial markets has become a critical issue for market operators and regulators. Advanced data feeds and audit trail information from market operators now allow for the full observation of market participants\u2019 actions. A key question is the extent to which it is possible to understand and characterize the behaviour of individual participants from observations of trading actions. In this paper, we consider the basic problems of categorizing and recognizing traders (or, equivalently, trading algorithms) on the basis of observed limit orders. These problems are of interest to regulators engaged in strategy identification for the purposes of fraud detection and policy development. Methods have been suggested in the literature for describing trader behaviour using classification rules defined over a feature space consisting of summary trading statistics of volume and inventory, along with derived variables that reflect the consistency of buying or selling behaviour. Our principal contribution is to suggest an entirely different feature space that is constructed by inferring key parameters of a sequential optimization model that we take as a surrogate for the decision-making process of the traders. In particular, we model trader behaviour in terms of a Markov decision process. We infer the reward (or objective) function for this process from observations of trading actions using a process from machine learning known as inverse reinforcement learning (IRL). The reward functions learned through IRL then constitute a feature space that can be the basis for supervised learning (for classification or recognition of traders) or unsupervised learning (for categorization of traders). Making use of a real-world data-set from the E-Mini futures contract, we compare two principal IRL variants, linear IRL and Gaussian Process IRL, against a method based on summary trading statistics. Results suggest that IRL-based feature spaces support accurate classification and meaningful clustering. Further, we argue that, because they attempt to learn traders\u2019 underlying value propositions under different market conditions, the IRL methods are more informative and robust than the summary statistic-based approach and are well suited for discovering new behaviour patterns of market participants. ","SELECTED":null}
{"Title":"Configurable hardware-based streaming architecture using Online Programmable-Blocks","keywords (cleaned)":"Computer hardware;Computer hardware description languages;Field programmable gate arrays (FPGA);General purpose computers;Information filtering;Intrusion detection;Query processing;Reconfigurable architectures;Reconfigurable hardware;Social networking (online);Algorithmic trading;Complex event processing;Configurable hardware;General purpose processors;Specialized hardware;Streaming applications;Streaming architecture;Targeted advertising;Hardware","Abstract":"The limitations of traditional general-purpose processors have motivated the use of specialized hardware solutions (e.g., FPGAs) to achieve higher performance in stream processing. However, state-of-the-art hardware-only solutions have limited support to adapt to changes in the query workload. In this work, we present a reconfigurable hardware-based streaming architecture that offers the flexibility to accept new queries and to change existing ones without the need for expensive hardware reconfiguration. We introduce the Online Programmable Block (OP-Block), a \"Lego-like\" connectable stream processing element, for constructing a custom Flexible Query Processor (FQP), suitable to a wide range of data streaming applications, including real-time data analytics, information filtering, intrusion detection, algorithmic trading, targeted advertising, and complex event processing. Through evaluations, we conclude that updating OP-Blocks to support new queries takes on the order of nano to micro-seconds (e.g., 40 ns to realize a join operator on an OP-Block), a feature critical to support of streaming applications on FPGAs. ","SELECTED":null}
{"Title":"Knowledge discovery in dynamic data using neural networks","keywords (cleaned)":"Calculation cost;Pattern recognition;Commerce;Automated trading systems;Key characteristics;Pattern recognition systems;Neural networks;Automated trading systems;Backpropagation training;ADALINE;Dynamic data;Training patterns;Trading systems;Knowledge discovery;Data mining","Abstract":"This article aims at knowledge discovery in dynamic data via classification based on neural networks. In our experimental study we have used three different types of neural networks based on Hebb, Adaline and backpropagation training rules. Our goal was to discover important market (Forex) patterns which repeatedly appear in the market history. Developed classifiers based upon neural networks should effectively look for the key characteristics of the patterns in dynamic data. We focus on reliability of recognition made by the described algorithms with optimized training patterns based on the reduction of the calculation costs. To interpret the data from the analysis we created a basic trading system and trade all recommendations provided by the neural network. ","SELECTED":null}
{"Title":"High frequency trading portfolio optimisation: Integration of financial and human factors","keywords (cleaned)":"Intelligent systems;Investments;Financial data processing;Commerce;Sample sizes;High frequency;Systems analysis;Time series analysis;Optimization;Sample sizes;portfolio optimisation;High frequency trading;regularization;Human engineering;Markowitz;portfolio optimisation","Abstract":"To use human factors together with financial ones in portfolio management task we analyze lengthy series of successes and losses of numerous automated high frequency trading systems that buy and sell assets. We found that in spite of sparse, bimodal non-Gaussian time series, modern Markowitz solutions can be applied to weigh up contributions of diverse trading strategies. Training history should be rather short in situations where technological, social, financial, economic and political situations are changing swiftly. The Markowitz portfolio coefficients finding algorithm can be improved by careful application of the regularization and matrix structurization methods. ","SELECTED":null}
{"Title":"Trend following algorithms in automated derivatives market trading","keywords (cleaned)":"Trend following;Futures contract;Human intervention;Runtimes;Stock market;Financial forecasting;Profitability;Futures contract;Market trends;Academic research;Automated trading systems;Commerce;Market fluctuations;Automation;Mechanical trading;Human judgments;Algorithms;Automated trading;Software program;Correct strategy","Abstract":"Trend following (TF) is trading philosophy by which buying\/selling decisions are made solely according to the observed market trend. For many years, many manifestations of TF such as a software program called Turtle Trader, for example, emerged in the industry. Surprisingly little has been studied in academic research about its algorithms and applications. Unlike financial forecasting, TF does not predict any market movement; instead it identifies a trend at early time of the day, and trades automatically afterwards by a pre-defined strategy regardless of the moving market directions during run time. Trend following trading has been popular among speculators. However it remains as a trading method where human judgment is applied in setting the rules (aka the strategy) manually. Subsequently the TF strategy is executed in pure objective operational manner. Finding the correct strategy at the beginning is crucial in TF. This usually involves human intervention in first identifying a trend, and configuring when to place an order and close it out, when certain conditions are met. In this paper, we evaluated and compared a collection of TF algorithms that can be programmed in a computer system for automated trading. In particular, a new version of TF called trend recalling model is presented. It works by partially matching the current market trend with one of the proven successful patterns from the past. Our experiments based on real stock market data show that this method has an edge over the other trend following methods in profitability. The results show that TF however is still limited by market fluctuation (volatility), and the ability to identify trend signal. ","SELECTED":null}
{"Title":"Regime-switching recurrent reinforcement learning for investment decision making","keywords (cleaned)":"r|transaction cost|portfolio|models|switching|automated trading|Regime-switching|portfolio management|Engineering|decision making|management|recurrent reinforcement learning|management applications|investment decision making|finance|R|regime-switching|financial time series|time series|trading|applications|costs|Financial Engineering|stochastic control|algorithm|reinforcement|transaction costs|edi|investment|mes|reinforcement learning|switch|volatility|performance","Abstract":"This paper presents the regime-switching recurrent reinforcement learning (RSRRL) model and describes its application to investment problems. The RSRRL is a regime-switching extension of the recurrent reinforcement learning (RRL) algorithm. The basic RRL model was proposed by Moody and Wu (Proceedings of the IEEE\/IAFE 1997 on Computational Intelligence for Financial Engineering (CIFEr). IEEE, New York, pp 300-307 1997) and presented as a methodology to solve stochastic control problems in finance. We argue that the RRL is unable to capture all the intricacies of financial time series, and propose the RSRRL as a more suitable algorithm for such type of data. This paper gives a description of two variants of the RSRRL, namely a threshold version and a smooth transition version, and compares their performance to the basic RRL model in automated trading and portfolio management applications. We use volatility as an indicator\/transition variable for switching between regimes. The out-of-sample results are generally in favour of the RSRRL models, thereby supporting the regime-switching approach, but some doubts exist regarding the robustness of the proposed models, especially in the presence of transaction costs. ","SELECTED":null}
{"Title":"Extreme returns: The case of currencies","keywords (cleaned)":"Crash risk;Exchange rates;Fat tails;High frequency;Jump process;Microstructure;Value-at-risk","Abstract":"Financial market crashes can occur even in the absence of news. This paper highlights four properties of price-contingent trading that increase the frequency of such events. Price-contingent trading is common across financial market, since it includes algorithmic trading, technical trading, and dynamic option hedging. The four properties we consider are: (1) high kurtosis in the distribution of order sizes; (2) clustering of trades within the day; (3) clustering of trades at certain prices; and (4) feedback between trading and returns. The paper estimates the relative importance of these factors using data from the foreign exchange market. Calibrated simulations indicate that interactions among these factors are at least as important as any single one. Among individual factors, the orders' size distribution and feedback effects have the strongest influence. Overall, price-contingent trading could account for half of realized excess kurtosis. The paper suggests that extreme returns unaccompanied by news are statistically inevitable in the presence of price-contingent trading. ","SELECTED":null}
{"Title":"DBToaster: Agile views in a dynamic data management system","keywords (cleaned)":"Algorithmic trading;Data analytics;Data management system;Dynamic data management;Dynamic data structure;High frequency HF;Incremental view maintenance;ITS architecture;Lightweight systems;Parallelizations;Scaling-up;Agile manufacturing systems;Conference calls;Data structures;Reconnaissance aircraft;Information management","Abstract":"This paper calls for a new breed of lightweight systems - dynamic data management systems (DDMS). In a nutshell, a DDMS manages large dynamic data structures with agile, frequently fresh views, and provides a facility for monitoring these views and triggering application-level events. We motivate DDMS with applications in large-scale data analytics, database monitoring, and high-frequency algorithmic trading. We compare DDMS to more traditional data management systems architectures. We present the DBToaster project, which is an ongoing effort to develop a prototype DDMS system. We describe its architecture design, techniques for high-frequency incremental view maintenance, storage, scaling up by parallelization, and the various key challenges to overcome to make DDMS a reality.","SELECTED":null}
{"Title":"Application of stochastic recurrent reinforcement learning to index trading","keywords (cleaned)":"Algorithmic trading;Higher frequencies;Recurrent reinforcement learning;Stock indices;Commerce;Genetic programming;Neural networks;Stochastic systems;Reinforcement learning","Abstract":"A novel stochastic adaptation of the recurrent reinforcement learning (RRL) methodology is applied to daily, weekly, and monthly stock index data, and compared to results obtained elsewhere using genetic programming (GP). The data sets used have been a considered a challenging test for algorithmic trading. It is demonstrated that RRL can reliably outperform buy-and-hold for the higher frequency data, in contrast to GP which performed best for monthly data.","SELECTED":null}
{"Title":"An e-market framework for informed trading","keywords (cleaned)":"Multi agent systems;e-procurement;Trading agent;Market reliability;Intelligent agents;Software prototyping;Virtual corporation;Core technology;Human agent;World Wide Web;Societies and institutions;Automation;Information management;Virtual institutions;E-Market;Electronic market;Marketing;Trading processes;Trading agent;E-trading;Electronic commerce;e-procurement;Automated trading;Mining;Data mining;Intelligent virtual agents","Abstract":"Fully automated trading, such as e-procurement, using the Internet is virtually unheard of today. Three core technologies are needed to fully automate the trading process: data mining, intelligent trading agents and virtual institutions in which informed trading agents can trade securely both with each other and with human agents in a natural way. This paper describes a demonstrable prototype e-trading system that integrates these three technologies and is available on the World Wide Web. This is part of a larger project that aims to make informed automated trading a reality.","SELECTED":null}
{"Title":"Market intraday momentum","keywords (cleaned)":"High frequency trading;Intraday;momentum;Overnight return;prediction","Abstract":"Based on high frequency S & P 500 exchange-traded fund (ETF) data from 1993\u20132013, we show an intraday momentum pattern: the first half-hour return on the market as measured from the previous day's market close predicts the last half-hour return. This predictability, which is both statistically and economically significant, is stronger on more volatile days, on higher volume days, on recession days, and on major macroeconomic news release days. Intraday momentum also exists for ten other most actively traded domestic and international ETFs. Theoretically, the intraday momentum is consistent not only with Bogousslavsky's (2016) model of infrequent portfolio rebalancing but also with a model of late-informed trading near the market close. ","SELECTED":null}
{"Title":"An artificial neural network-based stock trading system using technical analysis and big data framework","keywords (cleaned)":"artificial neural network;Buy-and-hold strategy;Stock market;Financial markets;Financial data processing;Network layers;Technical analysis;Artificial neural network models;Commerce;Electronic trading;Algorithmic trading;Big data;Neural networks;Time series analysis;Strategy optimization;Multi layer perceptron;Financial time series;Stock price prediction","Abstract":"In this paper, a neural network-based stock price prediction and trading system using technical analysis indicators is presented. The model developed first converts the financial time series data into a series of buy-sell-hold trigger signals using the most commonly preferred technical analysis indicators. Then, a Multilayer Perceptron (MLP) artificial neural network (ANN) model is trained in the learning stage on the daily stock prices between 1997 and 2007 for all of the Dow30 stocks. Apache Spark big data framework is used in the training stage. The trained model is then tested with data from 2007 to 2017. The results indicate that by choosing the most appropriate technical indicators, the neural net- work model can achieve comparable results against the Buy and Hold strategy in most of the cases. Furthermore, fine tuning the technical indicators and\/or optimization strategy can enhance the overall trading performance. Copyright 2017 ACM.","SELECTED":null}
{"Title":"Insider trading and market structure","keywords (cleaned)":"HFT|Insider trading|securities market|information|high frequency trading (HFT)|r|Securities market|insider trading|algorithms|informed trader|speed|R|trading|algorithmic trading|high frequency|market|market structure|algorithm|Article|prices|informed traders|ace|high frequency trading|trade","Abstract":"This Article argues that the emergence of algorithmic trading raises a significant challenge for the law and policy of insider trading. It shows that securities markets are dominated by a cohort of \"structural insiders,\" namely a set of traders able to utilize close physical and informational access to trade at speeds measured in milliseconds and microseconds, a practice loosely termed high frequency trading (HFT). By virtue of speed and physical proximity to exchanges, these HFT traders can systematically gain frst access to new information, trade on it, and change prices before the rest of the market can see its content. This Article makes three contributions. First, it introduces and develops the concept of structural insider trading. Securities markets increasingly rely on automated traders utilizing algorithms- or pre-programmed electronic instructions-for trading. Policy allows traders to enjoy important structural advantages: (i) to physically locate on or next to an exchange, shortening the time it takes for information to travel to and from the marketplace; and (ii) to receive feeds of richly detailed data directly to these co-located trading operations. With algorithms sophisticated enough to respond instantly and independently to new information, co-located automated traders can receive and trade on not-fully-public information ahead of other investors. Indeed, by the time that the rest of the market sees this information, it has long since become out-of-date. Secondly, this Article shows that structural insider trading exhibits harms that are substantially similar to those regulated under conventional theories of corporate insider trading. Structural insiders place other investors at a persistent informational disadvantage. Through their frst sight of marketmoving data, structural insiders can capture the best trades and Erode the profts of informed traders, reducing their incentives to participate in the marketplace. Despite the similarity in harms, however, this Article shows that current doctrine does not apply to restrict structural insider trading. Rather, structural insiders thrive in full public view, and with regulatory permission. Thirdly, this Article explores the implications of structural insider trading for the theory and doctrine of insider trading. It shows them to be increasingly incoherent in their application. In protecting investors against one set of insiders but not another, law and policy appear under profound strain in the face of innovative markets.","SELECTED":null}
{"Title":"Communication-efficient distributed online prediction by dynamic model synchronization","keywords (cleaned)":"algorithm|algorithms|online prediction|trading|algorithmic trading|information|high frequency data|high frequency|model synchronization|r|edi|prediction|data stream|data streams|concept drift|models|applications|performance|prediction-based","Abstract":"We present the first protocol for distributed online prediction that aims to minimize online prediction loss and network communication at the same time. This protocol can be applied wherever a prediction-based service must be provided timely for each data point of a multitude of high frequency data streams, each of which is observed at a local node of some distributed system. Exemplary applications include social content recommendation and algorithmic trading. The challenge is to balance the joint predictive performance of the nodes by exchanging information between them, while not letting communication overhead deteriorate the responsiveness of the service. Technically, the proposed protocol is based on controlling the variance of the local models in a decentralized way. This approach retains the asymptotic optimal regret of previous algorithms. At the same time, it allows to substantially reduce network communication, and, in contrast to previous approaches, it remains applicable when the data is non-stationary and shows rapid concept drift. We demonstrate empirically that the protocol is able to hold up a high predictive performance using only a fraction of the communication required by benchmark methods. ","SELECTED":null}
{"Title":"Sustainable economy inspired large-scale feed-forward portfolio construction","keywords (cleaned)":"Algorithmic trading;Automated trading;clustering;Markowitz;Mean-variance;Optimization;Portfolios;Portfolio optimization;regularization;Simulation","Abstract":"To understand large-scale portfolio construction tasks we analyse sustainable economy problems by splitting up large tasks into smaller ones and offer an evolutional feed-forward system-based approach. The theoretical justification for our solution is based on multivariate statistical analysis of multidimensional investment tasks, particularly on relations between data size, algorithm complexity and portfolio efficacy. To reduce the dimensionality\/sample size problem, a larger task is broken down into smaller parts by means of item similarity - clustering. Similar problems are given to smaller groups to solve. Groups, however, vary in many aspects. Pseudo randomly-formed groups compose a large number of modules of feed-forward decision-making systems. The evolution mechanism forms collections of the best modules for each single short time period. Final solutions are carried forward to the global scale where a collection of the best modules is chosen using a multiclass cost-sensitive perceptron. Collected modules are combined in a final solution in an equally weighted approach (1\/N Portfolio). The efficacy of the novel decision-making approach was demonstrated through a financial portfolio optimization problem, which yielded adequate amounts of real world data. For portfolio construction, we used 11,730 simulated trading robot performances. The dataset covered the period from 2003 to 2012 when environmental changes were frequent and largely unpredictable. Walk-forward and out-of-sample experiments show that an approach based on sustainable economy principles outperforms benchmark methods and that shorter agent training history demonstrates better results in periods of a changing environment. ","SELECTED":null}
{"Title":"Using conditional probability to identify trends in intra-day high-frequency equity pricing","keywords (cleaned)":"Stock predictions;High frequency HF;Probability;Commerce;Intraday trading;Bid and ask;Stock predictions;Conditional probability;Conditional probability;Traditional markets;Costs;High-frequency trading;Price movement","Abstract":"By examining the conditional probabilities of price movements in a popular US stock over different high-frequency intra-day timespans, varying levels of trend predictability are identified. This study demonstrates the existence of predictable short-term trends in the market; understanding the probability of price movement can be useful to high-frequency traders. Price movement was examined in trade-by-trade (tick) data along with temporal timespans between 1 s to 30 min for 52 one-week periods for one highly-traded stock. We hypothesize that much of the initial predictability of trade-by-trade (tick) data is due to traditional market dynamics, or the bouncing of the price between the stock's bid and ask. Only after timespans of between 5 to 10 s does this cease to explain the predictability; after this timespan, two consecutive movements in the same direction occur with higher probability than that of movements in the opposite direction. This pattern holds up to a one-minute interval, after which the strength of the pattern weakens. ","SELECTED":null}
{"Title":"The Science of Algorithmic Trading and Portfolio Management","keywords (cleaned)":"black box models|trading algorithms|Portfolio Management|r|Algorithmic Trading|electronic trading|data set|models|Trading|risk|trading system|trading models|algorithms|Management|market impact models|management|web|risk management|trading process|R|profitability|mathematics|trading|algorithmic trading|market|trading processes|data sets|market impact|Algorithm|market structure|algorithm|Portfolio|black box|trading systems|prices|knowledge|asset class|Portfolio management|profit|trade|performance","Abstract":"The Science of Algorithmic Trading and Portfolio Management, with its emphasis on algorithmic trading processes and current trading models, sits apart from others of its kind. Robert Kissell, the first author to discuss algorithmic trading across the various asset classes, provides key insights into ways to develop, test, and build trading algorithms. Readers learn how to evaluate market impact models and assess performance across algorithms, traders, and brokers, and acquire the knowledge to implement electronic trading systems. This valuable book summarizes market structure, the formation of prices, and how different participants interact with one another, including bluffing, speculating, and gambling. Readers learn the underlying details and mathematics of customized trading algorithms, as well as advanced modeling techniques to improve profitability through algorithmic trading and appropriate risk management techniques. Portfolio management topics, including quant factors and black box models, are discussed, and an accompanying website includes examples, data sets supplementing exercises in the book, and large projects. ","SELECTED":null}
{"Title":"Algorithmic trading engines versus human traders - Do they behave differently in securities markets?","keywords (cleaned)":"Human intervention;Market activity;Electronic market;Market volatility;Real-time market;Data sets;Placement strategy;Algorithms;Electronic commerce;Algorithmic trading;Securities market;Information systems;Securities trading;Electronic trading;Trading systems;Engines;Order submission;Quantitative models;Electronic market","Abstract":"After exchanges and alternative trading venues have introduced electronic execution mechanisms worldwide, the focus of the securities trading industry shifted to the use of fully electronic trading engines by banks, brokers and their institutional customers. These Algorithmic Trading engines enable order submissions without human intervention based on quantitative models applying historical and real-time market data. Although there is a widespread discussion on the pros and cons of Algorithmic Trading and on its impact on market volatility and market quality, little is known on how algorithms actually place their orders in the market and whether and in which respect this differs form other order submissions. Based on a dataset that - for the first time - includes a specific flag to enable the identification of orders submitted by Algorithmic Trading engines, the paper investigates the extent of Algorithmic Trading activity and specifically their order placement strategies in comparison to human traders in the Xetra trading system. It is shown that Algorithmic Trading has become a relevant part of overall market activity and that Algorithmic Trading engines fundamentally differ from human traders in their order submission, modification and deletion behavior as they exploit real-time market data and latest market movements.","SELECTED":null}
{"Title":"Material signals: A historical sociology of high-frequency trading","keywords (cleaned)":"algorithm|foreign exchange|algorithms|gate|article|HFT|trading|market|market participants|r|mes|high-frequency trading|high-frequency|futures","Abstract":"Drawing on interviews with 194 market participants (including 54 practitioners of high-frequency trading or HFT), this article first identifies the main classes of \u201csignals\u201d (patterns of data) that influence how HFT algorithms buy and sell shares and interact with each other. Second, it investigates historically the processes that have led to three of the most important categories of these signals, finding that they arise from three features of U.S. share trading that are the result of episodes of meso-level conflict. Third, the article demonstrates the contingency of these features by briefly comparing HFT in share trading to HFT in futures, Treasurys, and foreign exchange. The article thus argues that how HFT algorithms act and interact is a specific, contingent product not just of the current but also of the past interaction of people, organizations, algorithms, and machines. ","SELECTED":null}
{"Title":"Online evolving fuzzy rule-based prediction model for high frequency trading financial data stream","keywords (cleaned)":"Financial data;Intelligent systems;Electronic trading;Online systems;Recursively updating;Commerce;Online prediction;Forecasting;Finance;Online learning;Fuzzy rules;High frequency financial data stream;Fuzzy rule based systems;Data density;Fuzzy inference;Data communication systems","Abstract":"Analyzing and predicting the high frequency trading (HFT) financial data stream is very challenging due to the fast arrival times and large amount of the data samples. Aiming at solving this problem, an online evolving fuzzy rule-based prediction model is proposed in this paper. Because this prediction model is based on evolving fuzzy rule-based systems and a novel, simpler form of data density, it can autonomously learn from the live data stream, automatically build\/remove its rules and recursively update the parameters. This model responds quickly to all unpredictable sudden changes of financial data and re-adjusts itself to follow the new data pattern. Experimental results show the excellent prediction performance of the proposed approach with real financial data stream regardless of quick shifts of data patterns and frequent appearances of abnormal data samples. ","SELECTED":null}
{"Title":"Modelling high-frequency FX rate dynamics: A zero-delay multi-dimensional HMM-based approach","keywords (cleaned)":"Chains;Economics;Comparative analysis;Likelihood-based criteria;Parameter estimation;Japanese yen;Commerce;Evolution equations;Trading environments;Change of measure;Markov processes;Electronic trading;Markov chain;Multivariate HMM filtering;Zero-delay model;Zero delay;High-frequency trading;Hidden Markov models","Abstract":"We develop a zero-delay hidden Markov model (HMM) to capture the evolution of multivariate foreign exchange (FX) rate data under a frequent trading environment. Recursive filters for the Markov chain and pertinent quantities are derived, and subsequently employed to obtain estimates for model parameters. The rationale for zero-delay HMM hinges on the idea that with fast trading, available information must be incorporated immediately in the evolution equations of the financial variables being modelled. Our proposed model is compared with the usual one-step delay HMM, GARCH and random walk models using likelihood-based criteria and error-type metrics. Parameter estimation both under the static and dynamic settings are carried out as well as in the models used as benchmarks in a comparative analysis. Implementation details are provided. We include a numerical illustration of the methodology applied to the currency data on UK sterling pounds and US dollars both against the Japanese yen. Our empirical results demonstrate greater fitting capacity and forecasting power of the zero-delay HMM over the comparators included in our analysis. ","SELECTED":null}
{"Title":"Parallelizing High-Frequency Trading Applications by Using C++11 Attributes","keywords (cleaned)":"Parallel architectures;Fastflow;Fastflow;Codes (symbols);Parallel patterns;Computer programming;Commerce;Data stream processing;Multimedia services;Parallel programming;C++11 Attributes;Parallel patterns;Data handling;REPARA;Electronic trading;C++ (programming language);Data stream processing;High-frequency trading;High-frequency trading","Abstract":"With the wide diffusion of parallel architectures parallelism has become an indispensable factor in the application design. However, the cost of the parallelization process of existing applications is still too high in terms of time-to-development, and often requires a large effort and expertise by the programmer. The REPARA methodology consists in a systematic way to express parallel patterns by annotating the source code using C++11 attributes transformed automatically in a target parallel code based on parallel programming libraries (e.g. FastFlow, Intel TBB). In this paper we apply this approach in the parallelization of a real high-frequency trading application. The description shows the effectiveness of the approach in easily prototyping several parallel variants of the same code. We also propose an extension of a REPARA attribute to express a user-defined scheduling strategy, which makes it possible to design a high-throughput and low-latency parallelization of our code outperforming the other parallel variants in most of the considered test-cases. ","SELECTED":null}
{"Title":"Creative destruction in Wall Street's technological arms race: Evidence from patent data","keywords (cleaned)":"Chicago;High-frequency trading;Innovation;capital;United States Patent and Trademark Office;Financial markets;Commerce;Conventional measurements;Strategic acquisitions;United States;Illinois;Patents;infrastructure;Patents and inventions;Investments;Financial innovation;Capital markets;High frequency trading;Ecology;Market infrastructure;Financial markets","Abstract":"Technology and policy have transformed the market infrastructure of trading in capital markets and have helped financialize other markets, such as commodities trading. The associated 'technological arms race' has created a new market ecology which has made trading cheaper and faster but more volatile and fragmented. This paper charts the technological roots of this transformation from a conventional measurement of innovation perspective. We do so by employing content analysis techniques and extracting market infrastructure patent counts from the USPTO (United States Patent and Trademark Office) database for the period January 1976 to October 2013. From the resulting time series and a qualitative examination of patents we find that (1) the number of market infrastructure patents has dramatically increased since 1999, as confirmed by an associated structural break; (2) the new market ecology has, in true Schumpeterian style, been associated with a new breed of firms, most notably software firms and historically smaller brokerage firms that have invested heavily in technology internally and through strategic acquisitions; and (3) some incumbent firms have responded aggressively to the new market ecology, most notably the Chicago Mercantile Exchange and Goldman Sachs. We conclude that policymakers, regulators and academics wishing to further investigate the technological roots of recent changes in capital should refer to patent data. Our principal contribution is to highlight that Wall Street has been actively patenting market infrastructure innovations in a pattern consistent with claims that an associated 'technological arms race' started in the late 1990s. ","SELECTED":null}
{"Title":"Permutation approach, high frequency trading and variety of micro patterns in financial time series","keywords (cleaned)":"Micro patterns;Permutation approach;Financial data processing;Commerce;Micro patterns;High frequency trading;Higher frequencies;Financial time series;Exchange rates;Permutation entropy;High-frequency trading","Abstract":"Permutation approach is suggested as a method to investigate financial time series in micro scales. The method is used to see how high frequency trading in recent years has affected the micro patterns which may be seen in financial time series. Tick to tick exchange rates are considered as examples. It is seen that variety of patterns evolve through time; and that the scale over which the target markets have no dominant patterns, have decreased steadily over time with the emergence of higher frequency trading.","SELECTED":null}
{"Title":"Automated trading with machine learning on big data","keywords (cleaned)":"Keywords-machine learning;Economics;Classification (of information);Financial markets;Commerce;Stochastic systems;Logistic regressions;Artificial intelligence;Big data;Trading models;Investment funds;Learning systems;Logistic regressions;classification;Market impacts;Stochastic models;Data size;Automated trading;Profitability;Trading floors","Abstract":"Financial markets are now extremely efficient,nevertheless there are still many investment funds that generatealpha systematically beating markets' return benchmarks. Theemergence of big data gave professional traders the newterritory, leverage and evidence and renewed opportunitiesof their profitable exploitation by Machine Learning (ML)models, increasingly taking over the trading floor by 24\/7automated trading in response to the continuously fed datastreams. Rapidly increasing data sizes and strictly real-timerequirements of the trading models render large subset ofML methods intractable, overcomplex and impossible to applyin practise. In this work we demonstrate how to efficientlyapproach the problem of automated trading with large portfoliostrategy that continuously consumes streams of data acrossmultiple diverse markets. We demonstrate a simple scalabletrading model that learns to generate profit from multiple intermarketprice predictions and markets' correlation structure.We also introduce the stochastic trade diffusion technique tomaximise trading turnover while reducing strategy's exposureto market impact and construct the efficient risk-mitigatingportfolio that backtests with the strong positive return. ","SELECTED":null}
{"Title":"Real-time pricing and hedging of options on currency futures with artificial neural networks","keywords (cleaned)":"Hedging strategies;delta-hedging;Neural networks;Automated algorithms;Estimation problem;Electric network topology;High-frequency data;High-frequency trading;Neural networks;Commerce;Option pricing;Statistical tests;Algorithms;Option pricing;Economics;Investments;High frequency data;black model;Neural network topology;delta-hedging;Costs","Abstract":"High-frequency trading and automated algorithm impose high requirements on computational methods. We provide a model-free option pricing approach with neural networks, which can be applied to real-time pricing and hedging of FX options. In contrast to well-known theoretical models, an essential advantage of our approach is the simultaneous pricing across different strike prices and parsimonious use of real-time input variables. To test its ability for the purpose of high-frequency trading, we perform an empirical run-time trading simulation with a tick dataset of EUR\/USD options on currency futures of 4 weeks. In very short non-overlapping 15-minute out-of-sample intervals, theoretical option prices derived from the Black model compete against nonparametric option prices through two different neural network topologies. We show that the approximated pricing function of learning networks is suitable for generating fast run-time option pricing evaluation as their performance is slightly better in comparison to theoretical prices. The derivation of the network function is also useful for performing hedging strategies. We conclude that the performance of closed-form pricing models depends highly on the volatility estimator, whereas neural networks can avoid this estimation problem but require market liquidity for training. Nevertheless, we also have to take particular enhancements into account, which give us useful hints for further research and steps. Copyright ","SELECTED":null}
{"Title":"How to enable automated trading engines to cope with news-related liquidity shocks? Extracting signals from unstructured data","keywords (cleaned)":"Investments;Signal processing;Simulation;Costs;Liquidity;Commerce;e-Finance;Forecasting;Automation;E finances;Text mining;Engines;Automated trading;Data mining","Abstract":"Financial markets are characterised by high levels of complexity and non-linearity. Information systems have often been applied to support investors by forecasting price changes in securities markets. In addition to the asset price, liquidity represents another financial variable that has a high relevance for investors because it constitutes a main determinant of total transaction costs. Previous research has shown that the level of liquidity is affected by the publication of corporate disclosures. To derive an optimal order execution strategy that minimises the transaction costs, investors as well as automated trading engines must be able to anticipate changes in the available market liquidity. However, there is no research on how to forecast the impact of corporate disclosures on market liquidity. Therefore, we propose an IT artefact that allows automated trading engines to appropriately react to news-related liquidity shocks. The system indicates whether the publication of a regulatory corporate disclosure will be followed by a positive liquidity shock, i.e.; lower transaction costs compared to historical levels. Utilising text mining techniques, the content of the corporate disclosures is analysed to generate a trading signal. Furthermore, the trading signal is evaluated within a simulation-based use case that considers English and German corporate disclosures and is shown to be of economic value. ","SELECTED":null}
{"Title":"The intra-day performance of market timing strategies and trading systems based on Japanese candlesticks","keywords (cleaned)":"Empirical time series analysis;Technical trading;Trading strategies;Trading systems","Abstract":"We develop market timing strategies and trading systems to test the intra-day predictive power of Japanese candlesticks at the 5-minute interval on the 30 constituents of the DJIA index. Around a third of the candlestick rules outperform the buy-and-hold strategy at the conservative Bonferroni level. After adjusting for trading costs, however, just a few rules remain profitable. When we correct for data snooping by applying the SSPA test on double-or-out market timing strategies, no single candlestick rule beats the buy-and-hold strategy after transaction costs. We also design fully automated trading systems by combining the best-performing candlestick rules. No evidence of out-performance is found after transaction costs. Although Japanese candlesticks can somewhat predict intra-day returns on large US caps, we show that such predictive power is too limited for active portfolio management to outperform the buy-and-hold strategy when luck, risk, and trading costs are correctly measured. ","SELECTED":null}
{"Title":"Market Microstructure: Confronting Many Viewpoints","keywords (cleaned)":"r|research|market microstructure|quantitative finance|high-frequency data|electronic market|stylized facts|high-frequency trading|electronic markets|microstructure|finance|statistics|trading|market|arch|market impact|Market|price impact|costs|liquidity|Microstructure|financial prices|prices|edi|clustering|volatility|Market Microstructure|high-frequency","Abstract":"The latest cutting-edge research on market microstructure Based on the December 2010 conference on market microstructure, organized with the help of the Institut Louis Bachelier, this guide brings together the leading thinkers to discuss this important field of modern finance. It provides readers with vital insight on the origin of the well-known anomalous \"stylized facts\" in financial prices series, namely heavy tails, volatility, and clustering, and illustrates their impact on the organization of markets, execution costs, price impact, organization liquidity in electronic markets, and other issues raised by high-frequency trading. World-class contributors cover topics including analysis of high-frequency data, statistics of high-frequency data, market impact, and optimal trading. This is a must-have guide for practitioners and academics in quantitative finance. This edition first published in 2012 ","SELECTED":null}
{"Title":"Three decision making levels in portfolio management","keywords (cleaned)":"Financial data processing;Sample sizes;Artificial intelligence;portfolio optimisation;efficient frontier;Automated trading;Asset allocation;Automation;High frequency;Robots;systematic trading;regularization;Markowitz;Commerce;efficient frontier;Sample sizes;Profitability;Investments;High frequency trading;Automated trading;Asset allocation;portfolio optimisation;Decision making","Abstract":"To improve portfolio management process we suggest using profit histories of automated trading strategies instead of actual assets. Such history can be generated by simulating hundreds of automated trading strategies (robots). We developed three-level decision making system aimed to find the portfolio weights. At the first level, virtual robots trade the assets, at the second level we create virtual profit fusion agents that calculate weighted sums of the profit series created by the first level robots. At the third level, we rank the fusion agents, select a set of the best ones and construct the final portfolio. Experiments with real financial 2004-2011 years data confirm usefulness of the novel approach. ","SELECTED":null}
{"Title":"A generalized birth-death stochastic model for high-frequency order book dynamics","keywords (cleaned)":"Automated trading;Birth-death processes;First passage times;High frequency data;Limit order book","Abstract":"We use a generalized birth-death stochastic process to model the high-frequency dynamics of the limit order book, and illustrate it using parameters estimated from Level II data for a stock on the London Stock Exchange. A new feature of this model is that limit orders are allowed to arrive in multiple sizes, an important empirical feature of the order book. We can compute various quantities of interest without resorting to simulation, conditional on the state of the order book, such as the probability that the next move of the mid-price will be upward, or the probability, as a function of order size, that a limit ask order will be executed before a downward move in the mid-price. This generalizes the successful model of Cont et al. [Oper. Res., 2010, 58, 549-563] by means of a new technical approach to computing the distribution of first passage times. ","SELECTED":null}
{"Title":"Technology and market quality: The case of high frequency trading","keywords (cleaned)":"Investments;Data sets;Market quality;Commerce;IT investments;Technological innovation;Financial markets;High frequency;Capital markets;Market structure;Quality measures;High frequency trading;Algorithmic trading system;Economic aspects;Information systems;Trading systems;IT investments;Trading strategies;Technology","Abstract":"Technological innovations such as high frequency trading systems (HFT) and algorithmic trading systems have changed financial markets. We discuss technological and economic aspects of HFT and find that HFT have a major impact on all aspects of the internal market structure of exchanges. We use market quality measures on a unique dataset provided by NASDAQ in order to analyze the contribution of HFT to market quality. The empirical results are discussed in the context of external and internal factors of market quality. Our results indicate considerable differences in trading strategies: HFT engage in market making strategies and provide liquidity when it is expensive and demand liquidity when it is cheap. Their trades are more informed than non-HFT trades for stocks with a high market capitalization and therefore make prices more informative, but less informed across the entire sample.","SELECTED":null}
{"Title":"Demo: fpga-ToPSS - Line-speed event processing on FPGAs","keywords (cleaned)":"Financial data;High frequency HF;Financial data processing;Algorithmic trading;Publish\/Subscribe system;Field programmable gate arrays (FPGA);Software architecture;Reconfigurable hardware;Boolean expressions;Toronto;Quantum chemistry;Digital signal processing;Financial strategies;Complex event processing;Publish\/subscribe;Complex event processing;Event-processing engine;Publish\/subscribe;Processing platform;Low-latency;FPGA","Abstract":"In this demo, we present fpga-ToPSS (a member of Toronto Publish\/Subscribe System Family), an efficient event processing platform for high-frequency and low-latency algorithmic trading. Our event processing platform is built over reconfigurable hardware - FPGAs - to achieve line-rate processing. Furthermore, our event processing engine supports Boolean expression matching with an expressive predicate language that models complex financial strategies to autonomously mimic the buying and the selling of stocks based on real-time financial data. ","SELECTED":null}
{"Title":"Threshold recurrent reinforcement learning model for automated trading","keywords (cleaned)":"Financial data;Recurrent reinforcement learning;Financial data processing;Differential Sharpe ratio;Regime switching;Commerce;Time series;Automation;Indicator variables;Nonlinearities;Nonlinearities;Switching;Financial time series;Reinforcement;Learning algorithms;Automated trading;Structural break;Reinforcement learning;Trading strategies;Regime-switching","Abstract":"This paper presents the threshold recurrent reinforcement learning (TRRL) model and describes its application in a simple automated trading system. The TRRL is a regime-switching extension of the recurrent reinforcement learning (RRL) algorithm. The basic RRL model was proposed by Moody and Wu (1997) and used for uncovering trading strategies. We argue that the RRL is not sufficiently equipped to capture the non-linearities and structural breaks present in financial data, and propose the TRRL model as a more suitable algorithm for such environments. This paper gives a detailed description of the TRRL and compares its performance with that of the basic RRL model in a simple automated trading framework using daily data from four well-known European indices. We assume a frictionless setting and use volatility as an indicator variable for switching between regimes. We find that the TRRL produces better trading strategies in all the cases studied, and demonstrate that it is more apt at finding structure in non-linear financial time series than the standard RRL. ","SELECTED":null}
{"Title":"Adaptive Neuro-Fuzzy inference system for financial trading using Intraday Seasonality Observation Model","keywords (cleaned)":"Observation model;Event-based;Financial data processing;Time series;Seasonality;Fuzzy systems;Financial time series;Efficient market hypothesis;New model;Financial forecasting;Pattern recognition;Pattern recognition systems;Fuzzy reasoning;High frequency;Tracking (position);Fuzzy inference;Expert systems;Commerce;Fuzzy neural networks;Finance;Training phase;Intraday Seasonality Observation Model;Adaptive neuro-fuzzy inference system;Adaptive neuro-fuzzy inference system;Financial trading;High frequency trading","Abstract":"The prediction of financial time series is a very complicated process. If the efficient market hypothesis holds, then the predictability of most financial time series would be a rather controversial issue, due to the fact that the current price contains already all available information in the market. This paper extends the Adaptive Neuro Fuzzy Inference System for High Frequency Trading which is an expert system that is capable of using fuzzy reasoning combined with the pattern recognition capability of neural networks to be used in financial forecasting and trading in high frequency. However, in order to eliminate unnecessary input in the training phase a new event based volatility model was proposed. Taking volatility and the scaling laws of financial time series into consideration has brought about the development of the Intraday Seasonality Observation Model. This new model allows the observation of specific events and seasonalities in data and subsequently removes any unnecessary data. This new event based volatility model provides the ANFIS system with more accurate input and has increased the overall performance of the system.","SELECTED":null}
{"Title":"Algorithmic trading system: Design and applications","keywords (cleaned)":"News retrieval;Design;System design;User strategies;Financial data processing;Commerce;Algorithmic trading;Current system;Key issues;Systems analysis;Research and development;Algorithmic trading system;Overall design;Algorithms;Design and application;Portfolio optimization;Decision making","Abstract":"This paper provides an overview of research and development in algorithmic trading and discusses key issues involved in the current effort on its improvement, which would be of great value to traders and investors. Some current systems for algorithmic trading are introduced, together with some illustrations of their functionalities. We then present our platform named FiSim and discuss its overall design as well as some experimental results in user strategy comparisons. ","SELECTED":null}
{"Title":"A parallel workflow for real-time correlation and clustering of high-frequency stock market Data","keywords (cleaned)":"Correlation methods;Parallel algorithms;Real time systems;Scalability;Systems analysis;Clique-based clustering;Electronic exchanges;Financial industry;Real-time stock market;Data reduction","Abstract":"We investigate the design and implementation of a parallel workflow environment targeted towards the financial industry. The system performs real-time correlation analysis and clustering to identify trends within streaming high-frequency intra-day trading data. Our system utilizes state-of-the-art methods to optimize the delivery of computationally-expensive real-time stock market data analysis, with direct applications in automated\/algorithmic trading as well as knowledge discovery in high-throughput electronic exchanges. This paper describes the design of the system including the key online parallel algorithms for robust correlation calculation and clique-based clustering using stochastic local search. We evaluate the performance and scalability of the system, followed by a preliminary analysis of the results using data from the Toronto Stock Exchange. ","SELECTED":null}
{"Title":"Arbitrage pricing theory-based Gaussian temporal factor analysis for adaptive portfolio management","keywords (cleaned)":"Gaussian noise (electronic);Arbitrage pricing theory;Data reduction;Portfolio optimization;Temporal factor analysis;Parameter estimation;Performance;Downside risk;Decision support systems;Upside volatility;Risk assessment;Optimization;Learning systems;Sharpe ratio;Mathematical models;Decision making","Abstract":"Ever since the inception of Markowitz's modern portfolio theory, static portfolio optimization techniques were gradually phased out by dynamic portfolio management due to the growth of popularity in automated trading. In view of the intensive computational needs, it is common to use machine learning approaches on Sharpe ratio maximization for implementing dynamic portfolio optimization. In the literature, return-based approaches which directly used security prices or returns to control portfolio weights were often used. Inspired by the arbitrage pricing theory (APT), some other efforts concentrate on indirect modelling using hidden factors. On the other hand, with regard to the proper risk measure in the Sharpe ratio, downside risk was considered a better substitute for variance. In this paper, we investigate how the Gaussian temporal factor analysis (TFA) technique can be used for portfolio optimization. Since TFA is based on the classical APT model and has the benefit of removing rotation indeterminacy via temporal modelling, using TFA for portfolio management allows portfolio weights to be indirectly controlled by several hidden factors. Moreover, we extend the approach to some other variants tailored for investors according to their investment objectives and degree of risk tolerance. ","SELECTED":null}
{"Title":"Algorithmic financial trading with deep convolutional neural networks: Time series to image conversion approach","keywords (cleaned)":"Convolution;Stock market;Financial markets;Financial forecasting;Image processing;Technical analysis;Deep neural networks;Commerce;Time series;Convolutional neural network;Deep convolutional neural networks;Algorithmic trading;Neural networks;Algorithmic trading models;Computational intelligence techniques;Financial trading system;Electronic trading;Deep learning;Convolutional neural network","Abstract":"Computational intelligence techniques for financial trading systems have always been quite popular. In the last decade, deep learning models start getting more attention, especially within the image processing community. In this study, we propose a novel algorithmic trading model CNN-TA using a 2-D convolutional neural network based on image processing properties. In order to convert financial time series into 2-D images, 15 different technical indicators each with different parameter selections are utilized. Each indicator instance generates data for a 15 day period. As a result, 15 \u00d7 15 sized 2-D images are constructed. Each image is then labeled as Buy, Sell or Hold depending on the hills and valleys of the original time series. The results indicate that when compared with the Buy & Hold Strategy and other common trading systems over a long out-of-sample period, the trained model provides better results for stocks and ETFs. ","SELECTED":null}
{"Title":"Mean-Reverting Portfolio with Budget Constraint","keywords (cleaned)":"Signal processing;Problem solving;Numerical methods;Algorithmic trading;Cointegration;Mean reversion;Optimization;Electronic trading;Portfolios;Portfolio optimization;Cointegration;Budget control;majorization;Pairs trading;quantitative trading;nonconvex optimization;Industry;Signal processing algorithms;Algorithm design and analysis;Statistical arbitrage;Design;Financial markets;Mean reversion;Algorithmic trading;Commerce;Statistical arbitrage;Pairs trading;Investments;nonconvex optimization;Materials requirements planning","Abstract":"This paper considers the mean-reverting portfolio (MRP) design problem arising from statistical arbitrage (a.k.a. pairs trading) in the financial markets. It aims at designing a portfolio of underlying assets by optimizing the mean reversion strength of the portfolio, while taking into consideration the portfolio variance and an investment budget constraint. Several specific design problems are considered based on different mean reversion criteria. Efficient algorithms are proposed to solve the problems. Numerical results on both synthetic and market data show that the proposed MRP design methods can generate consistent profits and outperform the traditional design methods and the benchmark methods in the literature. ","SELECTED":null}
{"Title":"Cryptocurrency portfolio management with deep reinforcement learning","keywords (cleaned)":"Convolution;Algorithmic trading;deep reinforcement learning;Neural networks;Electronic trading;Decision making process;quantitative finance;deterministic policy gradient;Portfolio managements;Financial assets;Learning algorithms;Deep learning;Financial investments;Intelligent systems;Algorithmic trading;cryptocurrency;Electronic money;Convolutional neural network;Learning systems;Portfolio managements;Reinforcement learning;Machine learning;Investments;Convolutional neural network;Policy gradient;Portfolio selection;Decision making","Abstract":"Portfolio management is the decision-making process of allocating an amount of fund into different financial investment products. Cryptocurrencies are electronic and decentralized alternatives to government-issued money, with Bitcoin as the best-known example of a cryptocurrency. This paper presents a model-less convolutional neural network with historic prices of a set of financial assets as its input, outputting portfolio weights of the set. The network is trained with 0.7 years' price data from a cryptocurrency exchange. The training is done in a reinforcement manner, maximizing the accumulative return, which is regarded as the reward function of the network. Back test trading experiments with trading period of 30 minutes is conducted in the same market, achieving 10-fold returns in 1.8 month's periods. Some recently published portfolio selection strategies are also used to perform the same back tests, whose results are compared with the neural network. The network is not limited to cryptocurrency, but can be applied to any other financial markets. ","SELECTED":null}
{"Title":"Enhancing trading strategies with order book signals","keywords (cleaned)":"adverse selection;Algorithmic trading;High-frequency trading;Market making;order flow;Order imbalance","Abstract":"We use high-frequency data from the Nasdaq exchange to build a measure of volume imbalance in the limit order (LO) book. We show that our measure is a good predictor of the sign of the next market order (MO), i.e., buy or sell, and also helps to predict price changes immediately after the arrival of an MO. Based on these empirical findings, we introduce and calibrate a Markov chain-modulated pure jump model of price, spread, LO and MO arrivals and volume imbalance. As an application of the model, we pose and solve a stochastic control problem for an agent who maximizes terminal wealth, subject to inventory penalties, by executing trades using LOs. We use in-sample-data (January to June 2014) to calibrate the model to 11 equities traded in the Nasdaq exchange and use out-of-sample data (July to December 2014) to test the performance of the strategy. We show that introducing our volume imbalance measure into the optimization problem considerably boosts the profits of the strategy. Profits increase because employing our imbalance measure reduces adverse selection costs and positions LOs in the book to take advantage of favourable price movements. ","SELECTED":null}
{"Title":"Using a Genetic Algorithm to Improve Recurrent Reinforcement Learning for Equity Trading","keywords (cleaned)":"Algorithmic trading;Artificial intelligence;Genetic algorithms;Indicator selections;Recurrent reinforcement learning;Sharpe ratio","Abstract":"Recurrent reinforcement learning (RRL) has been found to be a successful machine learning technique for building financial trading systems. In this paper, we use a genetic algorithm (GA) to improve the trading results of a RRL-type equity trading system. The proposed trading system takes the advantage of GA\u2019s capability to select an optimal combination of technical indicators, fundamental indicators and volatility indicators for improving out-of-sample trading performance. In our experiment, we use the daily data of 180 S&P stocks (from the period January 2009 to April 2014) to examine the profitability and the stability of the proposed GA-RRL trading system. We find that, after feeding the indicators selected by the GA into the RRL trading system, the out-of-sample trading performance improves as the number of companies with a significantly positive Sharpe ratio increases. ","SELECTED":null}
{"Title":"Stock price prediction based on stock-specific and sub-industry-specific news articles","keywords (cleaned)":"Forecasting;Prediction accuracy;Text mining;Electronic trading;financial news;Financial forecasting;financial news;multiple kernel learning;Stock price prediction;Financial markets;Commerce;Information analysis;Investment management;Algorithms;Stock price prediction;Text mining;Investments;Price prediction models;multiple kernel learning;Costs;Data mining","Abstract":"Accurate forecasting of upcoming trends in the capital markets is extremely important for algorithmic trading and investment management. Before making a trading decision, investors estimate the probability that a certain news item will influence the market based on the available information. Speculation among traders is often caused by the release of a breaking news article and results in price movements. Publications of news articles influence the market state that makes them a powerful source of data in financial forecasting. Recently, researchers have developed trend and price prediction models based on information extracted from news articles. However, to date no previous research that investigates the advantages of using news articles with different levels of relevance to the target stock has been conducted. This research study uses the multiple kernel learning technique to effectively combine information extracted from stock-specific and sub-industry-specific news articles for prediction of an upcoming price movement. News articles are divided into these two categories based on their relevance to a targeted stock and analyzed by separate kernels. The experimental results show that utilizing two categories of news improves the prediction accuracy in comparison with methods based on a single news category. ","SELECTED":null}
{"Title":"Stock volatility modelling with augmented GARCH model with jumps","keywords (cleaned)":"Explanatory power;Commerce;News analytics;Trading volumes;Auto-regressive;Numerical index;Automated trading systems;Stock volatility modelling;Log likelihood;GARCH models","Abstract":"Knowing the characteristics of news in numerical indices one can use them in mathematical and statistical models and automated trading systems. Currently, the tools of the news analytics have been increasingly used by traders in the U.S. and Europe. The interest in news analytics is related to the ability to predict changes of prices, volatility and trading volume on the stock market. The emphasis of the paper is on assessing the added value of using news analytics data in improving the explanatory power of the GARCH-Jump model. Based on empirical evidences for some of FTSE100 companies, the paper examines two GARCH models with jumps. First we consider the well-known GARCH model with autoregressive conditional jump intensity proposed in [1]. Then we introduce the GARCH-Jumps model augmented with news intensity and obtain some empirical results. The main assumption of the model is that jump intensity might change over time and that jump intensity depends linearly on the number of news (the news intensity). The comparison of the values of log likelihood supports the hypothesis of impact of news on the jump intensity of volatility.","SELECTED":null}
{"Title":"Indicator selection for daily equity trading with recurrent reinforcement learning","keywords (cleaned)":"Recurrent reinforcement learning;Econometric analysis;Genetic algorithms;Machine learning techniques;Indicator selections;Technical analysis;Commerce;Lower frequencies;Genetic algorithms;Fundamental analysis;Costs;Reinforcement learning;High-frequency trading","Abstract":"Recurrent reinforcement learning (RRL), a machine learning technique, is very successful in training high frequency trading systems. When trading analysis of RRL is done with lower frequency financial data, e.g. daily stock prices, the decrease of autocorrelation in prices may lead to a decrease in trading profit. In this paper, we propose a RRL trading system which utilizes the price information, jointly with the indicators from technical analysis, fundamental analysis and econometric analysis, to produce long\/short signals for daily trading. In the proposed trading system, we use a genetic algorithm as a pre-screening tool to search suitable indicators for RRL trading. Moreover, we modify the original RRL parameter update scheme in the literature for out-of-sample trading. Empirical studies are conducted based on data sets of 238 S&P stocks. It is found that the trading performance concerning the out-of-sample daily Sharpe ratios turns better: the number of companies with a positive and significant Sharpe ratio increases after feeding the selected indicators jointly with prices information into the RRL system.","SELECTED":null}
{"Title":"A general framework for time-aware decision support systems","keywords (cleaned)":"Decision support systems;Temporal knowledge;tOWL;Decision support systems;Commerce;Knowledge based systems;Artificial intelligence;Temporal reasoning;Data handling;Knowledge representation;Automated trading;Market recommendations;Automata theory","Abstract":"In this paper we present a general framework for time-aware decision support systems. The framework uses the state-of-the-art tOWL language for the representation of temporal knowledge and enables temporal reasoning over the information that is represented in a knowledge base. Our approach uses state-of-the-art Semantic Web technology for handling temporal data. Through such an approach, the designer of a system can focus on the application intelligence rather than enforcing\/checking data related restrictions manually. Also, there is an increased support for reuse of temporal reasoning tools across applications. We illustrate the applicability of our framework by building a market recommendations aggregation system. This system automatically collects market recommendations from online sources and, based on the past performance of the analysts that issued a recommendation, generates an aggregated recommendation in the form of a buy, hold, or sell advice. We illustrate the flexibility of our proposed system by implementing multiple methods for the aggregation of market recommendations. ","SELECTED":null}
{"Title":"Discrete portfolio optimisation for large scale systematic trading applications","keywords (cleaned)":"Covariance matrix;Algorithmic trading;Optimization;portfolio optimisation;forward selection;Covariance matrix;mean variance optimisation;large scale;Portfolio construction;Automation;systematic trading;Markowitz;comgen;forward selection;Algorithmic trading;Commerce;Biomedical engineering;Heuristic methods;Automated trading systems;Information science;heuristic;Optimisations;Automated trading systems;portfolio optimisation","Abstract":"Markowitz's mean-variance portfolio optimisation is not suitable for a large number of assets due to the unacceptably slow quadratic optimisation procedure involved. This is particularly important in systematic\/algorithmic\/ automated trading applications where instead of assets, automated trading systems are used. We propose a much faster heuristic approach that scales linearly rather than the quadratic scaling in the Markowitz method. Moreover, our proposed approach, Comgen, is on average better than the Markowitz approach when applied to unseen data. Additionally, Comgen always finds a solution, whereas the Markowitz procedure occasionally fails as the covariance matrix is not always positive-semidefinite. In an empirical study of a \u223c2000 day history, we demonstrate the benefits of this novel approach by using \u223c3200 time series produced by automatic trading systems. We perform a 3 year walk-forward analysis and show that in most of the 12\u03013=36 months out of the sample periods, this novel approach produces a better Sharpe ratio than the Markowitz approach, at the same time being a thousand times faster (and 2400 times faster if number of assets is 4000). ","SELECTED":null}
{"Title":"How do individual investors trade?","keywords (cleaned)":"Foreign exchange market;monitoring effect;order flow;trading activity data set","Abstract":"This paper examines how high-frequency trading decisions of individual investors are influenced by past price changes. Specifically, we address the question as to whether decisions to open or close a position are different when investors already hold a position compared with when they do not. Based on a unique data set from an electronic foreign exchange trading platform, OANDA FXTrade, we find that investors' future order flow is (significantly) driven by past price movements and that these predictive patterns last up to several hours. This observation clearly shows that for high-frequency trading, investors rely on previous price movements in making future investment decisions. We provide clear evidence that market and limit orders flows are much more predictable if those orders are submitted to close an existing position than if they are used to open one. We interpret this finding as evidence for the existence of a monitoring effect, which has implications for theoretical market microstructure models and behavioral finance phenomena, such as the endowment effect. ","SELECTED":null}
{"Title":"Evaluation of a high-volume, low-latency market data processing system implemented with IBM middleware","keywords (cleaned)":"Performance evaluation;Performance optimizations;Algorithmic trading;Current system;Data handling;Optimization;Wireless sensor networks;Data volume;System optimizations;Hardware and software;Market-maker;Commodity hardware;Performance optimizations;Commerce;Middleware;Commodity hardware;Market data;Transport technology;Specialized software;Stock market;Low latency;Risk analysis;IBM middleware;Hardware platform;Market surveillance;Hardware;Market data processing","Abstract":"A stock market data processing system that can handle high data volumes at low latencies is critical to market makers. Such systems play a critical role in algorithmic trading, risk analysis, market surveillance, and many other related areas. The current systems tend to use specialized software and custom processors. We show that such a system can be built with general-purpose middleware and run on commodity hardware. The middleware we use is IBM System S which includes transport technology from IBM WebSphere MQ Low Latency Messaging (LLM). Our performance evaluation consists of two parts. First, we determined the effectiveness of each system optimization that the hardware and software infrastructure makes available. These optimizations were implemented at all software levels-application, middleware, and operating system. Second, we evaluated our system on different hardware platforms. ","SELECTED":null}
{"Title":"Examination of the profitability of technical analysis based on moving average strategies in BRICS","keywords (cleaned)":"Automated trading systems;BRICS;Moving average strategies;Portfolio analysis;Technical analysis","Abstract":"In this paper, we investigated the profitability of technical analysis as applied to the stock markets of the BRICS member nations. In addition, we searched for evidence that technical analysis and fundamental analysis can complement each other in these markets. To implement this research, we created a comprehensive portfolio containing the assets traded in the markets of each BRICS member. We developed an automated trading system that simulated transactions in this portfolio using technical analysis techniques. Our assessment updated the findings of previous research by including more recent data and adding South Africa, the latest member included in BRICS. Our results showed that the returns obtained by the automated system, on average, exceeded the value invested. There were groups of assets from each country that performed well above the portfolio average, surpassing the returns obtained using a buy and hold strategy. The returns from the sample portfolio were very strong in Russia and India. We also found that technical analysis can help fundamental analysis identify the most dynamic companies in the stock market. ","SELECTED":null}
{"Title":"Tensor representation in high-frequency financial data for price change prediction","keywords (cleaned)":"Artificial intelligence;Commerce;Correlation theory;Costs;Tensors;Automatic algorithms;High frequency HF;High-frequency trading;Large-scale dataset;Multilinear models;Price prediction;Tensor representation;Transaction records;Electronic trading","Abstract":"Nowadays, with the availability of massive amount of trade data collected, the dynamics of the financial markets pose both a challenge and an opportunity for high frequency traders. In order to take advantage of the rapid, subtle movement of assets in High Frequency Trading (HFT), an automatic algorithm to analyze and detect patterns of price change based on transaction records must be available. The multichannel, time-series representation of financial data naturally suggests tensor-based learning algorithms. In this work, we investigate the effectiveness of two multilinear methods for the mid-price prediction problem against other existing methods. The experiments in a large scale dataset which contains more than 4 millions limit orders show that by utilizing tensor representation, multilinear models outperform vector-based approaches and other competing ones. ","SELECTED":null}
{"Title":"Insider Trading 2.0? The Ethics of Information Sales","keywords (cleaned)":"Consumer Sentiment Index;High-frequency trading;Insider trading;Market segmentation;Price discrimination","Abstract":"The sale of faster access to financial market data has recently generated public controversy. NY Attorney General Eric Schneiderman has referred to such fast data feeds as \u201cInsider Trading 2.0\u201d. For example, Thomson Reuters sold the University of Michigan\u2019s Consumer Sentiment Index to computerized trading firms 2\u00a0seconds before releasing its data to its other paying clients. This paper explores the ethical issues involved in the sale of such information. Is selling faster access ethically the same as traditional insider trading, which generally involves a breach of fiduciary duty or the use of misappropriated information? Such practices are extremely different from traditional insider trading as there is neither a breach of fiduciary duty nor misappropriation of inside information. The ethical issues are similar to other market segmentation and price discrimination issues, in which different prices are charged to different customers. The ability to price discriminate across segments can actually benefit large segments of the population who may receive lower prices because others, such as the high-speed traders, are paying more. The sale of faster access to information, especially by exchanges, raises additional ethical issues. There may be adverse effects on market quality that must be addressed. The moral distaste for the practice expressed by some stems from the seeming unfairness of a modern market structure that provides advantages to a small group of computerized traders. ","SELECTED":null}
{"Title":"A stacked generalization system for automated FOREX portfolio trading","keywords (cleaned)":"Algorithmic trading;Forex forecasting;Machine learning;Portfolio managements;Stacked generalization","Abstract":"Multiple FOREX time series forecasting is a hot research topic in the literature of portfolio trading. To this end, a large variety of machine learning algorithms have been examined. However, it is now widely understood that, in real-world trading settings, no single machine learning model can consistently outperform the alternatives. In this work, we examine the efficacy and the feasibility of developing a stacked generalization system, intelligently combining the predictions of diverse machine learning models. Our approach establishes a novel inferential framework that comprises the following levels of data processing: (i) We model the dependence patterns between major currency pairs via a diverse set of commonly used machine learning algorithms, namely support vector machines (SVMs), random forests (RFs), Bayesian autoregressive trees (BART), dense-layer neural networks (NNs), and na\u00efve Bayes (NB) classifiers. (ii) We generate implied signals of exchange rate fluctuation, based on the output of these models, as well as appropriate side information obtained by analyzing the correlations across currency pairs in our training datasets. (iii) We finally combine these implied signals into an aggregate predictive waveform, by leveraging majority voting, genetic algorithm optimization, and regression weighting techniques. We thoroughly test our framework in real-world trading scenarios; we show that our system leads to significantly better trading performance than the considered benchmarks. Thus, it represents an attractive solution for financial firms and corporations that perform foreign exchange portfolio management and daily trading. Our system can be used as an integrated part in international commercial trade activities or in a quantitative investing framework for algorithmic trading and carry-trade speculation. ","SELECTED":null}
{"Title":"Effects of lit and dark market fragmentation on liquidity","keywords (cleaned)":"Algorithmic trading;Dark trading;Fragmentation;Internalization;Liquidity;Multilateral Trading Facility (MTF);OTC trading","Abstract":"Based on data from eight stock exchanges and a trade reporting facility for London Stock Exchange- and Euronext-listed equities, I investigate how lit and dark market fragmentation affects liquidity. Neither dark trading nor fragmentation between lit order books is found to harm liquidity. Lit fragmentation improves spreads and depth across markets and locally on the primary exchange, or at worst does not affect them. Benefits are greater for large stocks and stocks with less electronic trading. Lit fragmentation however harms the depth of small stocks. The adverse effects on the depth of large stocks result from algorithmic trading, not fragmentation. ","SELECTED":null}
{"Title":"Trade size, high-frequency trading, and colocation around the world","keywords (cleaned)":"Colocation;High-frequency trading;International market microstructure;Trade size","Abstract":"We examine the impact of changes in market microstructure, particularly algorithmic trading (AT) and high-frequency trading (HFT), on trade size across 24 stock exchanges around the world. Using colocation services as a proxy for AT and HFT, we find mixed results on the impact of AT and HFT on the average trade size. Furthermore, we test whether the presence of HFT leads to the introduction of colocation services. The data are consistent with the view that HFT pre-dates colocation by at least eight months on most exchanges, and has strong power in explaining the introduction of colocation services. In effect, our results show that colocation services do not properly measure effective AT and HFT; rather, colocation services are the result of HFT. Exchanges choose to offer colocation services due to the fact HFT requires higher speed transactions. Finally, we show there have been substantial changes in trade size in other countries such as China where there is no HFT and offer explanations for these changes and suggest avenues for future research. ","SELECTED":null}
{"Title":"Where has my time gone?","keywords (cleaned)":"Electronic trading;Losses;Mobile telecommunication systems;Application level;Application performance;End to end latencies;High-frequency trading;Holistic approach;Latency control;Quality of experience (QoE);User experience;Quality of service","Abstract":"Time matters. In a networked world, we would like mobile devices to provide a crisp user experience and applications to instantaneously return results. Unfortunately, application performance does not depend solely on processing time, but also on a number of different components that are commonly counted in the overall system latency. Latency is more than just a nuisance to the user, poorly accounted-for, it degrades application performance. In fields such as high frequency trading, as well as in many data centers, latency translates easily to financial losses. Research to date has focused on specific contributions to latency: from improving latency within the network to latency control on the application level. This paper takes an holistic approach to latency, and aims to provide a break-down of end-to-end latency from the application level to the wire. Using a set of crafted experiments, we explore the many contributors to latency. We assert that more attention should be paid to the latency within the host, and show that there is no silver bullet to solve the end-to-end latency challenge in data centers. We believe that a better understanding of the key elements influencing data center latency can trigger a more focused research, improving the user\u2019s quality of experience. ","SELECTED":null}
{"Title":"Continuous skyline queries on multicore architectures","keywords (cleaned)":"Network management;Parallel implementations;Risk management;Data handling;Optimization;Computer architecture;Electronic trading;Load balancing strategy;Software architecture;Data stream processing;Multicore architectures;Data stream processing;High-frequency trading;Data communication systems;sliding windows;Multicore programming;Sequential switching;sliding windows;skyline queries;Real time decision-making;Query languages;Network architecture;Multicore programming;Computer programming;Clustering algorithms;skyline queries;Decision making","Abstract":"The emergence of real-time decision-making applications in domains like high-frequency trading, emergency management, and service level analysis in communication networks has led to the definition of new classes of queries. Skyline queries are a notable example. Their results consist of all the tuples whose attribute vector is not dominated (in the Pareto sense) by one of any other tuple. Because of their popularity, skyline queries have been studied in terms of both sequential algorithms and parallel implementations for multiprocessors and clusters. Within the Data Stream Processing paradigm, traditional database queries on static relations have been revised in order to operate on continuous data streams. Most of the past papers propose sequential algorithms for continuous skyline queries, whereas there exist very few works targeting implementations on parallel machines. This paper contributes to fill this gap by proposing a parallel implementation for multicore architectures. We propose (i) a parallelization of the eager algorithm based on the notion of Skyline Influence Time, (ii) optimizations of the reduce phase and load-balancing strategies to achieve near-optimal speedup, and (iii) a set of experiments with both synthetic benchmarks and a real dataset in order to show our implementation effectiveness. Copyright ","SELECTED":null}
{"Title":"Latency arbitrage in fragmented markets: A strategic agent-based analysis","keywords (cleaned)":"Agent-based simulation;High-frequency trading;Latency arbitrage","Abstract":"We study the effect of latency arbitrage on allocative efficiency and liquidity in fragmented financial markets. We employ a simple model of latency arbitrage in which a single security is traded on two exchanges, with price quotes available to regular traders only after some delay. An infinitely fast arbitrageur reaps profits when the two markets diverge due to this latency in cross-market communication. Using an agent-based approach, we simulate interactions between high-frequency and zero-intelligence trading agents. From simulation data over a large space of strategy combinations, we estimate game models and compute strategic equilibria in a variety of market environments. We then evaluate allocative efficiency and market liquidity in equilibrium, and we find that market fragmentation and the presence of a latency arbitrageur reduces total surplus and negatively impacts liquidity. By replacing continuous-time markets with periodic call markets, we eliminate latency arbitrage opportunities and achieve further efficiency gains through the aggregation of orders over short time periods. ","SELECTED":null}
{"Title":"Profitability of directional change based trading strategies: The case of saudi stock market","keywords (cleaned)":"Automated trading;Directional changes;Financial forecasting;Financial markets;Simulation","Abstract":"An event-based framework of directional changes (DC) and overshoots maps financial market (FM) price time series into the so-called intrinsic time where events are the time scale of the price time series. This allows for multi-scale analysis of financial data. In the light of this, this paper formulates DC event approach into three automated trading strategies for investments in the FMs: ZI-Directional Change Trading (DCT0), DCT1, and DCT2. The main idea is to use intrinsic time scale based on DC events to learn the size and the direction of periodic patterns from the asset price historical dataset. Using simulation models of Saudi Stock Market, we evaluate the returns of the automated DC trading strategies. The analysis revealed interesting results and evidence that the proposed strategies can indeed generate effective trading for investors with a high rate of returns. The results of this study can be used further to develop decision support systems and autonomous trading agent strategies for the FM. ","SELECTED":null}
{"Title":"Machine News and Volatility: The Dow Jones Industrial Average and the TRNA Real-Time High-Frequency Sentiment Series","keywords (cleaned)":"Conditional volatility models;Financial markets;High frequency HF;Sentiment scores;Dow Jones Industrial averages;TRNA;Commerce;Algorithmic trading;Auto-regressive;Finance;DJIA;Securities industry;Electronic trading;Industrial research","Abstract":"This chapter features an analysis of the relationship between the volatility of the Dow Jones Industrial Average (DJIA) Index and a sentiment news series using daily data obtained from the Thomson Reuters News Analytics (TRNA) provided by The Securities Industry Research Center of the Asia Pacific. The expansion of online financial news sources, such as Internet news and social media sources, provides instantaneous access to financial news. Commercial agencies have started developing their own filtered financial news feeds, which are used by investors and traders to support their algorithmic trading strategies. In this chapter, we use a high-frequency sentiment series, developed by TRNA, to construct a series of daily sentiment scores for DJIA stock index component companies. A variety of forms of this measure, namely, basic scores, absolute values of the series, squared values of the series, and the first differences of the series, are used to estimate three standard volatility models, namely, GARCH (Generalized Autoregressive Conditional Heteroscedastic), EGARCH (Exponential Generalized Autoregressive Conditional Heteroscedasctic), and GJR (Glosten, Jagganathan and Rundle (1993)). We use these alternative daily DJIA market sentiment scores to examine the relationship between financial news sentiment scores and the volatility of the DJIA return series. We demonstrate how this calibration of machine-filtered news can improve volatility measures. ","SELECTED":null}
{"Title":"Network-level FPGA acceleration of low latency market data feed arbitration","keywords (cleaned)":"Acceleration;Modes of operation;computer interface;Financial markets;Hardware-accelerated;Messaging protocols;Network interface cards;FPGA;Low latency;Data feed arbitration;Commerce;Electronic data interchange;Data feed;Finance;Field programmable gate arrays (FPGA);Future applications;Acceleration approach;Reliability","Abstract":"Financial exchanges provide market data feeds to update their members about changes in the market. Feed messages are often used in time-critical automated trading applications, and two identical feeds (A and B feeds) are provided in order to reduce message loss. A key challenge is to support A\/B line arbitration efficiently to compensate for missing packets, while offering flexibility for various operational modes such as prioritising for low latency or for high data reliability. This paper presents a reconfigurable acceleration approach for A\/B arbitration operating at the network level, capable of supporting any messaging protocol. Two modes of operation are provided simultaneously: one prioritising low latency, and one prioritising high reliability with three dynamically configurable windowing methods. We also present a model for message feed processing latencies that is useful for evaluating scalability in future applications. We outline a new low latency, high throughput architecture and demonstrate a cycle-accurate testing framework to measure the actual latency of packets within the FPGA. We implement and compare the performance of the NASDAQ TotalView-ITCH, OPRA and ARCA market data feed protocols using a Xilinx Virtex-6 FPGA. For high reliability messages we achieve latencies of 42ns for TotalView-ITCH and 36.75ns for OPRA and ARCA. 6ns and 5.25ns are obtained for low latency messages. The most resource intensive protocol, TotalView-ITCH, is also implemented in a Xilinx Virtex- 5 FPGA within a network interface card; it is used to validate our approach with real market data. We offer latencies 10 times lower than an FPGA-based commercial design and 4.1 times lower than the hardware-accelerated IBM PowerEN processor, with throughputs more than double the required 10Gbps line rate. Copyright ","SELECTED":null}
{"Title":"Optimal order display in limit order markets with liquidity competition","keywords (cleaned)":"Hidden liquidity;High-frequency trading;Limit order book;Liquidity competition;Market impacts;Order flow dynamics","Abstract":"Order display is associated with benefits and costs. Benefits arise from increased execution-priority, while costs are due to adverse market impact. We analyze a structural model of optimal order placement that captures trade-off between the costs and benefits of order display. For a benchmark model of pure liquidity competition, we give a closed-form solution for optimal display sizes. We show that competition in liquidity supply incentivizes the use of hidden orders to prevent losses due to over-bidding. Thus, because aggressive liquidity competition is more prevalent in liquid stocks, our model predicts that the proportion of hidden liquidity is higher in liquid markets. Our theoretical considerations ares supported by an empirical analysis using high-frequency order-message data from NASDAQ. We find that there are no benefits in hiding orders in il-liquid stocks, whereas the performance gains can be significant in liquid stocks. ","SELECTED":null}
{"Title":"Low latency book handling in FPGA for high frequency trading","keywords (cleaned)":"Algorithms;Data handling;Economic and social effects;Electronic trading;Hardware;Static random access storage;Algorithmic trading;Hardware architecture;High-frequency trading;Lookup latency;Market data processing;Memory utilization;Proposed architectures;Trading systems;Memory architecture","Abstract":"Recent growth in algorithmic trading has caused a demand for lowering the latency of systems for electronic trading. FPGA cards are widely used to reduce latency and accelerate market data processing. To create a low latency trading system, it is crucial to effectively build a representation of the market state (book) in hardware. Thus, we have designed a new hardware architecture, which updates the book with the best bid\/offer prices based on the incoming messages from the exchange. For each message a corresponding financial instrument needs to be looked up and its record needs to be updated. Proposed architecture is utilizing cuckoo hashing for the book handling, which enables low latency symbol lookup and high memory utilization. In this paper we discuss a trade-off between lookup latency and memory utilization. With average latency of 253 ns the proposed architecture is able to handle 119 275 instruments while using only 144 Mbit QDR SRAM. ","SELECTED":null}
{"Title":"Behavioural investigations of financial trading agents using exchange portal (ExPo)","keywords (cleaned)":"Agent-based model;Software agents;Computational finance;Financial data processing;Open source software;ACE;Commerce;Assignment Adaptation;Auction;Exhibitions;Computational methods;Experiments;ABM;Finance;Agent-based model;Exchange portal;Automated trading;Agent-based computational economics;ExPo","Abstract":"Some major financial markets are currently reporting that 50% or more of all transactions are now executed by automated trading systems (ATS). To understand the impact of ATS proliferation on the global financial markets, academic studies often use standard reference strategies, such as \u201cAA\u201d and \u201cZIP\u201d, to model the behaviour of real trading systems. Disturbingly, we show that the reference algorithms presented in the literature are ambiguous, thus reducing the validity of strict comparative studies. As a remedy, we suggest disambiguated standard implementations of AA and ZIP. Using Exchange Portal (ExPo), an open-source financial exchange simulation platform designed for realtime behavioural economic experiments involving human traders and\/or trader-agents, we study the effects of disambiguating AA and ZIP, before introducing a novel method of assignment-adaptation (ASAD). Experiments show that introducing ASAD agents into a market with shocks can produce counter-intuitive market dynamics. ","SELECTED":null}
{"Title":"Automated asset management based on partially cooperative agents for a world of risks","keywords (cleaned)":"Multi agent systems;Managers;Multiagent systems;Competitive agents;Negotiation protocol;Managers;Analysis techniques;Asset management systems;Automation;Cooperative agents;Multiagent architecture;Commerce;Autonomous agents;Profitability;Multiagent architecture;Investments;architecture;Investors' preferences;Automated asset management;Risk perception;Automated trading","Abstract":"Despite the fact any investor prefers lower risk and higher return, investors may have different preferences about what would be an acceptable risk or a minimal return. For instance, some investors prefer to have a lower bound risk rather than gaining a higher return. In portfolio theory, it is commonly assumed the existence of one risk free asset that offers a positive return. This theoretical risk free asset combined with a risky portfolio creates a new portfolio that presents a linear relation between risk and return as the risk free asset weight (w f ) changes. Hence, any level of risk or of return is easy to achieve separately, just by changing w f . However, in a world without any risk free assets, the combination between assets creates nonlinear portfolios. Achieving a specific level of risk or return is not a trivial task. In this paper, we assume a risky world rather than the existence of a risk free asset, in order to model an automated asset management system. Furthermore, some automated asset managers give very different results when evolving in different contexts: hence, a very profitable manager can have very bad results in other market situations. This paper presents a multiagent architecture, aiming to tackle these problems. The architecture, named COAST (COmpetitive Agent SocieTy), is based on competitive agents that act autonomously on behalf of an investor in financial asset management. It allows the simultaneous and competitive use of several asset analysis techniques currently applied in the finance field. Some dedicated agents, called advisors, apply a particular technique to a single asset. The results provided by these advisors are then submitted to and analyzed by a special agent called coach, who evaluates its advisors' performance and defines an expectation about the future price of one specific asset. Within COAST, several coaches negotiate to define the best money allocation among different assets, by using a negotiation protocol defined in this paper. We also propose an investor description model that is able to represent different investors' preferences with defined acceptable limits of risk and\/or return. The COAST architecture was designed to operate adequately with any possible investor's preference. It was implemented using a financial market simulator called AgEx and tested using real data from the Nasdaq stock exchange. The test results show that the architecture performed well when compared to an adjusted market index. ","SELECTED":null}
{"Title":"Towards automated event studies using high frequency news and trading data","keywords (cleaned)":"Event studies;Service oriented computing;High frequency data;Stock price;Complex events;Economics;Commerce;Academic research;Event studies;Price Jump Detection;High frequency;Information technology;Unscheduled News;Industry;Business Process;Design decisions;High frequency data","Abstract":"Event studies have a long history in academic research and were used in disciplines as diverse as economics, law, information technology, marketing, and finance. One of the main challenges is that the process of undertaking such an event study is complex and many assumptions, trade-offs and design decisions need to be made. Based on Service-Oriented Computing principles, this paper proposes a business process on how to undertake and partly automate complex event studies on effects of (un)scheduled news on stocks prices using high frequency trading and news data. The proposed business process is illustrated using a case study that shows how to identify effects of unscheduled news on stock prices in the German DAX30 index. ","SELECTED":null}
{"Title":"The impacts of automation and high frequency trading on market quality","keywords (cleaned)":"automated market making;Electronic market;Market efficiency;Market regulation","Abstract":"In recent decades, US equity markets have changed from predominantly manual markets with limited competition to highly automated and competitive markets. These changes occurred earlier for NASDAQ stocks (primarily between 1994 and 2004) and later for NYSE-listed stocks (mostly following Reg NMS and the 2006 introduction of the NYSE hybrid market). This paper surveys the evidence of how these changes impacted market quality and shows that overall market quality has improved significantly, including bid-ask spreads, liquidity, and transitory price impacts (measured by short-term variance ratios). The greater improvement in market quality for NYSE-listed stocks relative to NASDAQ stocks beginning in 2006 suggests causal links between the staggered market structure changes and market quality. Using proprietary data sets, provided by two exchanges, that identify the activity of high frequency trading firms, studies show these firms contributed directly to narrowing bid-ask spreads, increasing liquidity, and reducing intraday transitory pricing errors and intraday volatility. Copyright ","SELECTED":null}
{"Title":"Time series and neural network forecast of daily stock prices","keywords (cleaned)":"And fundamental analysis;Box-Jenkins ARIMA methodology;Forecasting stock prices;Holt\/Winters exponential smoothing;Neural network model;Technical analysis;Time series decomposition","Abstract":"Time series analysis is somewhat parallel to technical analysis, but it differs from the latter by using different statistical methods and models to analyze historical stock prices and predict the future prices. With the rapid increases in algorithmic or high frequency trading in which trader make trading decisions by analyzing data patterns rather than fundamental factors affecting stock prices, both technical analyses and time series analyses become more relevant. In this study the authors apply the traditional time series decomposition (TSD), Holt\/Winters (H\/W) models, Box-Jenkins (B\/J) methodology, and neural network (NN) to 50 randomly selected stocks from September 1, 1998 to December 31, 2010 with a total of 3105 observations for each company's close stock price. This sample period covers high tech boom and bust, the historical 9\/11 event, housing boom and bust, and the recent serious recession and current slow recovery. During this exceptionally uncertain period of global economic and financial crises, it is expected that stock prices are extremely difficult to predict. All three time series approaches fit the data extremely well with R2 being around 0.995. For the hold-out period or out-of-sample forecasts over 60 trading days, the forecasting errors measured in terms of mean absolute percentage errors (MAPE) are lower for B\/J, H\/W, and normalized NN model, but forecasting errors are quite large for time series decomposition and non-normalized NN models. ","SELECTED":null}
{"Title":"Towards automated trading based on fundamentalist and technical data","keywords (cleaned)":"Multi agent systems;Intelligent robots;Extended versions;Time series;Test platforms;Financial market simulation;Technical information;Fuzzy clustering;Automated trading;Technical data;Automation;Mobile agents;Short-interval;Commerce;Autonomous agents;Reasoning process;Finance;Autonomous agents;Profitability;Robot soccer competition;Economic sectors;Stock market;Multiagent systems;Open sources;Electric ship equipment;Automated trading","Abstract":"Autonomous trading is often seen as artificial intelligence applied to finance by AI researchers, but it may also be a way to motivate the development of autonomous agents, just like robot soccer competitions are used to motivate the research in mobile robots. In fact, some initiatives could be observed in recent years, for instance [1] and [2]. In this paper, we present a multiagent system composed by several autonomous analysts that use fundamentalist information in their reasoning process. These fundamentalist information are composed by company profit, dividends, data related to the company economic sector among others. This kind of information is rarely used on autonomous trading, because most of the agents deal only with technical information, which is composed by price and volume time series. Furthermore, we do not find a open source stock market simulator with support to fundamentalist trader agents. We then created a significantly extended version of the open source financial market simulation tool, called AgEx. This designed version provides also fundamentalist information about the trader's assets. As well as, makes more efficient the exchange of messages within AgEx. This efficiency allows traders that may submit orders in very short intervals of just some seconds or even some fraction of second, to use AgEx as a test platform. Using this new version of AgEx, we implemented and tested the multiagent system based on fundamentalist agents, that we call FAS. The achieved results are presented and analyzed. ","SELECTED":null}
{"Title":"Data stream mining for market-neutral algorithmic trading","keywords (cleaned)":"Regression coefficients;Temporal data mining;Probabilistic laws;Ordinary Least Squares;Commerce;Algorithmic trading;Feature extraction;Trading systems;Principal component analysis;Least squares approximations;Curve fitting;Flexible least squares;Incremental principal component analysis;Information management;Data stream mining;Data mining;Financial datums","Abstract":"In algorithmic trading applications, a large number of coevolving financial data streams are observed and analyzed. A recurrent and important task is to determine how a given stream depends on others, over time, accounting for dynamic dependence patterns and without imposing any probabilistic law governing this dependence. We demonstrate how Flexible Least Squares (FLS), a penalized version of ordinary least squares that accommodates for dynamic regression coefficients, can be deployed successfully in this context. We describe a market-neutral algorithmic trading system based on a combined use of on-line feature extraction and recursive regression. The system has been proved to perform successfully when trading the S&P 500 Futures Index. Copyright 2008 ACM.","SELECTED":null}
{"Title":"Decentralized authorization and data security in web content delivery","keywords (cleaned)":"Decentralized control;Security of data;Data integrity;Computer programming languages;Web intermediaries;Network routing;Content delivery networks;Scalability;Authorization;Web;Software architecture;World Wide Web","Abstract":"The fast development of web services, or more broadly, service-oriented architectures (SOAs), has prompted more organizations to move contents and applications out to the Web. Softwares on the web allow one to enjoy a variety of services, for example translating texts into other languages and converting a document from one format to another. In this paper, we address the problem of maintaining data integrity and confidentiality in web content delivery when dynamic content modifications are needed. We propose a flexible and scalable model for secure content delivery based on the use of roles and role certificates to manage web intermediaries. The proxies coordinate themselves in order to process and deliver contents, and the integrity of the delivered content is enforced using a decentralized strategy. To achieve this, we utilize a distributed role lookup table and a role-number based routing mechanism. We give an efficient secure protocol, iDeliver, for content processing and delivery, and also describe a method for securely updating role lookup tables. Our solution also applies to the security problem in web-based workflows, for example maintaining the data integrity in automated trading, contract authorization, and supply chain management in large organizations. Copyright 2007 ACM.","SELECTED":null}
{"Title":"Two stock-trading agents: Market making and technical analysis","keywords (cleaned)":"Automated learning;Information feedback;Stock trading;Technical analysis;Automated trading;Computing power;External informations;Market simulation;Market volatility;Stock price movements;Technical analysis;Trading strategies;Artificial intelligence;Automation;Computational methods;Computer networks;Decision making;Information analysis;Simulation;Artificial intelligence;Automation;Commerce;Electronic commerce;Electronic trading;Financial markets;Intelligent agents;Multi agent systems;Finance;Autonomous agents","Abstract":"Evolving information technologies have brought computational power and real-time facilities into the stock market. Automated stock trading draws much interest from both the fields of computer science and of business, since it promises to provide superior ability in a trading market to any individual trader. Trading strategies have been proposed and practiced from the perspectives of Artificial Intelligence, market making, external information feedback, and technical analysis among others. This paper examines two automated stock-trading agents in the context of the Penn-Lehman Automated Trading (PLAT) simulator [1], which is a real-time, real-data market simulator. The first agent devises a market-making strategy exploiting market volatility without predicting the exact direction of the stock price movement. The second agent uses technical analysis. It might seem natural to buy when the market is on the rise and sell when it's on the decline, but the second agent does exactly the opposite. As a result, we call it the reverse strategy. The strategies used by both agents are adapted for automated trading. Both agents performed well in a PLAT live competition. In this paper, we analyze the performance of these two automated trading strategies. Comparisons between them are also provided. ","SELECTED":null}
{"Title":"High-frequency market making to large institutional trades","keywords (cleaned)":"price impact|HFT|equities|bid-ask spreads|mean reversion|dynamics|trading|trade execution|transaction costs|r|spread|transaction cost|market|market making|HFTs|trade|high-frequency trading|High-frequency|costs|high-frequency","Abstract":"We study market-making high-frequency trader (HFT) dynamics around large institutional trades in Canadian equities markets using order-level data with masked trader identification. Following a regulatory change that negatively affected HFT order activity, we find that bid-ask spreads increased and price impact decreased for institutional trades. The decrease in price impact is strongest for informed institutional traders. During institutional trade executions, HFTs submit more same-direction orders and increase their inventory mean reversion rates. Our evidence indicates that high-frequency trading is associated with lower transaction costs for small, uninformed trades and higher transaction costs for large, informed trades. ","SELECTED":null}
{"Title":"Stability of Evolving Fuzzy Systems Based on Data Clouds","keywords (cleaned)":"stability;Convergence of numerical methods;AnYa type fuzzy systems;Mackey-Glass time series predictions;Fuzzy systems;Methodological frameworks;Personnel training;Electronic trading;System stability;High-frequency trading;data clouds;Evolving Fuzzy Systems;Algorithm design and analysis;Learning algorithms;data clouds;Stability criteria;Online systems;Learning systems;Adaptation models;Identification (control systems);Membership functions;Non-linear system identification;evolving fuzzy systems (EFSs)","Abstract":"Evolving fuzzy systems (EFSs) are now well developed and widely used, thanks to their ability to self-adapt both their structures and parameters online. since the concept was first introduced two decades ago, many different types of EFSs have been successfully implemented. However, there are only very few works considering the stability of the EFSs, and these studies were limited to certain types of membership functions with specifically predefined parameters, which largely increases the complexity of the learning process. At the same time, stability analysis is of paramount importance for control applications and provides the theoretical guarantees for the convergence of the learning algorithms. In this paper, we introduce the stability proof of a class of EFSs based on data clouds, which are grounded at the AnYa type fuzzy systems and the recently introduced empirical data analytics (EDA) methodological framework. By employing data clouds, the class of EFSs of AnYa type considered in this paper avoids the traditional way of defining membership functions for each input variable in an explicit manner and its learning process is entirely data driven. The stability of the considered EFS of AnYa type is proven through the Lyapunov theory, and the proof of stability shows that the average identification error converges to a small neighborhood of zero. Although, the stability proof presented in this paper is specially elaborated for the considered EFS, it is also applicable to general EFSs. The proposed method is illustrated with Box-Jenkins gas furnace problem, one nonlinear system identification problem, Mackey-Glass time series prediction problem, eight real-world benchmark regression problems as well as a high-frequency trading prediction problem. Compared with other EFSs, the numerical examples show that the considered EFS in this paper provides guaranteed stability as well as a better approximation accuracy. ","SELECTED":null}
{"Title":"Generating long-term trading system rules using a genetic algorithm based on analyzing historical data","keywords (cleaned)":"Binary trees;Commerce;Decision trees;Genetic algorithms;Innovation;Trees (mathematics);Algorithmic trading;Binary decision trees;Correct strategy;Dynamic range;Historical data;Technical analysis;Technical indicator;Trading systems;Electronic trading","Abstract":"In current times, trading success depends on choosing a correct strategy. Algorithmic trading is often based on technical analysis - an approach where the values of one or several technical indicators are translated into buy or sell signals. Thus, every trader's main challenge is the choice and use of the most fitting trading rules. In our work, we suggest an evolutionary algorithm for generating and selecting the most fitting trading rules for interday trading, which are presented in the form of binary decision trees. A distinctive feature of this approach is the interpretation of the evaluation of the current state of technical indicators with the help of dynamic ranges that are recalculated on a daily basis. This allows to create long-term trading rules. We demonstrate the effectiveness of this system for the Top-5 stocks of the United States IT sector and discuss the ways to improve it. ","SELECTED":null}
{"Title":"Sampling frequency and the performance of different types of technical trading rules","keywords (cleaned)":"Commodity ETFs;High-frequency trading;Market efficiency;Technical analysis","Abstract":"The predictive ability of technical trading rules has been studied in great detail however many papers group all technical trading rules together into one basket. We argue that there are two main types of technical trading rules, namely rules based on trend-following and mean reversion. Utilising high-frequency commodity ETF data, we show that mean-reversion based rules perform increasingly better as sampling frequencies increase and that conversely the performance of trend-following rules deteriorate at higher-frequencies. These findings are possibly related to noise created by high-frequency traders. ","SELECTED":null}
{"Title":"Optimal order placement in limit order markets","keywords (cleaned)":"Algorithmic trading;Execution risk;Limit order markets;Machine learning;Optimal order execution;Order routing;Robbins\u2013Monro algorithm;Stochastic approximation;Supervised learning;Transaction cost","Abstract":"To execute a trade, participants in electronic equity markets may choose to submit limit orders or market orders across various exchanges where a stock is traded. This decision is influenced by characteristics of the order flows and queue sizes in each limit order book, as well as the structure of transaction fees and rebates across exchanges. We propose a quantitative framework for studying this order placement problem by formulating it as a convex optimization problem. This formulation allows the study of how the optimal order placement decision depends on the interplay between the state of order books, the fee structure, order flow properties and the aversion to execution risk. In the case of a single exchange, we derive an explicit solution for the optimal split between limit and market orders. For the general case of order placement across multiple exchanges, we propose a stochastic algorithm that computes the optimal routing policy and study the sensitivity of the solution to various parameters. Our algorithm does not require an explicit statistical model of order flow but exploits data on recent order fills across exchanges in the numerical implementation of the algorithm to acquire this information through a supervised learning procedure. ","SELECTED":null}
{"Title":"Fourier Analysis for Stock Price Forecasting: Assumption and Evidence","keywords (cleaned)":"Algorithmic trading;FFT;Fourier\u2019s transformation;level trading;market price series spectrogram;momentum;US stocks backtesting","Abstract":"The research addressed the relevant question whether the Fourier analysis really provides practical value for investors forecasting stock market price. To answer this question, the significant cycles were discovered using the Fourier analysis inside the price series of US stocks; then, the simulation of an agent buying and selling on minima and maxima of these cycles was made. The results were then compared to those of an agent operating chaotically. Moreover, the existing significant cycles were found using more precise methods, suggested in the research, and based on the results of an agent buying and selling on all possible periods and phases. It has been analysed whether these really existing cycles were in accordance with the significant cycles resulting from the Fourier analysis. It has been concluded that the Fourier analysis basically failed. Suchlike failures are expected on similar data series. In addition, momentum and level trading backtests have been used in a similar way. It has been found that the level trading does provide a certain practical value in comparison to the momentum trading method. The research also simplifies the complicated theoretical background for practitioners. ","SELECTED":null}
{"Title":"An agent-based model of competition between financial exchanges: Can frequent call mechanisms drive trade away from cDas?","keywords (cleaned)":"Agent-based model;Competing platforms;Multi agent systems;Financial data processing;Continuous double auction;High frequency HF;Market microstructure;Commerce;Approximate equilibriums;Computational methods;Agent-based model;Different mechanisms;Electronic trading;Autonomous agents;Market microstructure;High-frequency trading","Abstract":"In the debate over high frequency trading, the frequent call (Call) mechanism has recently received considerable attention as a proposal for replacing the continuous double auction (CDA) mechanisms that currently run most financial markets. One natural question, which has begun to spur the development of new models, is the effect of competition between platforms that use these two different mechanisms when agents can strategize over platform choice. In this paper we contribute to this nascent literature by developing an agent-based model of competition between a Call market and a CDA market. Our model incorporates patient informed traders (both high-frequency and not) who are willing to wait for order execution at their preferred price and impatient background traders who demand immediate execution. We show that there is a strong tendency for the Call market to absorb a significant fraction of trade under most equilibrium and approximate-equilibrium conditions. These equilibria typically lead to significantly higher welfare for the background traders, an important measure of social value, than the operation of an isolated CDA market. Copyright ","SELECTED":null}
{"Title":"Measuring the Leverage Effect in a High-Frequency Trading Framework","keywords (cleaned)":"Leverage effects;Financial data processing;Electronic trading;High-frequency trading;Random processes;Frequency estimation;Stochastic Volatility Model;Semimartingales;Finite sample performance;Sampling;Nonparametric estimation;Fourier analysis;Financial markets;Commerce;Stochastic systems;Portfolio managements;Semimartingales;Nonparametric estimation;Fourier transforms;Investments;High frequency data;Economic analysis;Stochastic models;High-frequency data;Leverage effects","Abstract":"Multifactor stochastic volatility models of the financial time series can have important applications in portfolio management and pricing\/hedging of financial instruments. Based on the semimartingale paradigm, we focus on the study and the estimation of the leverage effect, defined as the covariance between the price and the volatility process and modeled as a stochastic process. Our estimation procedure is based only on a preestimation of the Fourier coefficients of the volatility process. This approach constitutes a novelty in comparison with the nonparametric leverage estimators proposed in the literature, generally based on a preestimation of spot volatility, and it can be directly applied to estimate the leverage effect in the case of irregular trading observations and in the presence of microstructure noise contaminations, that is, in a high-frequency framework. The finite sample performances of the Fourier estimator of the leverage are tested in numerical simulations and in an empirical application to S&P 500 index futures. ","SELECTED":null}
{"Title":"High-frequency financial statistics with parallel R and Intel Xeon Phi coprocessor","keywords (cleaned)":"parallel R;Co-processors;Risk management;Frequency estimation;Intel Xeon Phi Coprocessor;Statistics;Financial risk management;Commerce;Finance;Statistical tests;massive parallelism;Investments;high-frequency financial analysis;New York Stock Exchange;Multiple hypothesis testing;massive parallelism;Simulation and optimization;Risk perception;Risk assessment;Big data;Financial analysis","Abstract":"Financial statistics covers a wide array of applications in the financial world, such as (high frequency) trading, risk management, pricing and valuation of securities and derivatives, and various business and economic analytics. Portfolio allocation is one of the most important problems in financial risk management. One most challenging part in portfolio allocation is the tremendous amount of data and the optimization procedures that require computing power beyond the currently available desktop systems. In this article, we focus on the portfolio allocation problem using high-frequency financial data, and propose a hybrid parallelization solution to carry out efficient asset allocations in a large portfolio via intra-day high-frequency data. We exploit a variety of HPC techniques, including parallel R, Intel Math Kernel Library, and automatic offloading to Intel Xeon Phi coprocessor in particular to speed up the simulation and optimization procedures in our statistical investigations. Our numerical studies are based on high-frequency price data on stocks traded in New York Stock Exchange in 2011. The analysis results show that portfolios constructed using high-frequency approach generally perform well by pooling together the strengths of regularization and estimation from a risk management perspective. We also investigate the computation aspects of large-scale multiple hypothesis testing for time series data. Using a combination of software and hardware parallelism, we demonstrate a high level of performance on high-frequency financial statistics. ","SELECTED":null}
{"Title":"A hybrid genetic-programming swarm-optimisation approach for examining the nature and stability of high frequency trading strategies","keywords (cleaned)":"Algorithms;Artificial intelligence;Economics;Evolutionary algorithms;Field programmable gate arrays (FPGA);Financial data processing;Financial markets;Genetic algorithms;Genetic programming;Learning systems;Particle swarm optimization (PSO);stability;Foreign exchange market;Genetic operators;High-frequency trading;Hybrid evolutionary algorithm;Market microstructure;Optimal trading strategy;Particle swarm optimisation;Signal information;Commerce","Abstract":"Advances in high frequency trading in financial markets have exceeded the ability of regulators to monitor market stability, creating the need for tools that go beyond market microstructure theory and examine markets in real time, driven by algorithms, as employed in practice. This paper investigates the design, performance and stability of high frequency trading rules using a hybrid evolutionary algorithm based on genetic programming, with particle swarm optimisation layered on top to improve the genetic operators' performance. Our algorithm learns relevant trading signal information using Foreign Exchange market data. Execution time is significantly reduced by implementing computationally intensive tasks using Field Programmable Gate Array technology. This approach is shown to provide a reliable platform for examining the stability and nature of optimal trading strategies under different market conditions through robust statistical results on the optimal rules' performance and their economic value. ","SELECTED":null}
{"Title":"The implications of high-frequency trading on market efficiency and price discovery","keywords (cleaned)":"High-frequency trading;price efficiency","Abstract":"This study investigates the implications of high-frequency trading (HFT) on market efficiency and price discovery by using state-space models and real-life one-minute high-frequency data of the six most traded currency pairs worldwide \u2013 USD\/EUR, USD\/JPY, USD\/GBP, USD\/AUD, USD\/CHF and USD\/CAD. We found significant evidence that HFT enhances market efficiency and has a beneficial role in price discovery by trading in the direction of the permanent component of the state-space model and in the opposite direction of its transitory component. ","SELECTED":null}
{"Title":"An algorithm-based statistical arbitrage high frequency trading system to forecast prices of natural gas futures","keywords (cleaned)":"Algorithmic trading;Efficient market hypothesis;Futures market;High frequency trading;Pairs trading;Statistical arbitrage","Abstract":"Professional fund managers, investment banks and regulatory authorities raise the question about the impact of algorithmic trading on trading businesses, economy and market efficiency. The questions rise if high frequency trading (HFT) provides more efficient, liquid markets and is economically beneficial. In this paper an algorithm based on statistical arbitrage is tested. Statistical arbitrage is a well-known trading strategy where profit arises from pricing inefficiencies between correlated financial instruments. An algorithm of statistical arbitrage tries to find a pair of correlated instruments that move together and take long\/short positions when they diverge abnormally, hoping that the prices will converge in the near future. Recent computational expansion in financial modeling and the ever increasing demand for order execution speed is moving market making and price discovery strategies into high frequency trading (HFT) or the milliseconds realm, where HFT already generates nearly 2\/3 of the overall trading volume. In this report we apply high frequency data from NYMEX exchange to test a trading system based on statistical arbitrage in one of the most liquid futures market, i.e. natural gas futures. The overall results suggest that statistical arbitrage in HFT environment significantly outperforms traditional trading strategies, provides liquidity to the markets and denies the efficient market hypothesis. ","SELECTED":null}
{"Title":"Low-latency and high bandwidth TCP\/IP protocol processing through an integrated HW\/SW approach","keywords (cleaned)":"High performance computing (HPC);High-frequency trading;Integrated system design;Packet transfer;Protocol processing;Sensitive application;System architects;TCP\/IP protocol;Bandwidth;Complex networks;Computer peripheral equipment;Ethernet;Internet telephony;Program processors;Transmission control protocol;Wire;Loading","Abstract":"Ultra low-latency networking is critical in many domains, such as high frequency trading and high performance computing (HPC), and highly desirable in many others such as VoIP and on-line gaming. In closed systems - such as those found in HPC - Infiniband, iWARP or RoCE are common choices as system architects have the opportunity to choose the best host configurations and networking fabric. However, the vast majority of networks are built upon Ethernet with nodes exchanging data using the standard TCP\/IP stack. On such networks, achieving ultra low-latency while maintaining compatibility with a standard TCP\/IP stack is crucial. To date, most efforts for low-latency packet transfers have focused on three main areas: (i) avoiding context switches, (ii) avoiding buffer copies, and (iii) off-loading protocol processing. This paper describes IBM PowerENTM and its networking stack, showing that an integrated system design which treats Ethernet adapters as first class citizens that share the system bus with CPUs and memory, rather than as peripheral PCI Express attached devices, is a winning solution for achieving minimal latency. The work presents outstanding performance figures, including 1.30\u03bcs from wire to wire for UDP, usually the chosen protocol for latency sensitive applications, and excellent latency and bandwidth figures for the more complex TCP. ","SELECTED":null}
{"Title":"Discovering the ecosystem of an electronic financial market with a dynamic machine-learning method","keywords (cleaned)":"clustering;High frequency trading;Machine learning;Trading strategies","Abstract":"Not long ago securities were traded by human traders in face-to-face markets. The ecosystem of an open outcry market was well-known, visible to a human eye, and rigidly prescribed. Now trading is increasingly done in anonymous electronic markets where traders do not have designated functions or mandatory roles. In fact, the traders themselves have been replaced by algorithms (machines) operating with little or no human oversight. While the process of electronic trading is not visible to a human eye, machine-learning methods have been developed to recognize persistent patterns in the data. In this study, we develop a dynamic machine-learning method that designates traders in an anonymous electronic market into five persistent categories: high frequency traders, market makers, opportunistic traders, fundamental traders, and small traders. Our method extends a plaid clustering technique with a smoothing framework that filters out transient patterns. The method is fast, robust, and suitable for a discovering trading ecosystems in a large number of electronic markets. ","SELECTED":null}
{"Title":"Accelerating maximum likelihood estimation for Hawkes point processes","keywords (cleaned)":"Engines;Field programmable gate arrays (FPGA);Parameter estimation;Strategy evaluations;FPGA-based implementation;Hardware acceleration;High-frequency trading;Numerical calculation;Occurrence pattern;Parameter estimation problems;Probabilistic models;Maximum likelihood estimation","Abstract":"Hawkes processes are point processes that can be used to build probabilistic models to describe and predict occurrence patterns of random events. They are widely used in high-frequency trading, seismic analysis and neuroscience. A critical numerical calculation in Hawkes process models is parameter estimation, which is used to fit a Hawkes process model to a data set. The parameter estimation problem can be solved by searching for a parameter set that maximises the log-likelihood. A core operation of this search process, the log-likelihood evaluation, is computationally demanding if the number of data points is large. To accelerate the computation, we present a log-likelihood evaluation strategy which is suitable for hardware acceleration. We then design and optimise a pipelined engine based on our proposed strategy. In the experiments, an FPGA-based implementation of the proposed engine is shown to be up to 72 times faster than a single-core CPU, and 10 times faster than an 8-core CPU. ","SELECTED":null}
{"Title":"Hierarchical Temporal Memory-based algorithmic trading of financial markets","keywords (cleaned)":"Algorithmic trading;Binary classifiers;Data sets;Features vector;Features vector;Financial markets;Futures market;Machine learning technology;Neural network model;Technical indicator;Temporal memory;Test sets;Trading strategies;Training schemes;Training sets;Algorithms;Benchmarking;Finance;Neural networks;Profitability;Software agents;Statistical tests;Commerce","Abstract":"This paper explores the possibility of using the Hierarchical Temporal Memory (HTM) machine learning technology to create a profitable software agent for trading financial markets. Technical indicators, derived from intraday tick data for the E-mini S&P 500 futures market (ES), were used as features vectors to the HTM models. All models were configured as binary classifiers, using a simple buy-and-hold trading strategy, and followed a supervised training scheme. The data set was divided into a training set, a validation set and three test sets; bearish, bullish and horizontal. The best performing model on the validation set was tested on the three test sets. Artificial Neural Networks (ANNs) were subjected to the same data sets in order to benchmark HTM performance. The results suggest that the HTM technology can be used together with a feature vector of technical indicators to create a profitable trading algorithm for financial markets. Results also suggest that HTM performance is, at the very least, comparable to commonly applied neural network models. ","SELECTED":null}
{"Title":"Robust and adaptive algorithms for online portfolio selection","keywords (cleaned)":"Adaptive systems;Portfolio allocation;Quantitative trading strategies;Statistics","Abstract":"We present an online approach to portfolio selection. The motivation is within the context of algorithmic trading, which demands fast and recursive updates of portfolio allocations as new data arrives. In particular, we look at two online algorithms: Robust-Exponentially Weighted Least Squares (R-EWRLS) and a regularized Online minimum Variance algorithm (O-VAR). Our methods use simple ideas from signal processing and statistics, which are sometimes overlooked in the empirical financial literature. The two approaches are evaluated against benchmark allocation techniques using four real data sets. Our methods outperform the benchmark allocation techniques in these data sets in terms of both computational demand and financial performance. ","SELECTED":null}
{"Title":"From a calculus to an execution environment for stream processing","keywords (cleaned)":"Sawzall;Calculations;CQL;Streamit;Software architecture;Semantics;Intermediate languages;Domain specific languages;Data processing;Intermediate languages;Stream processing;Streamit;Domain specific languages","Abstract":"At one level, this paper is about River, a virtual execution environment for stream processing. Stream processing is a paradigm well-suited for many modern data processing systems that ingest high-volume data streams from the real world, such as audio\/video streaming, high-frequency trading, and security monitoring. One attractive property of stream processing is that it lends itself to parallelization on multicores, and even to distribution on clusters when extreme scale is required. Stream processing has been co-evolved by several communities, leading to diverse languages with similar core concepts. Providing a common execution environment reduces language development effort and increases portability. We designed River as a practical realization of Brooklet, a calculus for stream processing. So at another level, this paper is about a journey from theory (the calculus) to practice (the execution environment). The challenge is that, by definition, a calculus abstracts away all but the most central concepts. Hence, there are several research questions in concretizing the missing parts, not to mention a significant engineering effort in implementing them. But the effort is well worth it, because using a calculus as a foundation yields clear semantics and proven correctness results. Copyright ","SELECTED":null}
{"Title":"Nash welfare allocation problems: Concrete issues","keywords (cleaned)":"Social welfare;Multi agent systems;Security of data;Agent society;Optimal allocation;Multiagent systems;Intelligent agents;Negotiations;Resource allocation;Behavioral research;Multi-Agent;Allocation problems;Automated trading;Nash welfare","Abstract":"The allocation of m resources between n agents is an AI problem with a great practical interest for automated trading. The general question is how to configure the behavior of bargaining agents to induce a socially optimal allocation. The literature contains many proposals for calculating a social welfare but the Nash welfare seems to be the one which has the most interesting properties for a fair agent society. It guarantees that all resources are fairly distributed among agents respecting their own preferences. This article shows first that the computation of this welfare is a difficult problem, contrary to common intuition. Many counter-examples describe the pitfalls of this resolution. In a second step, we describe our distributed multi-agent solution based on a specific agent's behavior and the results we get on difficult instances. We finally claim that this anytime solution is the only one able to effectively address this problem of obvious practical interest. ","SELECTED":null}
{"Title":"A model for optimal execution of atomic orders","keywords (cleaned)":"Standard optimization;Optimal execution strategy;Algorithmic trading;Atomic orders;Nonlinear optimization model;Objective functions;Optimization;Nonlinear programming;Atoms;Convex programming;Optimization models;Stock exchange;Strategy optimization;Representative sample;Commerce;Execution time;Common market;Algorithms;Basic elements;Execution strategies;Probability functions;Time points;Automated trading;Mathematical models","Abstract":"Atomic Orders are the basic elements of any algorithm for automated trading in electronic stock exchanges. The main concern in their execution is achieving the most efficient price. We propose two optimal strategies for the execution of atomic orders based on minimization of impact and volatility costs. The first considered strategy is based on a relatively simple nonlinear optimization model while the second allows re-optimization at some time point within a given execution time. In both cases a combination of market and limit orders is used. The key innovation in our approach is the introduction of a Fill Probability function which allows a combination of market and limit orders in the two optimization models we are discussing in this paper. Under certain conditions the objective functions of both considered problems are convex and therefore standard optimization tools can be applied. The efficiency of the resulting strategies is tested against two benchmarks representing common market practice on a representative sample of real trading data. ","SELECTED":null}
{"Title":"Services innovation transforming B2B gateway","keywords (cleaned)":"Services;Integration;Decision support systems;Electric energy storage;Commerce;Electronic data interchange;Telecommunication systems;SOA;Decision supports;EDI;ebXML","Abstract":"The objective of collaborative Business-to-Business (B2B) technologies is to minimize the gap between the trading partners. In the early 80s, the need for interactions among companies and their trading partners led to the development of Electronic Data Interchange (EDI) which for many years has been the most dependable way for companies to exchange B2B documents. However the deep learning curve and the cost of the required resources to support EDI prevented Small and Medium Enterprises (SMEs) to adopt the technology in the automated trading processes. ebXML was introduced to overcome the excising shortcomings of EDI, but has its own limitations mainly reflected in its capability in providing decision support analysis and large scale concurrent applications. In this paper, a distributed user centric B2B gateway approach is presented to support trading partners. The gateway takes the leverages of services innovation integrating resources from traditional ebusiness environments with loosely coupled decision support resources to complete trading intelligently. The Australian retail sector has been chosen as an application domain in understanding the impact of the proposed solution. ","SELECTED":null}
{"Title":"Towards cooperation among competitive trader agents","keywords (cleaned)":"Multi agent systems;Technical analysis;Competitive multi-agent systems;Global performance;Architectural approach;Technical analysis indexes;Multiagent systems;Learning algorithms;Commerce;Set theory;Learning skills;Simulation experiments;Autonomous agents;Multiagent architecture;Stock market;Stock price;Single-agent;Information systems;Electric ship equipment;Automated trading;AI techniques;Decision process","Abstract":"In order to manage their portfolios in stock markets, i.e., to determine buy and sell signals, human traders use a set of algorithms created by economists, which are based on stock prices series. These algorithms are usually referred as technical analysis. However, traders prefer to use a combination of several algorithms as indicators, rather than choosing a single one. The several signals provided are used to determine the trader order to buy or sell some stocks, or even to decide to not submit any order. In the last years, some researchers have tried to create new algorithms with learning skills in order to produce autonomous automatic traders, some of them using AI techniques. Inspired by the human traders' decision processes, our architectural approach composes heterogeneous autonomous trader agents in a competitive multiagent system. This architecture allows the use of several algorithms, based on different technical analysis indexes to manage portfolios. We have implemented this architecture and we have performed a set of simulation experiments using real-data from NASDAQ stock market. The experimental results were compared to the performance of single agents playing alone, and a better global performance was observed when traders compete with each other for resources. These preliminary results indicate that competition among agents, as proposed here, may reach very good results, even among agents created to act alone in this kind of market.","SELECTED":null}
{"Title":"An agent strategy for automated stock market trading combining price and order book information","keywords (cleaned)":"Computer simulation;Cost accounting;Information analysis;Inventory control;Strategic planning;Automated stock market;Historic order book data;NASDAQ market;Intelligent agents","Abstract":"This paper proposes a novel automated agent strategy for stock market trading, developed in the context of the Penn-Lehman Automated Trading (PLAT) simulation platform [1]. We provide a comprehensive experimental validation of our strategy using historic order book data from the NASDAQ market.","SELECTED":null}
{"Title":"Temporal Attention-Augmented Bilinear Network for Financial Time-Series Data Analysis","keywords (cleaned)":"Feedforward neural networks;Time series;Forecasting;Neural networks;Data handling;financial data analysis;Computer architecture;Electronic trading;Data reduction;Network layers;time-series prediction;Correlation theory;Data structures;Computational model;Commerce;Stochastic systems;Network architecture;Information analysis;financial data analysis;Finance;Feedforward neural networks;Bilinear projection;Time series analysis;temporal attention;Time series prediction;Mathematical models","Abstract":"Financial time-series forecasting has long been a challenging problem because of the inherently noisy and stochastic nature of the market. In the high-frequency trading, forecasting for trading purposes is even a more challenging task, since an automated inference system is required to be both accurate and fast. In this paper, we propose a neural network layer architecture that incorporates the idea of bilinear projection as well as an attention mechanism that enables the layer to detect and focus on crucial temporal information. The resulting network is highly interpretable, given its ability to highlight the importance and contribution of each temporal instance, thus allowing further analysis on the time instances of interest. Our experiments in a large-scale limit order book data set show that a two-hidden-layer network utilizing our proposed layer outperforms by a large margin all existing state-of-the-art results coming from much deeper architectures while requiring far fewer computations. ","SELECTED":null}
{"Title":"Deep Learning can Replicate Adaptive Traders in a Limit-Order-Book Financial Market","keywords (cleaned)":"Learning neural networks;Artificial intelligence;Intelligent agents;Electronic trading;Proof of concept;Financial engineering;Facsimile;Automated trading;Automation;Correlation theory;Intelligent agents;Algorithmic trading system;Electronic market;Deep learning;Real time systems;Financial markets;Market mechanisms;Commerce;Deep learning;Financial markets;Financial engineering;Automated trading;Conventional approach","Abstract":"We report successful results from using deep learning neural networks (DLNNs) to learn, purely by observation, the behavior of profitable traders in an electronic market closely modelled on the limit-order-book (LOB) market mechanisms that are commonly found in the real-world global financial markets for equities (stocks shares), currencies, bonds, commodities, and derivatives. Successful real human traders, and advanced automated algorithmic trading systems, learn from experience and adapt over time as market conditions change; our DLNN learns to copy this adaptive trading behavior. A novel aspect of our work is that we do not involve the conventional approach of attempting to predict time-series of prices of tradeable securities. Instead, we collect large volumes of training data by observing only the quotes issued by a successful sales-trader in the market, details of the orders that trader is executing, and the data available on the LOB (as would usually be provided by a centralized exchange) over the period that the trader is active. In this paper we demonstrate that suitably configured DLNNs can learn to replicate the trading behavior of a successful adaptive automated trader, an algorithmic system previously demonstrated to outperform human traders. We also demonstrate that DLNNs can learn to perform better (i.e., more profitably) than the trader that provided the training data. We believe that this is the first ever demonstration that DLNNs can successfully replicate a human-like, or super-human, adaptive trader operating in a realistic emulation of a real-world financial market. Our results can be considered as proof-of-concept that a DLNN could, in principle, observe the actions of a human trader in a real financial market and over time learn to trade equally as well as that human trader, and possibly better. ","SELECTED":null}
{"Title":"Benchmark dataset for mid-price forecasting of limit order book data with machine learning methods","keywords (cleaned)":"Experimental comparison;Time series;Forecasting;mid-price;Electronic trading;Limit order book;High-frequency trading;ridge regression;Limit order book;Correlation theory;Data representations;High-frequency trading;Experimental protocols;Financial markets;single hidden feedforward neural network;Expert systems;ridge regression;Commerce;Benchmarking;Machine learning;Learning systems;Regression analysis;Feedforward neural networks;Machine learning methods","Abstract":"Managing the prediction of metrics in high-frequency financial markets is a challenging task. An efficient way is by monitoring the dynamics of a limit order book to identify the information edge. This paper describes the first publicly available benchmark dataset of high-frequency limit order markets for mid-price prediction. We extracted normalized data representations of time series data for five stocks from the Nasdaq Nordic stock market for a time period of 10 consecutive days, leading to a dataset of \u223c4,000,000 time series samples in total. A day-based anchored cross-validation experimental protocol is also provided that can be used as a benchmark for comparing the performance of state-of-the-art methodologies. Performance of baseline approaches are also provided to facilitate experimental comparisons. We expect that such a large-scale dataset can serve as a testbed for devising novel solutions of expert systems for high-frequency limit order book data analysis. ","SELECTED":null}
{"Title":"Artificial Intelligence, the Missing Piece of Online Education?","keywords (cleaned)":"Intelligent robots;Artificial intelligence;Electronic trading;Robots;robotic strategy;Human behaviors;Teaching and learning;games;Online education;Students;Student learning outcomes;Commerce;Teaching;classroom experiments;Online education;E-learning;Artificial intelligence;Explosive growth;Competitive markets;Behavioral research;Automated trading","Abstract":"Despite the recent explosive growth of online education, it still suffers from suboptimal learning efficacy, as evidenced by low student completion rates. This deficiency can be attributed to the lack of facetime between teachers and students, and amongst students themselves. In this article, we use the teaching and learning of economics as a case study to illustrate the application of artificial intelligence (AI) based robotic players to help engage students in online, asynchronous environments, therefore, potentially improving student learning outcomes. In particular, students could learn about competitive markets by joining a market full of automated trading robots who find every chance to arbitrage. Alternatively, students could learn to play against other humans by playing against robotic players trained to mimic human behavior, such as anticipating spiteful rejections to unfair offers in the Ultimatum Game where a proposer offers a particular way to split the pot that the responder can only accept or reject. By training robotic players with past data, possibly coming from different country and regions, students can experience and learn how players in different cultures might make decisions under the same scenario. AI can thus help online educators bridge the last mile, incorporating the benefit of both online and in-person learning. ","SELECTED":null}
{"Title":"Implications of High-Frequency Trading for Security Markets","keywords (cleaned)":"Flash crash;High-frequency trading;Liquidity;literature survey;Volatility","Abstract":"High-frequency trading (HFT) has grown substantially in recent years due to fast-paced technological developments and their rapid uptake, particularly in equity markets. This review investigates how HFT could evolve and, by developing a robust understanding of its effects, identifies potential risks and opportunities that HFT could present in terms of financial stability and other market outcomes such as volatility, liquidity, price efficiency, and price discovery. Despite commonly held negative perceptions, the available evidence indicates that HFT and algorithmic trading may have several beneficial effects on markets. However, these types of trading may cause instabilities in financial markets in specific circumstances. Carefully chosen regulatory measures are needed to address concerns in the shorter term. However, further work is needed to inform policies in the longer term, particularly in view of likely uncertainties and lack of data. This work will be vital in supporting evidence-based regulation in this controversial and rapidly evolving field. ","SELECTED":null}
{"Title":"Disclosure \u201cScriptability\u201d","keywords (cleaned)":"disclosure;financial reporting;G10;G12;G14;G24;G30;G39;G41;High-frequency trading;information processing costs;M40;M41;M49;textual analysis;XBRL","Abstract":"In response to the increasing use of computer programs to process firm disclosures, this registered report develops a new measure of \u201cscriptability\u201d that reflects computerized, rather than human, information processing costs. We validate our measure using SEC filing-derived data from prior research and identify firm and disclosure characteristics related to it. In our planned hypothesis tests, we find some evidence that the speed of the market response to filings increases with scriptability, but find little evidence that scriptability affects the incidence and speed of news dissemination by Dow Jones. In additional analyses, we find that scriptability exhibits both positive and negative associations with changes in information asymmetry between market participants, depending on the filing, trading window, and measure examined. We also find little evidence that XBRL interacts with scriptability in a meaningful way. Overall, our study broadens our understanding of information processing costs and provides opportunities for new avenues of research. Copyright ","SELECTED":null}
{"Title":"A meta-grammatical evolutionary process for portfolio selection and trading","keywords (cleaned)":"Investments;Computational grammars;Technical analysis;Commerce;Grammatical evolution;Macroeconomic analysis;Fundamental analysis;Automated trading systems;Meta-GE;Macroeconomic analysis","Abstract":"This study presents the implementation of an automated trading system that uses three critical analyses to determine time-decisions and portfolios for investment. The approach is based on a meta-grammatical evolution methodology that combines technical, fundamental and macroeconomic analysis on a hybrid top-down paradigm. First, the method provides a low-risk portfolio by analyzing countries and industries. Next, aiming to focus on the most robust companies, the system filters the portfolio by analyzing their economic variables. Finally, the system analyzes prices and volumes to optimize investment decisions during a given period. System validation involves a series of experiments in the European financial markets, which are reflected with a data set of over nine hundred companies. The final solutions have been compared with static strategies and other evolutionary implementations and the results show the effectiveness of the proposal. ","SELECTED":null}
{"Title":"The regulatory, technology and market 'dark arts trilogy' of high frequency trading: A research agenda","keywords (cleaned)":"Financial markets;Electronic trading;Human activities;Commerce;Algorithms;Algorithmic trading;Financial trading;High frequency trading;Finance;Research agenda;Asymmetry;Dow Jones Industrial averages;Regulation;High-frequency trading","Abstract":"Computerization has transformed financial markets with high frequency trading displacing human activity with proprietary algorithms to lower latency, reduce intermediary costs, enhance liquidity and increase transaction speed. Following the \"Flash Crash\" of 2010 which saw the Dow Jones Industrial Average plunge 1000 points within minutes, high frequency trading has come under the radar of multi-jurisdictional regulators. Combining a review of the extant literature on high frequency trading with empirical data from interviews with financial traders, computer experts and regulators, we develop concepts of regulatory adaptation, technology asymmetry and market ambiguity to illustrate the 'dark art' of high frequency trading. Findings show high frequency trading is a multi-faceted, complex and secretive practice. It is implicated in market events, but correlation does not imply causation, as isolating causal mechanisms from interconnected automated financial trading is highly challenging for regulators who seek to monitor algorithmic trading across multiple jurisdictions. This article provides information systems researchers with a set of conceptual tools for analysing high frequency trading. ","SELECTED":null}
{"Title":"Quadratic Hawkes processes for financial prices","keywords (cleaned)":"Financial prices;Hawkes processes;High-frequency trading;Market microstructure;Pearsons diffusion;Time-reversal asymmetry;Volatility modelling","Abstract":"We introduce and establish the main properties of QHawkes (\u2018Quadratic\u2019 Hawkes) models. QHawkes models generalize the Hawkes price models introduced in Bacry and Muzy [Quant. Finance, 2014, 14(7), 1147\u20131166], by allowing feedback effects in the jump intensity that are linear and quadratic in past returns. Our model exhibits two main properties that we believe are crucial in the modelling and the understanding of the volatility process: first, the model is time-reversal asymmetric, similar to financial markets whose time evolution has a preferred direction. Second, it generates a multiplicative, fat-tailed volatility process, that we characterize in detail in the case of exponentially decaying kernels, and which is linked to Pearson diffusions in the continuous limit. Several other interesting properties of QHawkes processes are discussed, in particular the fact that they can generate long memory without necessarily being at the critical point. A non-parametric fit of the QHawkes model on NYSE stock data shows that the off-diagonal component of the quadratic kernel indeed has a structure that standard Hawkes models fail to reproduce. We provide numerical simulations of our calibrated QHawkes model which is indeed seen to reproduce, with only a small amount of quadratic non-linearity, the correct magnitude of fat-tails and time reversal asymmetry seen in empirical time series. ","SELECTED":null}
{"Title":"Sequential quantiles via Hermite series density estimation","keywords (cleaned)":"Online distribution function estimation;Online quantile estimation;Sequential distribution function estimation;Sequential quantile estimation","Abstract":"Sequential quantile estimation refers to incorporating observations into quantile estimates in an incremental fashion thus furnishing an online estimate of one or more quantiles at any given point in time. Sequential quantile estimation is also known as online quantile estimation. This area is relevant to the analysis of data streams and to the one-pass analysis of massive data sets. Applications include network traffic and latency analysis, real time fraud detection and high frequency trading. We introduce new techniques for online quantile estimation based on Hermite series estimators in the settings of static quantile estimation and dynamic quantile estimation. In the static quantile estimation setting we apply the existing Gauss-Hermite expansion in a novel manner. In particular, we exploit the fact that Gauss-Hermite coefficients can be updated in a sequential manner. To treat dynamic quantile estimation we introduce a novel expansion with an exponentially weighted estimator for the Gauss-Hermite coefficients which we term the Exponentially Weighted Gauss-Hermite (EWGH) expansion. These algorithms go beyond existing sequential quantile estimation algorithms in that they allow arbitrary quantiles (as opposed to pre-specified quantiles) to be estimated at any point in time. In doing so we provide a solution to online distribution function and online quantile function estimation on data streams. In particular we derive an analytical expression for the CDF and prove consistency results for the CDF under certain conditions. In addition we analyse the associated quantile estimator. Simulation studies and tests on real data reveal the Gauss-Hermite based algorithms to be competitive with a leading existing algorithm. ","SELECTED":null}
{"Title":"Detecting intraday financial market states using temporal clustering","keywords (cleaned)":"Financial market states;Market microstructure;State space reduction;Temporal clustering","Abstract":"We propose the application of a high-speed maximum likelihood clustering algorithm to detect temporal financial market states, using correlation matrices estimated from intraday market microstructure features. We first determine the ex-ante intraday temporal cluster configurations to identify market states, and then study the identified temporal state features to extract state signature vectors (SSVs) which enable online state detection. The SSVs serve as low-dimensional state descriptors which can be used in learning algorithms for optimal planning in the high-frequency trading domain. We present a feasible scheme for real-time intraday state detection from streaming market data feeds. This study identifies an interesting hierarchy of system behaviour which motivates the need for timescale-specific state space reduction for participating agents. ","SELECTED":null}
{"Title":"Trading volume in financial markets: An introductory review","keywords (cleaned)":"Econophysics;SIAH;Financial markets;Financial data processing;Mixture of distributions;Commerce;Trading volumes;Trading volumes;MDH;quantitative finance;Large scale systems;Financial analysis;Finance;Econophysicss;Electronic trading;complex systems;Dynamical properties;High-frequency trading","Abstract":"In this article, I introduce a short review on the statistical and dynamical properties of the high-frequency trading volume and its relation to other financial quantities such as the price fluctuations and trading value. In addition, I compare these results - which were obtained within the framework of applications of Physics to quantitative financial analysis - with the mainstream financial hypotheses of mixture of distributions (MDH) and sequential arrival of information (SIAH). ","SELECTED":null}
{"Title":"High Frequency Trading and US Stock Market Microstructure: A Study of Interactions between Complexities, Risks and Strategies Residing in U.S. Equity Market Microstructure","keywords (cleaned)":"Strategies|High Frequency|information|nse|r|trading risks|market microstructure|Regulation|Trading|risk|innovation|algorithms|High frequency|human|speed|microstructure|market system|expo|R|Regulation NMS|Risk|Stock|trading|trading strategies|fragmentation|high frequency|spread|market|systemic risk|strategies|technological innovation|Market|asymmetry|risk averse|Risks|regulation|liquidity|algorithm|competition|Stock Market|dynamics|Microstructure|stock|price shocks|High frequency trading|asset class|high frequency trading|mes|stakeholder|order book|Market Microstructure|risks|High Frequency Trading","Abstract":"We examine the conditions, complexities and risks of a fragmented market microstructure to contextualize the role of algorithmic and high frequency trading in the US equity markets. The establishment of a national market system and Regulation NMS was meant to promote competition, recognizing the evolution and changing dynamics introduced by technological innovation. This evolution and governing rule set has had many positive effects in terms of competition, fee compression, tighter spread potential and volumes. Our paper identifies certain unintended consequences and complexities of the national market system including fragmentation, sub second quoting and trading, complex order types, data asymmetry, technological innovation, unique strategies and the algorithms that power them. When acting in concert, these complexities give rise to opportunities as well as emerging risks. This high-speed system can be unstable and susceptible to inherent conflicts of interest, market abuse and price shocks. These shocks can be amplified by positive feedback loops accelerating single stock declines and also posing systemic risks in time scales beyond real-time physical human comprehension and reaction times. Furthermore they can produce contagion, which we refer to as 'Flash Splashes' caused by rapid withdrawals and injections of liquidity in increasingly linked asset classes, indices, sectors and global liquidity pools. High frequency trading strategies can be both passive and aggressive and usually display risk averse and low inventory characteristics. These strategies leverage fragmentation as they create or capture informational asymmetries. They interact directly with sell side algorithms that can hide intentions, hunt liquidity and sweep the order book. These interactions create market dynamics that can benefit and challenge anyone exposed to US equity markets. Every market participant has a risk profile unique to their strategy and objective and while regulations will be enriched or revised and certain unfair practices eliminated great attention should be paid to understanding modern high speed trading risks and both the positive and negative impacts on all stakeholders. We have examined the regulations, complexities and risks to bring clarity and understanding to the current trading ecosystem for its users. \" ","SELECTED":null}
{"Title":"Dynamic conditional correlation multiplicative error processes","keywords (cleaned)":"DCC-GARCH;Gaussian domain;Liquidity risk;Multiplicative error models;Trading processes","Abstract":"We introduce a dynamic model for multivariate processes of (non-negative) high-frequency trading variables revealing time-varying conditional variances and correlations. Modeling the variables' conditional mean processes using a multiplicative error model, we map the resulting residuals into a Gaussian domain using a copula-type transformation. Based on high-frequency volatility, cumulative trading volumes, trade counts and market depth of various stocks traded at the NYSE, we show that the proposed transformation is supported by the data and allows capturing (multivariate) dynamics in higher order moments. The latter are modeled using a DCC-GARCH specification. We suggest estimating the model by composite maximum likelihood which is sufficiently flexible to be applicable in high dimensions. Strong empirical evidence for time-varying conditional (co-)variances in trading processes supports the usefulness of the approach. Taking these higher-order dynamics explicitly into account significantly improves the goodness-of-fit and out-of-sample forecasts of the multiplicative error model. ","SELECTED":null}
{"Title":"Bulk price forecasting using spark over NSE data set","keywords (cleaned)":"ARIMA;ARMA;Time series;Arches;Forecasting;Acoustic streaming;Electronic trading;Electric sparks;Financial forecasting;RDD;NSE;Statistics;econometrics;Financial markets;Scala;Streaming;Spark;Finance;Arches;GARCH;Economics;Big data;Data mining","Abstract":"Financial forecasting is a widely applied area, making use of statistical prediction using ARMA, ARIMA, ARCH and GARCH models on stock prices. Such data have unpredictable trends and non-stationary property which makes even the best long term predictions grossly inaccurate. The problem is countered by keeping the prediction shorter. These methods are based on time series models like auto regressions and moving averages, which require computationally costly recurring parameter estimations. When the data size becomes considerable, we need Big data tools and techniques, which do not work well with time series computations. In this paper we discuss such a finance domain problem on the Indian National Stock Exchange (NSE) data for a period of one year. Our main objective is to device a light weight prediction for the bulk of companies with fair accuracy, useful enough for algorithmic trading. We present a minimal discussion on these classical models followed by our Spark RDD based implementation of the proposed fast forecast model and some results we have obtained. ","SELECTED":null}
{"Title":"Time series analysis indicators under directional changes: The case of saudi stock market","keywords (cleaned)":"Automated trading;Directional changes;Financial forecasting;Financial markets;Saudi Stock Market","Abstract":"We introduce a set of time series analysis indicators under an event based framework of directional changes (DC) and overshoots. Our aim is to map continuous financial market price data into the so-called DC Framework -A state based discretization of basically dissected price time series. The DC framework analysis relied on understanding the price time series as an event-based process, as an alternative of focusing on their stochastic character. Defining a scheme for state reduction of DC Framework, we show that it has a dependable hierarchical structure that permits for analysis of financial data. We show empirical examples within the Saudi Stock Market. The new DC indicators represent the foundation of a completely new generation of financial tools for studying volatility, risk measurement, and building advanced forecasting and automated trading models. ","SELECTED":null}
{"Title":"A reexamination of high frequency trading regulation effectiveness in an artificial market framework","keywords (cleaned)":"Market liquidity;High frequency trading market regulation;Multi agent systems;Financial data processing;Market liquidity and volatility;Commerce;Agent-based model;Trading volumes;Computational methods;Market regulation;Electronic trading;Artificial markets;Autonomous agents;High-frequency trading","Abstract":"In this paper we analyze the impact of the French cancel order tax on market quality measured by market liquidity and volatility. Additionally, this paper raises the question whether this tax leads to reduction of high-frequency trading (HFT) activities and a declining in trading volume. Moreover, we test market rules that have not been yet introduced using artificial market framework. ","SELECTED":null}
{"Title":"Intelligent trading architecture","keywords (cleaned)":"Communication modeling;Robot control software;Microprocessor chips;Reference implementation;Scalable parallel computing;High performance computing systems;Algorithmic trading;Commerce;Algorithmic trading;system and software architectures;System architectures;Correlation theory;information theory;Automated trading systems;Robots;Computer architecture;Software architecture;Autonomous agents;Automated trading systems","Abstract":"This work presents the Intelligent Trading Architecture (ITA), which is a new automated trading system architecture that supports multiple strategies for multiple market conditions through hierarchical trading signals generation. The central idea of the proposed system architecture is decomposing the trading problem into a set of tasks that are handled by distributed autonomous agents under a minimal central coordination. With this kind of architecture, we can take advantage of currently available and future high-performance computing systems. These systems, due to the way computer architecture has evolved in the recent past and foreseeable future, are composed of multiple processor cores. We are implementing the ITA software architecture employing the Carnegie Mellon Navigation (CARMEN) robot control software and using a publish\/subscribe communication model. Together, CARMEN and this communication model allow the implementation of high-performance, scalable parallel computing systems that leverage the architecture of multi-core systems. For this work, we evaluated the data structures and algorithms employed by the symbol module of the ITA software architecture, which is responsible for maintaining the synchronized local copies of exchanges limit order books (LOB) for the instruments traded by the system. Our LOB implementation strongly outperformed a reference implementation in all evaluated parameters by more than one order of magnitude in some cases, achieving average throughputs of 4 million orders\/s when creating new orders, 3 million orders\/s when changing existing orders, and 17 million orders\/s when querying orders. Copyright ","SELECTED":null}
{"Title":"Knowledge discovery in dynamic data using neural networks","keywords (cleaned)":"Calculation cost;Electronic trading;Pattern recognition;Network environments;Distributed computer systems;Commerce;Automated trading systems;Key characteristics;Pattern recognition systems;Cloud computing;Neural networks;Automated trading systems;Backpropagation training;Training patterns;Algorithms;Trading systems;Knowledge discovery;Neural network algorithm;Data mining","Abstract":"The paper proposes a new approach to implement common neural network algorithms in the network environment. In our experimental study we have used three different types of neural networks based on Hebb, daline and backpropagation training rules. Our goal was to discover important market (Forex) patterns which repeatedly appear in the market history. Developed classifiers based upon neural networks should effectively look for the key characteristics of the patterns in dynamic data. We focus on reliability of recognition made by the described algorithms with optimized training patterns based on the reduction of the calculation costs. To interpret the data from the analysis we created a basic trading system and trade all recommendations provided by the neural network. ","SELECTED":null}
{"Title":"Technology upgrades in emerging equity markets: Effects on liquidity and trading activity","keywords (cleaned)":"Emerging markets;High frequency trading;Liquidity;Market microstructure;Technological upgrade","Abstract":"This study examines the effects of technological changes on liquidity of stock markets. Utilizing daily data of 361 stocks from 10 emerging market exchanges, namely Colombia, Indonesia, Johannesburg, Korea, Malaysia, Mexico, Russia, Shanghai, Shenzhen and Thailand, a panel data regression analysis shows that technological upgrade decreases the bid-ask spread and increases trading activity. In other words, launching a more sophisticated trading platform contributes to the overall liquidity of the market. ","SELECTED":null}
{"Title":"Geometric semantic genetic programming for financial data","keywords (cleaned)":"Hill-climbing;Index;Commodity;Commerce;Fitness landscape;Semantics;Genetic algorithms;Geometry;Regression analysis;Exchange rates;Exchange rates;Automated trading;Hill climbing;Genetic programming","Abstract":"We cast financial trading as a symbolic regression problem on the lagged time series, and test a state of the art symbolic regression method on it. The system is geometric semantic genetic programming, which achieves good performance by converting the fitness landscape to a cone landscape which can be searched by hill-climbing. Two novel variants are introduced and tested also, as well as a standard hill-climbing genetic programming method. Baselines are provided by buy-and-hold and ARIMA. Results are promising for the novel methods, which produce smaller trees than the existing geometric semantic method. Results are also surprisingly good for standard genetic programming. New insights into the behaviour of geometric semantic genetic programming are also generated. ","SELECTED":null}
{"Title":"Low latency FPGA acceleration of market data feed arbitration","keywords (cleaned)":"Commerce;Computer architecture;Electronic data interchange;Acceleration approach;Automated trading;Commercial design;Downstream applications;Hardware-accelerated;Messaging protocols;Network interface cards;Windowing methods;Network architecture","Abstract":"A critical source of information in automated trading is provided by market data feeds from financial exchanges. Two identical feeds, known as the A and B feeds, are used in reducing message loss. This paper presents a reconfigurable acceleration approach to A\/B arbitration, operating at the network level, and supporting any messaging protocol. The key challenges are: providing efficient, low latency operations; supporting any market data protocol; and meeting the requirements of downstream applications. To facilitate a range of downstream applications, one windowing mode prioritising low latency, and three dynamically configurable windowing methods prioritising high reliability are provided. We implement a new low latency, high throughput architecture and compare the performance of the NASDAQ TotalView-ITCH, OPRA and ARCA market data feed protocols using a Xilinx Virtex-6 FPGA. The most resource intensive protocol, TotalView-ITCH, is also implemented in a Xilinx Virtex-5 FPGA within a network interface card. We offer latencies 10 times lower than an FPGA-based commercial design and 4.1 times lower than the hardware-accelerated IBM PowerEN processor, with throughputs more than double the required 10Gbps line rate. ","SELECTED":null}
{"Title":"Novel automated multi-agent investment system based on simulation of self-excitatory oscillations","keywords (cleaned)":"Agents-based modelling;Automated trading systems;Self-excited oscillations;Social mediums","Abstract":"Modern financial markets are dominated by the algorithmic trading tools, which essentially changed previously observed regularities in the investment data sets. In order to develop robust and reliable trading algorithms new approaches have to be employed. In this paper we investigate how to (i) model spread of self-excited oscillations in social mediums and (ii) simulate data series of rare events' (e.g. crises). We proposed a novel multi-agent investment simulation system, composed from a multitude of artificial investing agents. Consequently, we developed a simulation tool, which can deal with self-excited oscillations and high dimensionality small sample size problems arising in the modern financial markets. The novel multi-agent system was tested with real market ant synthetic data for the period between 2007 and 2013. In an out-of-sample regime, the novel approach outperformed benchmark trading strategies. ","SELECTED":null}
{"Title":"Intraday liquidity patterns in limit order books","keywords (cleaned)":"Algorithmic trading;High-frequency data;Limit order book;Liquidity;Market impacts;Trade scheduling","Abstract":"Purpose: Algorithmic trading attempts to reduce trading costs by selecting optimal trade execution and scheduling algorithms. Whilst many common approaches only consider the bid-ask spread when measuring market impact, the authors aim to analyse the detailed limit order book data, which has more informational content. Design\/methodology\/approach: Using data from the London Stock Exchange's electronic SETS platform, the authors transform limit order book compositions into volume-weighted average price curves and accordingly estimate market impact. The regression coefficients of these curves are estimated, and their intraday patterns are revealed using a nonparametric kernel regression model. Findings: The authors find that market impact is nonlinear, time-varying, and asymmetric. Inferences drawn from marginal probabilities regarding Granger-causality do not show a significant impact of slope coefficients on the opposite side of the limit order book, thus implying that each side of the market is simultaneously rather than sequentially influenced by prevailing market conditions. Research limitations\/implications: Results show that intraday seasonality patterns of liquidity may be exploited through trade scheduling algorithms in an attempt to minimise the trading costs associated with large institutional trades. Originality\/value: The use of the detailed limit order book to reveal intraday patterns in liquidity provision offers better insight into the interactions of market participants. Such valuable information cannot be fully recovered from the traditional transaction data-based approaches. ","SELECTED":null}
{"Title":"Location-based matching in publish\/subscribe revisited","keywords (cleaned)":"Pub\/sub;Complex event processing;Emerging applications;Publish\/subscribe;Algorithms;(location-based) publish\/subscribe;Algorithmic trading;Complex event processing;Middleware;Data dissemination;Location-based information;Processing applications;Intrusion Detection Systems;Scalable algorithms;Content-based publish;Matching algorithm;Location based","Abstract":"Event processing is gaining rising interest in industry and in academia. The common application pattern is that event processing agents publish events while other agents subscribe to events of interest. Extensive research has been devoted to developing efficient and scalable algorithms to match events with subscribers' interests. The predominant abstraction used in this context is the content-based publish\/subscribe (pub\/sub) paradigm for modeling an event processing application. Applications that have been referenced in this space include emerging applications in co-spaces that rely on location-based information [1, 7], algorithmic trading and (financial) data dissemination [13], and intrusion detection system [4]. In this work, we focus primarily on the role of state-of-the-art matching algorithms in location-based pub\/sub applications. ","SELECTED":null}
{"Title":"Prediction based - High frequency trading on financial time series","keywords (cleaned)":"Financial data processing;Expected values;Commerce;Algorithmic trading;Forecasting;Time series analysis;Nonlinear estimator;Prediction-based;Financial time series;Electronic trading;Feedforward neural networks (FFNN);Feedforward neural networks;Trading strategies;High-frequency trading","Abstract":"In this paper we investigate prediction based trading on financial time series assuming general AR(J) models. A suitable nonlinear estimator for predicting the future values will be provided by a properly trained FeedForward Neural Network (FFNN) which can capture the characteristics of the conditional expected value. In this way, one can implement a simple trading strategy based on the predicted future value of the asset price and comparing it to the current value. The method is tested on FOREX data series and achieved a considerable profit on the mid price. In the presence of the bid-ask spread, the gain is smaller but it still ranges in the interval 2-6 percent in 6 months without using any leverage. FFNNs can provide fast prediction which can give rise to high frequency trading on intraday data series.","SELECTED":null}
{"Title":"UTC time transfer for high frequency trading using is-95 CDMA base station transmissions and IEEE-1588 precision time protocol","keywords (cleaned)":"Capital markets;CDMA base stations;Competitive advantage;High speed computers;High-frequency trading;Microsecond accuracy;Precision time protocols;Telecommunications industry associations;Commerce;Competition;Industry;Platinum alloys;Regulatory compliance;Code division multiple access","Abstract":"Today, High Frequency Trading (HFT) requires low latency and knowledge of UTC time in capital markets where a difference of better than one millisecond provides a competitive advantage to trading firms. HFTs co-locate high speed computer systems in rented space next to the market centers to reduce latency of raw data, giving them advanced knowledge of the market orders. Better than 10-microsecond accuracy to UTC can be achieved using Code Division Multiple Access (CDMA), in compliance with Telecommunications Industry Association (TIA\/EIA) Standard IS-95 and IEEE-1588 Precision Time Protocol (PTP) without routing an external antenna. This paper will present a method to provide UTC time using CDMA and PTP to these trading firms, why microsecond accuracy is required, and the performance of UTC time transfer as compared to GPS-derived UTC time. ","SELECTED":null}
{"Title":"Enhancing automated trading engines to cope with news-related liquidity shocks","keywords (cleaned)":"Signal processing;Liquidity;Commerce;Forecasting;Automation;Publishing;Information systems;e-Finance;Text mining;Automated trading;Data mining;And simulation","Abstract":"Liquidity constitutes one of the main determinants of implicit transaction costs. Deriving optimal execution strategies that minimize transaction costs, automated trading engines need to forecast future liquidity levels. By means of an empirical study we provide evidence that the publication of regulatory corporate disclosures is followed by abnormal liquidity levels. As we do not find abnormal liquidity levels prior to the publication, we assume the content to be largely unanticipated. Forecasting models purely based on quantitative input data may therefore not be able to pick up on the liquidity trends in a timely manner. Against this background, we propose two trading signals that allow automated trading engines to appropriately react to news-related liquidity shocks: First, a simple binary \"news\" or \"no news\" signal. Second, a signal that indicates whether or not the publication of a regulatory corporate disclosure will be followed by a negative liquidity shock. Utilizing text mining techniques, the content of the corporate disclosures is analyzed to extract the trading signal. The trading signals are evaluated within a simulation-based use case and turn out to be valuable. We strongly advise developers of automated trading engines to integrate unstructured qualitative data into their models, i.e. the proposed trading signals.","SELECTED":null}
{"Title":"Evolutionary single-position automated trading","keywords (cleaned)":"Artificial intelligence;Automation;Computation theory;Earnings;Finance;Neural networks;Artificial neural network (ANNs);Automated trading;Design and optimization;European;Evolutionary approaches;Evolutionary computation (EC);Evolutionary computation;Heidelberg (CO);Input data;Trading strategies;Commerce","Abstract":"Automated Trading is the activity of buying and selling financial instruments for the purpose of gaining a profit, through the use of automated trading rules. This work presents an evolutionary approach for the design and optimization of artificial neural networks to the discovery of profitable automated trading rules. Experimental results indicate that, despite its simplicity, both in terms of input data and in terms of trading strategy, such an approach to automated trading may yield significant returns. ","SELECTED":null}
{"Title":"A computational exploration of the efficacy of fibonacci sequences in technical analysis and trading","keywords (cleaned)":"Fibonacci geometry;Price patterns;Technical analysis;Trading systems","Abstract":"Among the vast assemblage of technical analysis tools, the ones based on Fibonacci recurrences in asset prices are relatively more scientific. In this paper, we review some of the popular technical analysis methodologies based on Fibonacci sequences and also advance a theoretical rationale as to why security prices may be seen to follow such sequences. We also analyse market data for an indicative empirical validation of the efficacy or otherwise of such sequences in predicting critical security price retracements that may be useful in constructing automated trading systems. ","SELECTED":null}
{"Title":"Automated trading in Agent-Based markets for communication bandwidth","keywords (cleaned)":"Bandwidth marketplaces;Electronic market;Intelligent agents;Software agents","Abstract":"Automated agentsfor electronic markets are very valuable when a large amount of information must be processed quickly or employing human traders is not cost-effective. In markets for communication bandwidth, rights to transmit data are traded through a network. Since demand fluctuates considerably every few seconds, agent-based spot markets provide extra liquidity.This paper considers the design of agents for use in a double auction market for communication bandwidth. The suggested criteria and framework for building adaptive agents are based on statistical decision theory.The proposed agents can differentiate stable from unstable market conditions and respond appropriately. Copyright ","SELECTED":null}
{"Title":"Hybrid approach for automated trading systems","keywords (cleaned)":"Automated trading systems;Hang seng index;Technical analysis;Automation;Database systems;Feature extraction;Fuzzy sets;Inference engines;Knowledge based systems;Statistical methods;Financial data processing","Abstract":"Automated trading systems have been proved to be very helpful to traders and investors. However, most of the existing systems implemented with any statistical or neural network approaches are incapable to give accurate advice because they cannot capture the off-market information for which most of it is non-numeric. In this paper, an automated trading system which integrates the techniques from technical analysis, pattern recognition, and knowledge-based system is presented. The numeric data from the stock is processed by both the statistical method and feature extraction. The intermediate results are then used by the rule-based system which captures the off-market information in form of facts and rules. After that, the advice can be generated through the inference with the data stored in the database. For resolving the inconsistencies among the intermediate results from different components of the system. Concepts from the fuzzy set theories are employed. Further, some data from the past Hang Seng Index (HSI) is used for illustration.","SELECTED":null}
{"Title":"A new multi-period investment strategies method based on evolutionary algorithms","keywords (cleaned)":"Information providing;Investments;Electronic trading;Multi objective;Commerce;Investment decisions;Algorithmic trading;Monte Carlo methods;Evolutionary algorithms;Portfolio theories;Utility functions;Strategic planning;Multi-objective;Portfolio optimization;Investment strategy","Abstract":"This work introduces a new algorithmic trading method based on evolutionary algorithms and portfolio theory. The limitations of traditional portfolio theory are overcome using a multi-period definition of the problem. The model allows the inclusion of dynamic restrictions like transaction costs, portfolio unbalance, and inflation. A Monte Carlo method is proposed to handle these types of restrictions. The investment strategies method is introduced to make trading decisions based on the investor\u2019s preference and the current state of the market. Preference is determined using heuristics instead of theoretical utility functions. The method was tested using real data from the Mexican market. The method was compared against buy-and-holds and single-period portfolios for metrics like the maximum loss, expected return, risk, the Sharpe\u2019s ratio, and others. The results indicate investment strategies perform trading with less risk than other methods. Single-period methods attained the lowest performance in the experiments due to their high transaction costs. The conclusion was investment decisions that are improved when information providing from many different sources is considered. Also, profitable decisions are the result of a careful balance between action (transaction) and inaction (buy-and-hold). ","SELECTED":null}
{"Title":"Concept drift robust adaptive novelty detection for data streams","keywords (cleaned)":"Classification (of information);System change;intermethod comparison;Bioelectric phenomena;Least squares approximations;Error detection;concept drift;Feature extraction;Interactive computer systems;concept drift;Data stream;Statistics;Outlier detection;seizure;Learning algorithms;normalized least mean square;Learning algorithms;Adaptive filters;Real time systems;concept drift;Adaptive filters;Outlier detection;Adaptive filters;unsupervised novelty detection;Entropy;Fuzzy systems;article;electroencephalogram;controlled study;Novelty detection;Data mining;Fault detection","Abstract":"In this paper we study the performance of two original adaptive unsupervised novelty detection methods (NDMs) on data with concept drift. Newly, the concept drift is considered as a challenging data imbalance that should be ignored by the NDMs, and only system changes and outliers represent novelty. The field of application for such NDMs is broad. For example, the method can be used as a supportive method for real-time system fault detection, for onset detection of events in biomedical signals, in monitoring of nonlinearly controlled processes, for event driven automated trading, etc. The two newly studied methods are the error and learning based novelty detection (ELBND) and the learning entropy (LE) based detection. These methods use both the error and weight increments of a (supervised) learning model. Here, we study these methods with normalized least-mean squares (NLMS) adaptive filter, and while the NDMs were studied on various real life tasks, newly, we carry out the study on two types of data streams with concept drift to analyze the general ability for unsupervised novelty detection. The two data streams, one with system changes, second with outliers, represent different novelty scenarios to demonstrate the performance of the proposed NDMs with concept drifts in data. Both tested NDMs work as a feature extractor. Thus, a classification framework is used for the evaluation of the obtained features and NDM benchmarking, where two other NDMs, one based on the adaptive model plain error, second using the sample entropy (SE), are used as the reference for the comparison to the proposed methods. The results show that both newly studied NDMs are superior to the merely use of the plain error of adaptive model and also to the sample entropy based detection while they are robust against the concept drift occurrence. ","SELECTED":null}
{"Title":"How Does High-Frequency Trading Affect Low-Frequency Trading?","keywords (cleaned)":"High-frequency trading;Limit order book;Liquidity;Order execution quality","Abstract":"High-frequency trading dominates trading in financial markets. How it affects the low-frequency trading, however, is still unclear. Using NASDAQ order book data, the authors investigate this question by categorizing orders as either high or low frequency, and examining several measures. They find that high-frequency trading enhances liquidity by increasing the trade frequency and quantity of low-frequency orders. High-frequency trading also reduces the waiting time of low-frequency limit orders and improves their likelihood of execution. The results indicate that high-frequency trading has a liquidity provision effect and improves the execution quality of low-frequency orders. ","SELECTED":null}
{"Title":"Profitability Edge by Dynamic Back Testing Optimal Period Selection for Technical Parameters Optimization, in Trading Systems with Forecasting: The d-BackTest PS method","keywords (cleaned)":"Back testing optimization algorithms;d-BackTest PS method;Expert back testing systems;Forex;MACD optimization;Optimal historical data period selection","Abstract":"Back testing process is widely used today in forecasting experiments tests. This method is to calculate the profitability of a trading system, applied to specific past period. The data which are used, correspond to that specific past period and are called \u201chistorical data\u201d or \u201ctraining data\u201d. There is a plethora of trading systems, which include technical indicators, trend following indicators, oscillators, control indicators of price level, etc. It is common nowadays for calculations of technical indicator values to be used along with the prices of securities or shares, as training data in fuzzy, hybrid and support vector machine\/regression (SVM\/SVR) systems. Whether the data are used in fuzzy systems, or for SVM and SVR systems training, the historical data period selection on most occasions is devoid of validation (In this research we designate historical data as training data). We substantiate that such an expert trading system, has a profitability edge\u2014with regard to future transactions\u2014over currently applied trading strategies that merely implement parameters\u2019 optimization. Thus not profitable trading systems can be turned into profitable. To that end, first and foremost, an optimal historical data period must be determined, secondarily a parameters optimization computation must be completed and finally the right conditions of parameters must be applied for optimal parameters\u2019 selection. In this new approach, we develop an integrated dynamic computation algorithm, called the \u201cd-BackTest PS Method\u201d, for selection of optimal historical data period, periodically. In addition, we test conditions of parameters and values via back-testing, using multi agent technology, integrated in an automated trading expert system based on Moving Average Convergence Divergence (MACD) technical indicator. This dynamic computation algorithm can be used in Technical indicators, Fuzzy, SVR and SVM and hybrid forecasting systems. The outcome crystalizes in an autonomous intelligent trading system. ","SELECTED":null}
{"Title":"Pairs trading: the case of Norwegian seafood companies","keywords (cleaned)":"Profitability;Norway;Pairs trading;seafood;Stock market;cointegration analysis;price dynamics;Transaction cost;Norwegian seafood companies;trade;High-frequency trading;Cointegration;Statistical arbitrage","Abstract":"In this article, I investigate the performance of a pairs trading strategy on 18 seafood company stocks traded in the Norwegian consumer goods sector on the Oslo Stock Exchange. I apply both high-frequency and daily data from January 2005 to December 2014. I use two approaches\u2013a distance approach and a cointegration approach\u2013and compare the results. For both the distance and the cointegration approaches, nonconvergence of the pairs is high, which may indicate that more fundamental information about the companies traded should be accounted for. None of the strategies evaluated had significant profits after accounting for transaction costs. It therefore remains unclear which approach is best suited for pairs selection. Using high-frequency data yielded empirical distributions that were symmetrical and had a lower degree of leptokurtosis compared to the daily data. ","SELECTED":null}
{"Title":"A mixed data sampling copula model for the return-liquidity dependence in stock index futures markets","keywords (cleaned)":"Copula;CSI 300 index futures;Liquidity;Mixed data sampling;Return","Abstract":"Understanding and quantifying the dependence of returns and liquidity is critical for liquidity risk management. In this paper the idea of mixed data sampling (MIDAS) is extended from linear correlation in Colacito et al. (2011) to the more general dependence measure: copula, and a copula-MIDAS model is proposed to describe the asymmetric return-liquidity dependence of CSI 300 index futures with short-run and long-run components. Based on the skewed t copula-MIDAS model, it is found that extreme decreases in returns tend to be accompanied by extreme increases in bid-ask spreads, but extreme increases in returns may not coincide with extreme reductions in bid-ask spreads. Furthermore, the return-spread dependence consists of both short-run and long-run components, and the long-run component will influence the return-spread dependence in the next two weeks. Last, the out-of-sample forecast of liquidity risk stresses the importance of considering asymmetry and long-run trend in return-spread dependence as it enables investors to well predict liquidity risk in times of market crashes. The results imply that high frequency trading investors of CSI 300 index futures should pay more attentions to prevent the potential liquidity risk when the bid-ask spreads are widened. And investors are suggested to use the past two-week high frequency data to forecast the current return-spread dependence in liquidity risk management. ","SELECTED":null}
{"Title":"The rise of the machines in commodities markets: new evidence obtained using Strongly Typed Genetic Programming","keywords (cleaned)":"Algorithmic trading;Commodities markets;Evolutionary algorithms;High frequency trading;Market efficiency;Market regulation","Abstract":"Market regulators around the world are still debating whether or not high-frequency trading (HFT) is beneficial or harmful to market quality. We develop artificial commodities market populated with HFT scalpers and traditional commodities traders using Strongly Typed Genetic Programming (STGP) trading algorithm. We simulate real-life commodities trading at the millisecond timeframe by applying STGP to the S&P GSCI data stamped at the millisecond interval. We observe that HFT scalpers anticipate the order flow leading to severe damages to institutional traders. To mitigate the negative implications of HFT scalpers on commodities markets, we propose a minimum resting trading order period of more than 150\u00a0ms. ","SELECTED":null}
{"Title":"Latency and liquidity provision in a limit order book","keywords (cleaned)":"High-frequency trading;Limit order book;Liquidity provisions;Price impact","Abstract":"We use a recent, high-quality data set from Nasdaq to perform an empirical analysis of order flow in a limit order book before and after the arrival of a market order. For each of the stocks that we study, we identify a sequence of distinct phases across which the net flow of orders differs considerably. We note that some of our results are consistent with the widely reported phenomenon of stimulated refill, but that others are not. We therefore propose alternative mechanical and strategic motivations for the behaviour that we observe. Based on our findings, we argue that strategic liquidity providers consider both adverse selection and expected waiting costs when deciding how to act. ","SELECTED":null}
{"Title":"Heterogenous market hypothesis evaluation using multipower variation volatility","keywords (cleaned)":"High-frequency trading;Value engineering;Value-at-risk;Commerce;Value at Risk;Statistical properties;Heterogenous market hypothesis;Realized volatility;Auto regressive models;Electronic trading;Efficient market hypothesis;Heterogenous autoregressive models;Out-of-sample forecast;Frequency estimation","Abstract":"High-frequency trading activities are one of the common phenomena in nowadays financial markets. Enormous amounts of high-frequency trading data are generated by huge numbers of market participants in every trading day. The availability of this information allows researchers to further examine the statistical properties of informationally efficient market hypothesis (EMH). Heterogenous market hypothesis (HMH) is one of the important extensions of EMH literature. HMH introduced nonlinear trading behaviors of heterogenous market participants instead of normality assumption under the EMH homogenous market participants. In this study, we attempt to explore more high-frequency volatility estimators in the HMH examination. These include the bipower, tripower, and quadpower variation integrated volatility estimates using Heterogenous AutoRegressive (HAR) models. The empirical findings show that these alternatives multipower variation (MPV) estimators provide better estimation and out-of-sample forecast evaluations as compared to the standard realized volatility. In other words, the usage of MPV estimators is able to better explain the HMH statistically. At last, a market risk determination is illustrated using value-at-risk approach. ","SELECTED":null}
{"Title":"Interactions among High-Frequency Traders","keywords (cleaned)":"investment bank|price impact|HFT|high-frequency trading (HFT)|trading|information|market|r|Trade|High-Frequency|efficiency|High-Frequency Traders|investment|price efficiency|high-frequency trading|order flow|intraday|high-frequency","Abstract":"Using unique transactions data for individual high-frequency trading (HFT) firms in the U.K. equity market, we examine the extent to which the trading activity of individual HFT firms is correlated with each other and the impact on price efficiency. We find that HFT order flow, net positions, and total volume exhibit significantly higher commonality than those of a comparison group of investment banks. However, intraday HFT order flow commonality is associated with a permanent price impact, suggesting that commonality in HFT activity is information based and so does not generally contribute to undue price pressure and price dislocations. ","SELECTED":null}
{"Title":"An entropy-based analysis of the relationship between the DOW JONES Index and the TRNA Sentiment series","keywords (cleaned)":"mass media;Internet;Entropy;TRNA;information;DJIA;social media;Data sets;sentiment;Finance","Abstract":"This article features an analysis of the relationship between the DOW JONES Industrial Average (DJIA) Index and a sentiment news series using daily data obtained from the Thomson Reuters News Analytics (TRNA) provided by SIRCA (The Securities Industry Research Centre of the Asia Pacific). The recent growth in the availability of on-line financial news sources, such as internet news and social media sources provides instantaneous access to financial news. Various commercial agencies have started developing their own filtered financial news feeds which are used by investors and traders to support their algorithmic trading strategies. TRNA is one such data set. In this study, we use the TRNA data set to construct a series of daily sentiment scores for DJIA stock index component companies. We use these daily DJIA market sentiment scores to study the relationship between financial news sentiment scores and the stock prices of these companies using entropy measures. The entropy and mutual information (MI) statistics permit an analysis of the amount of information within the sentiment series, its relationship to the DJIA and an indication of how the relationship changes over time. ","SELECTED":null}
{"Title":"Directional changes: A new way to look at price dynamics","keywords (cleaned)":"Artificial intelligence;Commerce;Electronic trading;Algorithmic trading;Directional changes;Fixed time interval;price dynamics;Price movement;Sampling data;Transaction price;Costs","Abstract":"Prices in financial markets are normally summarized by time series, where transaction prices are sampled at fixed time intervals. Directional change is an alternative way of sampling data: transaction price is sampled when a significant change in the price is recorded. In this paper, we explain how directional changes can provide a valuable alternative perspective to price movements. We also describe the frontier of directional change research, which include forecasting, algorithmic trading and market tracking. ","SELECTED":null}
{"Title":"A note on the relationship between high-frequency trading and latency arbitrage","keywords (cleaned)":"Agent-based model;Algorithmic trading;Genetic programming;High frequency trading;Market efficiency;Market regulation","Abstract":"We develop three artificial stock markets populated with two types of market participants \u2014 HFT scalpers and aggressive high frequency traders (HFTrs). We simulate real-life trading at the millisecond interval by applying Strongly Typed Genetic Programming (STGP) to real-time data from Cisco Systems, Intel and Microsoft. We observe that HFT scalpers are able to calculate NASDAQ NBBO (National Best Bid and Offer) at least 1.5\u00a0ms ahead of the NASDAQ SIP (Security Information Processor), resulting in a large number of latency arbitrage opportunities. We also demonstrate that market efficiency is negatively affected by the latency arbitrage activity of HFT scalpers, with no countervailing benefit in volatility or any other measured variable. To improve market quality, and eliminate the socially wasteful arms race for speed, we propose batch auctions in every 70\u00a0ms of trading. ","SELECTED":null}
{"Title":"High frequency traders in a simulated market","keywords (cleaned)":"Agent-based simulation;Algorithmic trading;High-frequency trading;Liquidity;Stock market volatility","Abstract":"Purpose-An agent-based market simulation is utilized to examine the impact of high frequency trading (HFT) on various aspects of the stock market. This study aims to provide a baseline understanding of the effect of HFT on markets by using a paradigm of zero-intelligence traders and examining the resulting structural changes. Design\/methodology\/approach-A continuous double auction setting with zero-intelligence traders is used by adapting the model of Gode and Sunder (1993) to include algorithmic high frequency (HF) traders who retrade by marking up their shares by a fixed percentage. The simulation examines the effects of two independent factors, the number of HF traders and their markup percentage, on several dependent variables, principally volume, market efficiency, trader surplus and volatility. Results of the simulations are tested with two-way ANOVA and Tukey's post hoc tests. Findings-In the simulation results, trading volume, efficiency and total surplus vary directly with the number of traders employing HFT. Results also reveal that market volatility increased with the number of HF traders. Research limitations\/implications-Increases in volume, efficiency and total surplus represent market improvements due to the trading activities of HF traders. However, the increase in volatility is worrisome, and some of the surplus increase appears to come at the expense of long-term-oriented investors. However, the relatively recent development of HFT and dearth of appropriate data make direct calibration of any model difficult. Originality\/value-The simulation study focuses on the structural impact of HF traders on several aspects of the simulated market, with the effects isolated from other noise and problems with empirical data. A baseline for comparison and suggestions for future research are established. ","SELECTED":null}
{"Title":"Applications of combined financial strategies based on universal adaptive forecasting","keywords (cleaned)":"Commerce;Electronic trading;Finance;Adaptive forecasting;Algorithmic trading;Financial strategies;Historical data;Numerical experiments;Trading platform;Trading strategies;Algorithms","Abstract":"We consider an online adaptive forecasting algorithm for time series elements. Based on this algorithm, we define a universal strategy for the financial market: such a strategy ensures asymptotically maximal profit compared to any trading strategy where decisions are made based on rules that depend continuously on the input information. To reduce risk, in simultaneous trading of several financial instruments we perform adaptive redistribution of the current capital among them according to the AdaHedge algorithm. We propose variations of a combined game with various algorithmic trading strategies. We give results of numerical experiments based on historical data of the MICEX and BATS (US) trading platforms. ","SELECTED":null}
{"Title":"Front-Running Scalping Strategies and Market Manipulation: Why Does High-Frequency Trading Need Stricter Regulation?","keywords (cleaned)":"Algorithmic trading;Evolutionary algorithms;G10;G12;G14;G15;G18;G19;G20;G23;G28;G29;Genetic programming;High-frequency trading;Market efficiency;Market regulation","Abstract":"Regulators continue to debate whether high-frequency trading (HFT) is beneficial to market quality. Using Strongly Typed Genetic Programming (STGP) trading algorithm, we develop several artificial stock markets populated with HFT scalpers and strategic informed traders. We simulate real-life trading in the millisecond time frame by applying STGP to real-time and historical data from Apple, Exxon Mobil, and Google. We observe that HFT scalpers front-run the order flow, resulting in damage to market quality and long-term investors. To mitigate these negative implications, we propose batch auctions every 30 milliseconds of trading. ","SELECTED":null}
{"Title":"Visual analysis to support regulators in electronic order book markets","keywords (cleaned)":"regulatory approach;reconstruction;complexity;Financial data visualization;Exploratory visual analytics;financial system;market conditions;visual analysis;Academic research;Limit order book;data acquisition;electronic equipment","Abstract":"Electronic markets and automated trading have resulted in a drastic increase in the quantity and complexity of regulatory data. Reconstructing the limit order book and analyzing order flow is an emerging challenge for financial regulators. New order types, intra-market behavior, and other exchange functionality further complicate the task of understanding market behavior at multiple levels. Data visualizations have proven to be a fundamental tool for building intuition and enabling exploratory data analysis in many fields. In this paper, we propose the incorporation of visualizations in the workflow of multiple financial regulatory roles, including market surveillance, enforcement and supporting academic research. ","SELECTED":null}
{"Title":"Can High-frequency Trading Strategies Constantly Beat the Market?","keywords (cleaned)":"regulatory framework;Profitability;Malus x domestica;trade flow;Forecasting and simulation;Algorithmic trading;Financial markets;Forecasting methods;Market regulation;Forecasting methods;Stock market;Computer simulation;Genetic algorithms;High-frequency trading","Abstract":"Policymakers are still debating whether or not high-frequency trading (HFT) is beneficial or harmful to financial markets. We develop four artificial stock markets populated with HFT scalpers and aggressive high-frequency traders using Strongly Typed Genetic Programming trading algorithm. We simulate real-life HFT by applying Strongly Typed Genetic Programming to real-time millisecond data of Apple, Bank of America, Russell 1000 and Russell 2000 and observe that HFT scalpers front-run the order flow generating persistent profits. We also use combinations of forecasting techniques as benchmarks to demonstrate that HFT scalping strategies anticipate the trading order flow and constantly beat the market. ","SELECTED":null}
{"Title":"The performance survey of in memory database","keywords (cleaned)":"Memory architecture;Scalability;Data-level parallelism;Electronic trading;Big data;High-frequency trading;Multicore programming;Trading systems;Performance evaluation;Trading systems;Information management;Multi core and many cores;Commerce;Concurrency control;In Memory Database;Database systems;Memory Computing;Real-time transactions;Capability improvement;Memory database;Big data","Abstract":"To satisfy the ever-increasing performance demand of Big Data and critical applications the data management needs to offer the flexible schema, high availability, light weight replica, high volume and high scalability features so as to facilitate the transaction. The in memory database (IMDB) eliminates the I\/O bottleneck by storing data in main memory. We give a deeper analysis of current main-stream IMDB systems performance which focuses on the data structure, architecture, volume, concurrency, availability and scalability. The V3 performance model is proposed to evaluate the Velocity, Volume and Varity of the 19 IMDB systems, in order to highlight the candidates with realtime transaction and high volume processing capacity coordinately. Test results clearly demonstrate that NewSQL is better at dealing with high-frequency trading models. To fully utilize the advantages of the multi-core and many-core processors capability improvements, a three-level optimization design strategy, which includes the memory-access level, the kernel-speedup level and the data-partition level also be proposed using the hardware parallelism for achieving task-level and data-level parallelism of IMDB programs, guarantees the IMDB could accelerate the real-time transaction in an efficient way. We believe that IMDB should become a compulsive option for enterprise users. ","SELECTED":null}
{"Title":"High-frequency financial statistics through high-performance computing","keywords (cleaned)":"Commerce;Electronic trading;Finance;Frequency estimation;Investments;Risk assessment;Risk management;Risk perception;Statistical tests;Statistics;Time series analysis;Financial risk management;High performance computing;Hybrid parallelization;Multiple hypothesis testing;New York Stock Exchange;Optimization procedures;Simulation and optimization;Hardware and software;Financial markets","Abstract":"Financial statistics covers a wide array of applications in the financial world, such as (high-frequency) trading, risk management, pricing and valuation of securities and derivatives, and various business and economic analytics. Portfolio allocation is one of the most important problems in financial risk management. One most challenging part in portfolio allocation is the tremendous amount of data and the optimization procedures that require computing power beyond the currently available desktop systems. In this article, we focus on the portfolio allocation problem using high-frequency financial data, and propose a hybrid parallelization solution to carry out efficient asset allocations in a large portfolio via intra-day highfrequency data.We exploit a variety of HPC techniques, including parallel R, Intelr Math Kernel Library, and automatic offloading to Intelr Xeon Phi coprocessor in particular to speed up the simulation and optimization procedures in our statistical investigations. Our numerical studies are based on high-frequency price data on stocks traded in New York Stock Exchange in 2011. The analysis results show that portfolios constructed using high-frequency approach generally perform well by pooling together the strengths of regularization and estimation from a risk management perspective.We also investigate the computation aspects of large-scale multiple hypothesis testing for time series data. Using a combination of software and hardware parallelism, we demonstrate a high level of performance on highfrequency financial statistics. ","SELECTED":null}
{"Title":"Research in high frequency trading and pairs selection algorithm with Baltic region stocks","keywords (cleaned)":"Commerce;Financial markets;Profitability;Risk assessment;High frequency data;High-frequency trading;Research strategy;Selection algorithm;Statistical arbitrage;Electronic trading","Abstract":"Pair trading is a popular strategy where a profit arises from pricing inefficiencies between stocks. The idea is simple: find two stocks that move together and take long\/short positions when they diverge abnormally, hoping that the prices will converge in the future. During last few years high frequency trading in milliseconds or nanoseconds has drawn attention of not only to financial players but and to researchers and engineers. The main objective of this research is to check three different statistical arbitrage strategies using high frequency trading with 14 OMX Baltic market stocks and measure their efficiency and risks. One strategy used in this paper was first implemented by M.S Perlini, the other one by J.F. Caldeira and G.V. Moura and the last one was presented by D. Herlemont. Together with the strategies a pair selecting algorithm was presented. All three strategies were modified in order to be able to work with high frequency data. At the end of the research strategies where measured accordingly to the profit they did generate. ","SELECTED":null}
{"Title":"Multi-scale representation of high frequency market liquidity","keywords (cleaned)":"Foreign exchange;High frequency trading;information theory;Liquidity;multi-scale","Abstract":"We introduce an event based framework mapping financial data onto a state based discretisation of time series. The mapping is intrinsically multi-scale and naturally accommodates itself with tick-by-tick data. Within this framework, we define an information theoretic quantity that characterises the unlikeliness of price trajectories and, akin to a liquidity measure, detects and predicts stress in financial markets. In particular, we show empirical examples within the foreign exchange market where the new measure not only quantifies liquidity but also seems to act as an early warning signal. ","SELECTED":null}
{"Title":"Investing in emerging markets using neural networks and particle swarm optimisation","keywords (cleaned)":"Particle swarm optimisation;Investments;Industry standards;trade;Particle swarm optimisation;stock selection;Commerce;Correlation coefficient;Stock trading model;Emerging markets;Electronic trading;Automated trading;Particle swarm optimization (PSO);Emerging markets;stock selection","Abstract":"Emerging markets represent a particular challenge to both investors and those interested in developing automated trading strategies. However as well as exposing investors to potential risk, these markets can also offer high returns. Here, a stock trading model is developed for these markets, using both particle swarm optimisation and neural networks. Learning is in part driven by the Matthews correlation coefficient, a task-unspecific but effective fitness measure for unbalanced data sets, used by the authors in previous work, and in addition by a realistic measure of trading profit that incorporates transaction costs. The recommendations from the hybrid model are compared to those obtained from an industry standard stock selection method, with favourable results. ","SELECTED":null}
{"Title":"A binary ensemble classifier for high-frequency trading","keywords (cleaned)":"Bins;Machine learning techniques;Large volumes;Making process;High frequency HF;Commerce;Artificial intelligence;Specific time;Learning systems;Gold;Electronic trading;Ensemble classifiers;Decision makers;High-frequency trading;Decision making","Abstract":"The aim of this study was to model and use machine learning techniques to maximize the chance of a market maker be executed successfully in a stock market, that is, when their bid and ask orders are filled at the desired prices. In this context, a binary ensemble classifier was created to decide whether, at a specific time, is or not propitious to start a new market making process. Conducting the study over a large volume of data for high-frequency traders, we showed that the new proposed ensemble classifier was able to improve the efficiency of the isolated models and the precision of the models are better than random decision makers. ","SELECTED":null}
{"Title":"High-frequency trading strategies using wavelet-transformed order book information and dynamic Bayesian networks","keywords (cleaned)":"Order book;Artificial intelligence;Dynamics;Electronic trading;Hierarchical Hidden Markov Model;High-frequency trading;Wavelet transforms;Dynamic Bayesian networks;Wavelet transforms;Price prediction;Hidden Markov models;Wavelet transforms;Price prediction;Commerce;Learning systems;Markov processes;Machine learning;Transaction price;Order Book Information;Bayesian networks;Costs;Trading strategies","Abstract":"This paper presents a feature vector representing intraday USD\/COP transaction prices and order book dynamics using zig-zag patterns. A Hierarchical Hidden Markov Model is used to capture the market sentiment dynamics choosing from uptrend or downtrend latent regimes based on observed feature vector realizations calculated from transaction prices and wavelet-transformed order book volume dynamics. The HHMM learned a natural switching buy\/uptrend sell\/downtrend trading strategy using a training-validation framework over one month of market data. The model was tested on the following two months, and its performance was reported and compared to results obtained from randomly classified market states and a feed-forward Neural Network. This paper also separately assessed the contribution to the model's performance of the order book information and the wavelet transformation. ","SELECTED":null}
{"Title":"Predicting stock market trends using random forests: A sample of the Zagreb stock exchange","keywords (cleaned)":"Commerce;Decision trees;Electronic trading;Finance;Investments;Microelectronics;10-fold cross-validation;Automated trading;Classification accuracy;Highly accurate;Predictive models;Stock market prediction;Technical indicator;Zagreb Stock Exchange;Financial markets","Abstract":"Stock market prediction is considered to be a challenging task for both investors and researchers, due to its profitability and intricate complexity. Highly accurate stock market predictive models are very often the basis for the construction of algorithms used in automated trading. In this paper, 5-days-ahead and 10-days-ahead predictive models are built using the random forests algorithm. The models are built on the historical data of the CROBEX index and on a few companies listed at the Zagreb Stock Exchange from various sectors. Several technical indicators, popular in quantitative analysis of stock markets, are selected as model inputs. The proposed method is empirically evaluated using stratified 10-fold cross-validation, achieving an average classification accuracy of 76.5% for 5-days-ahead models and 80.8% for 10-daysahead models. ","SELECTED":null}
{"Title":"Performance analysis of the parallel code execution for an algorithmic trading system, generated from UML models by end users","keywords (cleaned)":"Investments;Codes (symbols);High performance computing;Financial data processing;UML;XML;Commerce;Parallel programming;Code Generation;Automatic programming;Algorithmic trading;Algorithms;BSP;Unified Modeling Language;performance prediction;Network components;performance prediction;High performance computing;Parallel programming;Code Generation","Abstract":"In this paper, we describe practical results of an algorithmic trading prototype and performance optimization related experiments for end-user code generation from customized UML models. Our prototype includes high-performance computing solutions for algorithmic trading systems. The performance prediction feature can help the traders to understand how powerful the machine they need when they have a very diverse portfolio or help hem to define the max size of their portfolio for a given machine. The traders can use our Watch Monitor for supervising the PNL (Profit and Loss) of the portfolio and other information so far. A portfolio management module could be added later for aggregating all strategies information together in order to maintain the risk level of the portfolio automatically. The prototype can be modified by end-users on the UML model level and then used with automatic Java code generation and execution within the Eclipse IDE. An advanced coding environment was developed for providing a visual and declarative approach to trading algorithms development. We learned exact and quantitative conditions under which the system can adapt to varying data and hardware parameters. ","SELECTED":null}
{"Title":"High-Frequency Trading: Implications for Market Efficiency and Fairness","keywords (cleaned)":"Algorithmic trading;Financial investments;Market efficiency;Financial markets;Investments;Financial data processing;Efficient stock market;Flash crash;Efficiency;Commerce;Finance;Market fairness;Electronic trading;Efficient market hypothesis;Policy makers;High-frequency trading;order flow","Abstract":"High-frequency trading (HFT) has become more commonplace in the last few years, and more importantly, it has become more noticeable by the general investing public as well as the policy-makers. The growing use of HFT raises some important questions and this chapter will address the following three: (1) provide an introduction to the nature of HFT and its progression in terms of use in execution of financial investment order flow, (2) implication of HFT for market efficiency specifically in relation to the \"efficient market hypothesis,\" and (3) implications for \"fairness\" in the financial markets. ","SELECTED":null}
{"Title":"Optimal trading of algorithmic orders in a liquidity fragmented market place","keywords (cleaned)":"Algorithmic trading;Fill probability;Multiple trading venues;Nonlinear programming;Optimal execution strategy;Price impact","Abstract":"An optimization model for the execution of algorithmic orders at multiple trading venues is herein proposed and analyzed. The optimal trajectory consists of both market and limit orders, and takes advantage of any price or liquidity improvement in a particular market. The complexity of a multi-market environment poses a bi-level nonlinear optimization problem. The lower-level problem admits a unique solution thus enabling the second order conditions to be satisfied under a set of reasonable assumptions. The model is computationally affordable and solvable using standard software packages. The simulation results presented in the paper show the model\u2019s effectiveness using real trade data. From the outset, great effort was made to ensure that this was a challenging practical problem which also had a direct real world application. To be able to estimate in realtime the probability of fill for tens of thousands of orders at multiple price levels in a liquidity fragmented market place and finally carry out an optimization procedure to find the most optimal order placement solution is a significant computational breakthrough. ","SELECTED":null}
{"Title":"High-frequency equity index futures trading using recurrent reinforcement learning with candlesticks","keywords (cleaned)":"Artificial intelligence;Commerce;Financial data processing;Financial markets;Learning systems;Reinforcement learning;Time series;Time series analysis;Algorithmic trading;Algorithmic trading system;Financial time series;Japanese candlesticks;Machine learning methods;Recurrent reinforcement learning;Research communities;Spatio-temporal structures;Electronic trading","Abstract":"In 1997, Moody and Wu presented recurrent reinforcement learning (RRL) as a viable machine learning method within algorithmic trading. Subsequent research has shown a degree of controversy with regards to the benefits of incorporating technical indicators in the recurrent reinforcement learning framework. In 1991, Nison introduced Japanese candlesticks to the global research community as an alternative to employing traditional indicators within the technical analysis of financial time series. The literature accumulated over the past two and a half decades of research contains conflicting results with regards to the utility of using Japanese candlestick patterns to exploit inefficiencies in financial time series. In this paper, we combine features based on Japanese candlesticks with recurrent reinforcement learning to produce a high-frequency algorithmic trading system for the E-mini S&P 500 index futures market. Our empirical study shows a statistically significant increase in both return and Sharpe ratio compared to relevant benchmarks, suggesting the existence of exploitable spatio-Temporal structure in Japanese candlestick patterns and the ability of recurrent reinforcement learning to detect and take advantage of this structure in a high-frequency equity index futures trading environment. ","SELECTED":null}
{"Title":"Optimizing sparse mean reverting portfolios with AR-HMMs in the presence of secondary effects","keywords (cleaned)":"Ornstein-Uhlenbeck process;Stochastic search algorithms;Electronic trading;Financial data processing;Time series;Commerce;Stochastic systems;Algorithmic trading;Markov models;Mean reversion;Optimization;Markov models;Markov processes;Financial time series;Algorithms;Portfolio optimization;Cardinality constraints;Hidden Markov models;Parameter estimation","Abstract":"In this paper we optimize mean reverting portfolios subject to cardinality constraints. First, the parameters of the corresponding Ornstein-Uhlenbeck (OU) process are estimated by auto-regressive Hidden Markov Models (AR-HMM) in order to capture the underlying characteristics of the financial time series. Portfolio optimization is then performed according to maximizing the mean return by the means of the introduced ARHMM prediction algorithm. The optimization itself is carried out by stochastic search algorithms. The presented solutions satisfy the cardinality constraint thus providing a sparse portfolios which minimizes the transaction costs and maximizes the interpretability of the results. The performance has been tested on historical data obtained from S&P 500 and FOREX. The results demonstrate that a good average return can be achieved by the proposed ARHMM based trading algorithms in realistic scenarios. Furthermore, profitability can also be accomplished in the presence of secondary effects.","SELECTED":null}
{"Title":"Traders and time: Who moves the market?","keywords (cleaned)":"Euronext paris;Financial durations;Informed trader;Liquidity provider;Log-ACD model;Market microstructure","Abstract":"Purpose: This paper is aimed to investigate the impact of different categories of traders on price and volume durations at Euronext Paris. The two series are respectively related to the instantaneous volatility and the market liquidity; hence, they are particularly suited to test microstructure hypotheses. Design\/methodology\/approach: ALog-autoregressive conditional duration model was adopted to include the information on the traders\u2019 identity at the transaction level. High-frequency data were used and how the informed traders and the liquidity provider affect the arrival of market events was studied. The robustness of our results was also checked by testing different distributions and controlling for microstructure effects. Findings: It was found that informed traders and the liquidity provider exert a dominant role in accelerating the market activity. This result depends on the state of the market, i.e. it is effective only during periods of high frequency of transactions. The estimates for price durations show that a high instantaneous volatility can be mainly ascribed to a great concentration of informed traders. Informed traders are also found to shorten volume durations by clustering small-size orders to disguise their private signal. For both durations, the liquidity provider is also found to foster the market activity, likely because of his contractual duties. Originality\/value: The article is of interest for researchers in the field of market microstructure, as well as for specialists in the high-frequency trading. Results provide an empirical confirmation of information models which theorize an accelerating effect for informed trading. To the best of the authors\u2019 knowledge, this is the first contribution to study the impact of traders\u2019categories at the transaction level and with different definitions of durations. ","SELECTED":null}
{"Title":"System architecture for on-line optimization of automated trading strategies","keywords (cleaned)":"Communication modeling;Stock market;system and software architectures;Algorithmic trading;automated trading systems (ATS);Software architecture;Commerce;Algorithmic trading;Automation;Signals generation;Automated trading systems;System architectures;Crossover strategies;Online optimization;Autonomous agents;Stock market","Abstract":"This work proposes a new automated trading system (ATS) architecture that supports multiple strategies for multiple market conditions through hierarchical trading signals generation employing h-signals, which are trading signals that are generated using other trading signals. The central idea of the proposed system architecture is to decompose the trading problem into a set of tasks handled by distributed autonomous agents under a minimal central coordination. We implemented the proposed ATS using a software architecture that employed a publish\/subscribe communication model. In the current stage of development, we are able to run our ATS in back-test mode with moving-average crossover strategies on minute-by-minute market databases. We achieved very satisfactory performance results, processing 306.791 database rows representing more than two years of data in only 47 seconds. ","SELECTED":null}
{"Title":"The role of algorithmic trading systems on stock market efficiency","keywords (cleaned)":"Korean stock market;Investments;Phase equilibria;Financial data processing;Efficiency;Liquidity;Commerce;Algorithmic trading;Equilibrium;Electronic communications networks;Information technology;Asymmetric volatility;Algorithmic trading system;Operation efficiencies;Algorithms;Efficiency of information;Market participants;Efficiency of operation","Abstract":"The rapid development of information technology has changed the dynamics of financial markets. The main purpose of this study is laid on examining the role of IT based stock trading on financial market efficiency. This research specifically focused on algorithmic trading. Algorithmic trading enables investors to trade stocks through a computer program without the need for human interventions. Based on an empirical analysis of the Korean stock market, this study discovered the positive impact of algorithmic trading on stock market efficiency at three-fold. First, the study results indicate that algorithmic trading contributes to the reduction in asymmetric volatility, which causes inefficiency of information in a stock market. Second, an algorithmic trading also increases the operation efficiency of a stock market. Arbitrage trading contributes on the equilibrium between the spot market and futures market as well as on the price discovery. Third, algorithmic trading provides liquidity for market participants contributing to friction free transactions. The research results indicate that stock exchanges based on electronic communications networks (ECNs) without human intervention could augment a financial market quality by increasing trading share volumes and market efficiency so that it can eventually contribute to the welfare of market investors. ","SELECTED":null}
{"Title":"High-fidelity per-flow delay measurements with reference latency interpolation","keywords (cleaned)":"Aggregates;Per-flow;Application services;High-performance computing;Scalable architectures;Interpolation;Switching;Algorithmic trading;Repair;Orders of magnitude;Routers;Measurements;Standard deviation;Computer architecture;Routers;Switching;latency;Flow measurement","Abstract":"New applications such as soft real-time data center applications, algorithmic trading, and high-performance computing require extremely low latency (in microseconds) from networks. Network operators today lack sufficient fine-grain measurement tools to detect, localize, and repair delay spikes that cause application service level agreement (SLA) violations. A recently proposed solution called LDA provides a scalable way to obtain latency, but only provides aggregate measurements. However, debugging application-specific problems requires per-flow measurements since different flows may exhibit significantly different characteristics even when they are traversing the same link. To enable fine-grained per-flow measurements in routers, we propose a new scalable architecture called reference latency interpolation (RLI) that is based on our observation that packets potentially belonging to different flows that are closely spaced to each other exhibit similar delay properties. In our evaluation using simulations over real traces, we show that while having small overhead, RLI achieves a median relative error of 12% and one to two orders of magnitude higher accuracy than previous per-flow measurement solutions. We also observe RLI achieves as high accuracy as LDA in aggregate latency estimation, and RLI outperforms LDA in standard deviation estimation. ","SELECTED":null}
{"Title":"Humans versus agents: Competition in financial markets of the 21st Century","keywords (cleaned)":"Market efficiency;Electronic market;Auction;Human-computer interaction;Financial data processing;Strategic interactions;Framework;Commerce;Emotional response;Human computer interaction;Experiments;NeuroIS;Information systems;Finance;Automated trading;Electronic market","Abstract":"Information systems have revolutionized the nature of markets. Traditionally, markets inherently comprised the strategic interaction of human traders only. Nowadays, however, automated trading agents are responsible for at least 60% of the US trading volume on financial stock markets. In this respect, financial markets of the 21st century are different to markets of previous centuries. Fuelled by discussions on their possible risks, there is a need for research on the effects of automated trading agents on market efficiency and on human traders. In order to systematically investigate these issues, we introduce a market framework for human-computer interaction. This framework is then applied in a case study on a financial market scenario. In particular, we plan to conduct a NeuroIS experiment in which we analyze overall market efficiency as well as the trading behavior and emotional responses of human traders when they interact with computerized trading agents.","SELECTED":null}
{"Title":"Financial events recognition in web news for algorithmic trading","keywords (cleaned)":"Algorithmic trading;Financial events;High productivity;Low costs;Multi-disciplinary approach;Online news;Position papers;Stock price;Text mining;Algorithms;Artificial intelligence;Data mining;Finance;Commerce","Abstract":"Due to its high productivity at relatively low costs, algorithmic trading has become increasingly popular over the last few years. As news can improve the returns generated by algorithmic trading, there is a growing need to use online news information in algorithmic trading in order to react real-time to market events. The biggest challenge is to automate the recognition of financial events from Web news items as an important input next to stock prices for algorithmic trading. In this position paper, we propose a multi-disciplinary approach to financial events recognition in news for algorithmic trading called FERNAT, using techniques from finance, text mining, artificial intelligence, and the Semantic Web. ","SELECTED":null}
{"Title":"Optimal algorithms for trading large positions","keywords (cleaned)":"Overall execution;Optimal control systems;Market place;Algorithms;Discrete-time stochastic optimal control;Closed form;Algorithmic trading;Commerce;Optimal trading strategy;Value functions;Selling rules;Stochastic optimal control problem;Resource Constraint;Volume-weighted averages;Stochastic optimal control;Optimization algorithms;VWAP;Trading strategies;Market data","Abstract":"In this paper, we are concerned with the problem of efficiently trading a large position on the market place. If the execution of a large order is not dealt with appropriately this will certainly break the price equilibrium and result in large losses. Thus, we consider a trading strategy that breaks the order into small pieces and execute them over a predetermined period of time so as to minimize the overall execution shortfall while matching or exceeding major execution benchmarks such as the volume-weighted average price (VWAP). The underlying problem is formulated as a discrete-time stochastic optimal control problem with resource constraints. The value function and optimal trading strategies are derived in closed form. Numerical simulations with market data are reported to illustrate the pertinence of these results. ","SELECTED":null}
{"Title":"An experimental analysis of online unidirectional conversion problem","keywords (cleaned)":"Performance of algorithm;Non-preemptive;Competitive ratio;Computational results;Theory and practice;Online algorithms;Financial markets;Performance guarantees;Finance;Experimental analysis;Online algorithms;Algorithms;Automated trading;Gap between theory and practice;Experimental evaluation","Abstract":"Financial markets are highly volatile and decision making in these markets is highly risky. With the introduction of automated trading, a number of techniques are developed to facilitate the automation of financial markets. We consider a set of preemptive as well as non-preemptive online algorithms and evaluate them on real world as well as synthetically produced data. We present extensive computational results based on the observed performance of algorithms in terms of experimentally achieved competitive ratio, number of transactions performed and consistency of the results. We also investigate the gap between the worst case competitive ratio and experimentally achieved competitive ratio and conclude that algorithms perform better than their performance guarantee suggest. We conclude by highlighting a number of open questions. ","SELECTED":null}
{"Title":"Run-time reconfiguration for a reconfigurable algorithmic trading engine","keywords (cleaned)":"Reconfigurable hardware;Commerce;Algorithm design;Algorithms;Algorithmic trading;Algorithmic trading;Reconfiguration time;Business performance;Performance loss;Re-configurable;Run-time reconfiguration;Market data;Worst case scenario;Partial reconfiguration;market conditions","Abstract":"In this paper we present an analysis of using run-time reconfiguration of reconfigurable hardware to modify trading algorithms during use. This provides flexibility in algorithm design, enabling the implementation to be reactive to changes in market conditions, increasing in performance. We study what can be achieved to reduce performance loss in algorithms while reconfiguration takes place, such as buffering information during this time. Our results show our average partial reconfiguration time is 0.002091 seconds, using historic highest market data rates would result in about 5,000 messages being missed or require buffering. This is the worst case scenario, normally the system would only require a fraction of messages. The reconfiguration time is acceptable if it is under the required limit by the user to prevent business performance suffering. ","SELECTED":null}
{"Title":"High frequency trading strategy using the Hilbert transform","keywords (cleaned)":"High frequency data;Computing power;Market trends;Commerce;Metadata;Trading strategies;Financial markets;High frequency;Information channels;Hilbert transform;Data and information;Complex variable;Cyclic patterns;Mathematical transformations;Information management;Market cycles","Abstract":"The recent escalation in computing power and database capacity in major financial exchanges has brought a substantial increase in both data and information availability. The ever faster development of computers, sensors, and information channels in financial IT has rendered trading data easier to be collected than ever before. The present study examines how the Hilbert transform may be used for the detection of financial market cycles and market trends. In particular, this paper focuses on measuring cyclic patterns of high frequency data to enhance intraday trading. The proposed approach exhibits several advantages over more traditional trading methods, attributed to its consideration of complex variables.","SELECTED":null}
{"Title":"Data mining for algorithmic asset management","keywords (cleaned)":"support vector|information|nse|r|transaction cost|algorithmic trading system|risk|trading system|ensemble learning|management|historical data|arbitrage|statistical arbitrage|mining|streaming|pin|trading|algorithmic trading|market|Statistical arbitrage|strategies|algorithmic trading systems|costs|framework|algorithm|return|trading systems|data mining|transaction costs|financial data|Data mining","Abstract":"Statistical arbitrage refers to a class of algorithmic trading systems implementing data mining strategies. In this chapter we describe a computational framework for statistical arbitrage based on support vector regression. The algorithm learns the fair price of the security under management by minimining a regularized \u03b5-insensitive loss function in an on-line fashion, using the most recent market information acquired by means of streaming financial data. The difficult issue of adaptive learning in non-stationary environments is addressed by adopting an ensemble learning approach, where a meta-algorithm strategically combines the opinion of a pool of experts. Experimental results based on nearly seven years of historical data for the iShare S&P 500 ETF demonstrate that satisfactory risk-adjusted returns can be achieved by the data mining system even after transaction costs. ","SELECTED":null}
{"Title":"Dynamic high frequency trading: A neuro-evolutionary approach","keywords (cleaned)":"Evolutionary approaches;Fitness functions;Futures market;High frequency;High frequency data;Trading agent;Transaction cost;Computer science;Evolutionary algorithms;Function evaluation;Commerce","Abstract":"Neuro-evolution of augmenting topologies (NEAT) is a recently developed neuro-evolutionary algorithm. This study uses NEAT to evolve dynamic trading agents for the German Bond Futures Market. High frequency data for three German Bond Futures is used to train and test the agents. Four fitness functions are tested and their out of sample performance is presented. The results suggest the methodology can outperform a random agent. However, while some structure was found in the data, the agents fail to yield positive returns when realistic transaction costs are included. A number of avenues of future work are indicated. ","SELECTED":null}
{"Title":"Stream processing performance for blue Gene\/P supercomputer","keywords (cleaned)":"Benchmarking;Computers;Military applications;Military data processing;Real time systems;Supercomputers;Wireless sensor networks;Automated trading;Battlefield management systems;Blue genes;Communications medias;Data stream;Fraud detections;High bandwidths;High throughput;Low latency;Low-latency networks;Market datum;On-line transactions;Real-time datum;Security agencies;Stream computing;Stream processing;Stream processing systems;Streaming applications;Streaming datum;Security of data","Abstract":"Stream processing systems are designed to support ap- plications that use real time data. Examples of streaming applications include security agencies processing data from communications media, battlefield management systems for military operations, consumer fraud detection based on on- line transactions, and automated trading based on finan- cial market data. Many stream processing applications are faced with the challenge of increasingly large volumes of data and the requirement to deliver low-latency responses predicated by analysis of that data. In this paper, we assess the applicability of the Blue Gene architecture for stream computing applications. This work is part of a larger effort to demonstrate the efficacy of using a Blue Gene for stream- ing applications. Blue Gene supercomputers provide a high-bandwidth low-latency network connecting a set of I\/O and compute nodes. We examine Blue Gene's suitability for stream com- puting applications by assessing its messaging capability for typical stream computing messaging workloads. In par- ticular, this paper presents results from micro-benchmarks we used to evaluate the raw performance of Blue Gene\/P (Blue Gene\/P) supercomputer under loads produced by high volumes of streaming data. We measure the perfor- mance of data streams that originate outside the supercom- puter, are directed through the I\/O nodes to the compute nodes and then terminate outside. Our performance experi- ments demonstrate that the Blue Gene\/P hardware delivers low-latency and high-throughput capability in a manner us- able by streaming applications. ","SELECTED":null}
{"Title":"Humans versus computers: Differences in their ability to absorb and process information for business decision purposes - and the implications for the future","keywords (cleaned)":"Algorithmic trading;Attention deficit;Cognitive ability;Computers;Data processing power;Equation;Formula;Humans;Information acquisition;Information overload;information processing;Information providing;Information strategy;Interpretation","Abstract":"Information overload is a very common problem in the business information industry. However, the root of this problem is not too much information; rather it is the fact that humans have limited attention spans to handle this information. Humans possess more processing power than computers but computers score over us in having almost unlimited attention spans. We must allocate our attention span in such a way as to maximize the information we obtain, given the constraint of our mental abilities. We have adopted a number of strategies to cope, like using intelligent search agents or spreading what attention we have thinly across as many information sources as possible (in other words skimming). The financial services industry, however, has adopted a radical approach: cut the human out of key parts of information acquisition and processing and use computers instead. This article argues that this approach could well spread to the wider business information market in the next few years. Far from being a threat, this could prove liberating for information professionals, allowing them to become more expert in specific industry sectors. The new breed of industry information portals, or community sites, will greatly aid in this transition and will build professional skills and efficiency. Copyright ","SELECTED":null}
{"Title":"Web-service-agents-based securities trading simulation system","keywords (cleaned)":"Business environments;Artificial intelligence;Intelligent agents;Simulation systems;Chicago;Decision support systems;Agent technology;Stock exchange;Decision support systems;Distributed security;Real time;Real time systems;Web services;Securities trading simulation;User interfaces;Commerce;Global trends;Network-based;Web-services;Computer-based system;Investment strategy;Integrating agents;Information systems;Interoperability;Automated trading;Decision making","Abstract":"A global trend towards automated trading systems encourages the appearances of different kinds of decision support systems (DSS) for securities trading. However, the efficiency and effectiveness of the trading decisions and investment strategies suggested by a computer based system are always being oppugned by researchers for abnegating the specialists' professional intermediary roles in traditional floor trading. Without proper validations, evaluations and comparisons, those valuable decision support systems can hardly be adopted trustily. This paper presents a Web-service-agents-based Securities Trading Simulation System (STSS) as a platform to validate, evaluate and compare the existing securities trading decision support systems by using the real world securities trading data from Chicago Stock Exchange. Agent technology is applied to deal with the complex, dynamic, and distributed securities trading processes. Web-services techniques are proposed for more interoperability and scalability in network-based business environment. By integrating agent technology with Web-services to make use of the advantages of both, this approach provides a more intelligent, flexible, and comprehensive platform to simulate the securities trading process. Performance of different decision support systems for securities trading can be validated, evaluated and compared either separately or simultaneously on a real time basis.","SELECTED":null}
{"Title":"Performance Analysis of a Counter-Intuitive Automated Stock-Trading Agent","keywords (cleaned)":"Autonomous trading;Penn-Lehman automated trading (PLAT);Reverse strategy;Stock market;Winner strategy;Autonomous agents;Computer simulation;Decision making;Linear systems;Optimization;Real time systems;Telecommunication networks;Artificial intelligence","Abstract":"Autonomous trading in stock markets is an area of great interest in both academic and commercial circles. A lot of trading strategies have been proposed and practiced from the perspectives of Artificial Intelligence, market making, external data indication, technical analysis, etc. This paper examines some properties of a counter-intuitive automated stock-trading strategy in the context of the Penn-Lehman Automated Trading (PLAT) simulator, which is a real-time, real-data market simulator. While it might seem natural to buy when the market is on the rise and sell when it is on the decline, our strategy does exactly the opposite. As a result, we call it the reverse strategy. The reverse strategy was the winner strategy in the first and second PLAT live competitions. In this paper, we analyze the performance of the reverse strategy. Also, we suggest ways to control the risk of using the reverse strategy in certain kinds of markets.","SELECTED":null}
{"Title":"Technical analysis and sentiment embeddings for market trend prediction","keywords (cleaned)":"Supervised learning;Machine learning techniques;Financial markets;Technical analysis;Market trends;Commerce;Satisfactory solutions;Stock market prediction;Embeddings;Forecasting;Time series analysis;Technical and fundamental analysis;Sentiment analysis;Fundamental analysis;Electronic trading;Sentiment embeddings;Machine learning;Market trend prediction;High-frequency trading","Abstract":"Stock market prediction is one of the most challenging problems which has been distressing both researchers and financial analysts for more than half a century. To tackle this problem, two completely opposite approaches, namely technical and fundamental analysis, emerged. Technical analysis bases its predictions on mathematical indicators constructed on the stocks price, while fundamental analysis exploits the information retrieved from news, profitability, and macroeconomic factors. The competition between these schools of thought has led to many interesting achievements, however, to date, no satisfactory solution has been found. Our work aims to combine both technical and fundamental analysis through the application of data science and machine learning techniques. In this paper, the stock market prediction problem is mapped in a classification task of time series data. Indicators of technical analysis and the sentiment of news articles are both exploited as input. The outcome is a robust predictive model able to forecast the trend of a portfolio composed by the twenty most capitalized companies listed in the NASDAQ100 index. As a proof of real effectiveness of our approach, we exploit the predictions to run a high frequency trading simulation reaching more than 80% of annualized return. This project represents a step forward to combine technical and fundamental analysis and provides a starting point for developing new trading strategies. ","SELECTED":null}
{"Title":"Do Proprietary Algorithmic Traders Withdraw Liquidity during Market Stress?","keywords (cleaned)":"information|r|stock price|Trade|high-frequency traders|gate|Stock Exchange|Liquidity|high-frequency trading|price movement|Stock|trading|market|adverse selection|order imbalance|Market|Algorithm|liquidity|algorithm|return|stock|mes|trade|fast|volatility|high-frequency","Abstract":"We investigate the role of proprietary algorithmic traders in facilitating liquidity in a limit order market. Using order-level data from the National Stock Exchange of India, we find that proprietary algorithmic traders increase limit order supply following periods of both high short-term stock-specific volatility and extreme stock price movement. Even following periods of high marketwide volatility, they do not decrease their supply of liquidity. We define orders from high-frequency traders as a subclass of orders from proprietary algorithmic traders that are revised in less than three milliseconds. The behavior of high-frequency trading mimics the behavior of its parent class. This is inconsistent with the theory that fast traders leave the market when stress situations arise, although their limit-order-supplying behavior becomes weaker when the increase in short-term volatility is more informational than transitory. Agency algorithmic traders and nonalgorithmic traders behave opposite to proprietary algorithmic traders by reducing the supply of liquidity during stress situations. The presence of faster traders in the market possibly instills the fear of adverse selection in them. We document that the order imbalance of agency algorithmic traders is positively related to future short-term returns, whereas the order imbalance of proprietary algorithmic traders is negatively related to future short-term returns. ","SELECTED":null}
{"Title":"Advanced Markov-based machine learning framework for making adaptive trading system","keywords (cleaned)":"LSTM;Machine learning;Stock price","Abstract":"Stock market prediction and trading has attracted the effort of many researchers in several scientific areas because it is a challenging task due to the high complexity of the market. More investors put their effort to the development of a systematic approach, i.e., the so called \"Trading System (TS)\" for stocks pricing and trend prediction. The introduction of the Trading On-Line (TOL) has significantly improved the overall number of daily transactions on the stock market with the consequent increasing of the market complexity and liquidity. One of the most main consequence of the TOL is the \"automatic trading\", i.e., an ad-hoc algorithmic robot able to automatically analyze a lot of financial data with target to open\/close several trading operations in such reduced time for increasing the profitability of the trading system. When the number of such automatic operations increase significantly, the trading approach is known as High Frequency Trading (HFT). In this context, recently, the usage of machine learning has improved the robustness of the trading systems including HFT sector. The authors propose an innovative approach based on usage of ad-hoc machine learning approach, starting from historical data analysis, is able to perform careful stock price prediction. The stock price prediction accuracy is further improved by using adaptive correction based on the hypothesis that stock price formation is regulated by Markov stochastic propriety. The validation results applied to such shares and financial instruments confirms the robustness and effectiveness of the proposed automatic trading algorithm. ","SELECTED":null}
{"Title":"The dark side of liquidity: shedding light on dark pools\u2019 marketing and market-making","keywords (cleaned)":"Algorithmic trading;dark pools of liquidity;market studies;semiotics;social studies of finance;visual culture","Abstract":"In this article, we explore the case of dark pools of liquidity, which are trading venues that do not display order books and other trading-related data. We argue that, in a context where liquidity remains essentially invisible, dark-pool providers use visual advertisements that iconically represent liquid markets. In so doing, they defuse the idea that dark liquidity is harmful to market efficiency and fair pricing. We use Barthesian and Greimasian semiotics to study how a major bank advertised its dark-liquidity services through iconic visual signs. We contribute to economic sociology and social studies of finance by foregrounding the role of visual advertising in the construction of liquid markets. To do so, we draw on insights from market studies and visual culture. ","SELECTED":null}
{"Title":"On-GPU thread-data remapping for branch divergence reduction","keywords (cleaned)":"Parallel processing systems;Memory architecture;Graphics processing unit;Branch divergence;Data analytics;GPGPU;Parallel computing;Algorithmic trading;Thread-data remapping;General-purpose gpu computing;Memory access patterns;SIMD;Program processors;High performance computing;Deep learning","Abstract":"General Purpose GPU computing (GPGPU) plays an increasingly vital role in high performance computing and other areas like deep learning. However, arising from the SIMD execution model, the branch divergence issue lowers efciency of conditional branching on GPUs, and hinders the development of GPGPU. To achieve runtime on-the-spot branch divergence reduction, we propose the frst on-GPU thread-data remapping scheme. Before kernel launching, our solution inserts codes into GPU kernels immediately before each target branch so as to acquire actual runtime divergence information. GPU software threads can be remapped to datasets multiple times during single kernel execution. We propose two thread-data remapping algorithms that are tailored to the GPU architecture. Effective on two generations of GPUs from both NVIDIA and AMD, our solution achieves speedups up to 2.718 with third-party benchmarks. We also implement three GPGPU frontier benchmarks from areas including computer vision, algorithmic trading and data analytics. They are hindered by more complex divergence coupled with different memory access patterns, and our solution works better than the traditional thread-data remapping scheme in all cases. As a compiler-assisted runtime solution, it can better reduce divergence for divergent applications that gain little acceleration on GPUs for the time being. ","SELECTED":null}
{"Title":"Algorithmic trading and liquidity: Long term evidence from Austria","keywords (cleaned)":"Algorithmic trading;Austrian stock market;Market liquidity","Abstract":"We analyze the relation between algorithmic trading and liquidity using a novel data set from the Austrian equity market. Our sample covers almost 4.5 years, it identifies the market share of algorithmic trading at the stock-day level, and it comes from a market that has hitherto not been analyzed. We address the endogeneity problem using an instrumental variables approach. Our results indicate that an increase in the market share of algorithmic trading causes a reduction in quoted and effective spreads while quoted depth and price impacts are unaffected. They are consistent with algorithmic traders on average acting as market makers. ","SELECTED":null}
{"Title":"An automated system for stock market trading based on logical clustering","keywords (cleaned)":"Stock market;Earnings;Financial markets;Investment decision making;Logical clustering;Ogical clustering;Commerce;Automated trading systems;Investments;Automation;Boolean algebra;Fundamental analysis;Automated trading systems;Mathematical frameworks;Electronic trading;Investment portfolio;Interpolative boolean algebras;Linterpolative boolean algebra;Decision making","Abstract":"In this paper a novel clustering-based system for automated stock market trading is introduced. It relies on interpolative Boolean algebra as underlying mathematical framework used to construct logical clustering method which is the central component of the system. The system uses fundamental analysis ratios, more precisely market valuation ratios, as clustering variables to differentiate between undervaluated and overvaluated stocks. To structure investment portfolio, the proposed system uses special weighting formulas which automatically diversify investment funds. Finally, a simple trading simulation engine is developed to test our system on real market data. The proposed system was tested on Belgrade Stock Exchange historical data and was able to achieve a high rate of return and to outperform the BelexLine market index as a benchmark variable. The paper has also provided in-depth analysis of the system\u2019s investment decision making process which reveals some exciting insights. ","SELECTED":null}
{"Title":"TSFDC: A trading strategy based on forecasting directional change","keywords (cleaned)":"Algorithmic trading;Directional changes;FX trading","Abstract":"Directional Change (DC) is a technique to summarize price movements in a financial market. According to the DC concept, data is sampled only when the magnitude of price change is significant according to the investor. In this paper, we develop a contrarian trading strategy named TSFDC. TSFDC is based on a forecasting model which aims to predict the change of the direction of market's trend under the DC context. We examine the profitability, risk and risk-adjusted return of TSFDC in the FX market using eight currency pairs. The results suggest that TSFDC outperforms the buy and hold approach and another DC-based trading strategy. Copyright ","SELECTED":null}
{"Title":"A peak price tracking-based learning system for portfolio selection","keywords (cleaned)":"Backstepping;portfolio selection (PS);High-frequency trading;Computational time;State-of-the-art system;Portfolio selection;Commerce;Defensive systems;Desired trajectories;Fast back-propagation algorithms;peak price tracking (PPT);Aggressive strategy;Backpropagation algorithms;Back-stepping method;Electronic trading;linear learning system;Real time systems","Abstract":"We propose a novel linear learning system based on the peak price tracking (PPT) strategy for portfolio selection (PS). Recently, the topic of tracking control attracts intensive attention and some novel models are proposed based on backstepping methods, such that the system output tracks a desired trajectory. The proposed system has a similar evolution with a transform function that aggressively tracks the increasing power of different assets. As a result, the better performing assets will receive more investment. The proposed PPT objective can be formulated as a fast backpropagation algorithm, which is suitable for large-scale and time-limited applications, such as high-frequency trading. Extensive experiments on several benchmark data sets from diverse real financial markets show that PPT outperforms other state-of-the-art systems in computational time, cumulative wealth, and risk-adjusted metrics. It suggests that PPT is effective and even more robust than some defensive systems in PS. ","SELECTED":null}
{"Title":"Dynamically controlled length of training data for sustainable portfolio selection","keywords (cleaned)":"Algorithms;Data mining;Time series;Length of training data;sustainability;training;analytical method;Sustainable portfolio;research work;Automated trading;Investments","Abstract":"In a constantly changing market environment, it is a challenge to construct a sustainable portfolio. One cannot use too long or too short training data to select the right portfolio of investments. When analyzing ten types of recent (up to April 2018) extremely high-dimensional time series from automated trading domains, it was discovered that there is no a priori 'optimal' length of training history that would fit all investment tasks. The optimal history length depends of the specificity of the data and varies with time. This statement was also confirmed by the analysis of dozens of multi-dimensional synthetic time series data generated by excitable medium models frequently considered in studies of chaos. An algorithm for determining the optimal length of training history to produce a sustainable portfolio is proposed. Monitoring the size of the learning data can be useful in data mining tasks used in the analysis of sustainability in other research disciplines. ","SELECTED":null}
{"Title":"A hybrid financial trading support system using multi-category classifiers and random forest","keywords (cleaned)":"Walk forward approach;Random forests;Financial markets;Random forests;Weighted Multicategory GEPSVM;Financial forecasting;Decision support systems;Multicategory;Commerce;Technical analysis;Electronic trading;Artificial intelligence;Eigenvalues and eigenfunctions;Technical analysis;Walk forward;Data handling;Finance;Support vector machines;Decision trees;Hybrid systems","Abstract":"This study presents a decision support system for algorithmic trading in the financial market that uses a new hybrid approach for making automatic trading decision. The hybrid approach integrates weighted multicategory generalized eigenvalue support vector machine (WMGEPSVM) and random forest (RF) algorithms (named RF-WMGEPSVM) to generate \u201cBuy\/Hold\/Sell\u201d signals. The WMGEPSVM technique has an advantage of handling the unbalanced data set effectively. The input variables are generated from a number of technical indicators and oscillators that are widely used in industry by professional financial experts. Selection of relevant input variables can enhance the predictive capability of the prediction algorithms. RF technique is employed to discover the optimal feature subset from a large set of technical indicators. The proposed hybrid system is tested using \u201cwalk forward\u201d approach for its capability of taking an automatic trading decision on daily data of five index futures, viz., NASDAQ, DOW JONES, S&P 500, NIFTY 50 and NIFTY BANK. RF-WMGEPSVM achieves the notable improvement over the buy\/hold strategy and other predictive models contemplated in this study. It is also observed that combining WMGEPSVM with RF further improves the results. Empirical results confirm the effectiveness of RF-WMGEPSVM in the real market scenarios having bullish, bearish or flat trend. ","SELECTED":null}
{"Title":"Optimal Decisions in a Time Priority Queue","keywords (cleaned)":"adverse selection;Algorithmic trading;High frequency trading;impulse control;Limit order book;queuing model","Abstract":"We show how the position of a limit order (LO) in the queue influences the decision of whether to cancel the order or let it rest. Using ultra-high-frequency data from the Nasdaq exchange, we perform empirical analysis on various LO book events and propose novel ways for modelling some of these events, including cancellation of LOs in various positions and size of market orders. Based on our empirical findings, we develop a queuing model that captures stylized facts on the data. This model includes a distinct feature which allows for a potentially random effect due to the agent\u2019s impulse control. We apply the queuing model in an algorithmic trading setting by considering an agent maximizing her expected utility through placing and cancelling of LOs. The agent\u2019s optimal strategy is presented after calibrating the model to real data. A simulation study shows that for the same level of standard deviation of terminal wealth, the optimal strategy has a 2.5% higher mean compared to a strategy which ignores the effect of position, or an 8.8% lower standard deviation for the same level of mean. This extra gain stems from posting an LO during adverse conditions and obtaining a good queue position before conditions become favourable. ","SELECTED":null}
{"Title":"A new approach for detecting high-frequency trading from order and trade data","keywords (cleaned)":"Borsa Istanbul;HFT detection;High-frequency trading (HFT);Low latency trading","Abstract":"We suggest a two-step approach in detecting HFT activity from order and trade data. While the first step focuses on multiple actions of an order submitter in low latency, the second searches for the surroundings of these orders to link related orders. On a sample of 2015 data from Borsa Istanbul, we estimate that average HFT involvement is 1.23%. HFT activity is generally higher in large cap stocks (2.88%). Most HFT orders are in the form of very rapidly canceled order submissions. A robustness check reveals a mean accuracy rate of 97% in the linkage of orders. ","SELECTED":null}
{"Title":"A survey on computer automated trading in Indian stock markets","keywords (cleaned)":"Algorithmic trading;Electronic trading;Fundamental analysis;Market Micro Structure;Trading processes","Abstract":"In India day by day increasing investors attitude to invest amount in stock markets due to increasingly use of automated systems, social network analysis, predictive analysis, data mining tools and machine learning. In this paper we provide an overview of the secrecy and complexity of the algorithms and how the automated trading systems will work. ","SELECTED":null}
{"Title":"Do news articles have an impact on trading? - Korean market studies with high frequency data","keywords (cleaned)":"Korean stock market;Financial markets;Electronic trading;High frequency data;Investment decision making;Investments;Commerce;Artificial intelligence;Asset pricing;Big data;Information economics;Finance;market studies;High frequency trading data;Market reactions;Big data analysis;High-frequency trading;Decision making","Abstract":"News is an important source of information for investment decision-making. Many studies analyzing listed companies in the US & Japan have been reported. However, the number of studies focusing on Korean stock markets is limited. This study analyzes the influence of news articles on Korean stock markets with high frequency trading data. Especially, we focus on analyses of the relationship between news articles and financial markets. Furthermore, we also analyze differences in market reactions according to language (English or Korean) of news articles and present three case studies. ","SELECTED":null}
{"Title":"Application of Machine Learning: An Analysis of Asian Options Pricing Using Neural Network","keywords (cleaned)":"Option pricing;Financial data processing;Artificial intelligence;Behavioral finance;Electronic trading;Neural network algorithm;Closed form modeling;Intelligent systems;Financial markets;Behavioral finance;Neural networks (NNS);Commerce;Portfolio managements;Learning systems;Machine learning;Option pricing;Neural networks (NNS);Economics;Investments;Asian options;Education;Electronic commerce;Closed-form models;Asian options;Monte Carlo methods;Costs","Abstract":"Pricing Asian Option is imperative to researchers, analysts, traders and any other related experts involved in the option trading markets and the academic field. Not only trading highly affected by the accuracy of the price of Asian options but also portfolios that involve hedging of commodity. Several attempts have been made to model the Asian option prices with closed-form over the past twenty years such as the Kemna-Vorst Model and Levy Approximation. Although today the two closed-form models are still widely used, their accuracy and reliability are called into question. The reason is simple; the Kemna-Vorst model is derived with an assumption of geometric mean of the stocks. In practice, Average Priced Options are mostly arithmetic and thus always have a volatility high than the volatility of a geometric mean making the Asian options always underpriced. On the other hand, the Levy Approximation using Monte Carlo Simulation as a benchmark, do not perform well when the product of the sigma (volatility) and square root maturity of the underlying is larger than 0.2. When the maturity of the option enlarges, the performance of the Levy Approximation largely deteriorates. If the closed-form models could be improved, higher frequency trading of Asian option will become possible. Moreover, building neural networks for different contracts of Asian Options allows reuse of computed prices and large-scale portfolio management that involves many contracts. In this thesis, we use Neural Network to fill the gap between the price of a closed-form model and that of an Asian option. The significance of this method answers two interesting questions. First, could an Asian option trader with a systematic behavior in pricing learned from previous quotes improve his pricing or trading performance in the future? Second, will a training set of previous data help to improve the performance of a financial model? We perform two simulation experiments and show that the performance of the closed-form model is significantly improved. Moreover, we extend the learning process to real data quote. The use of Neural Network highly improves the accuracy of the traditional closed-form model. The model's original price is not so much accurate as what we estimate using Neural network and could not capture the high volatility effectively; still, it provides a relative reasonable fit to the problem (Especially the Levy Model). The analysis shows that the Neural Network Algorithms we used affect the results significantly. ","SELECTED":null}
{"Title":"Informed trading and the price impact of block trades: A high frequency trading analysis","keywords (cleaned)":"Block trades;Informed trading;Opacity;Price discovery;Price impact;Transparency","Abstract":"Using high frequency data from the London Stock Exchange (LSE), we investigate the relationship between informed trading and the price impact of block trades on intraday and inter-day basis. Price impact of block trades is stronger during the first hour of trading; this is consistent with the hypothesis that information accumulates overnight during non-trading hours. Furthermore, private information is gradually incorporated into prices despite heightened trading frequency. Evidence suggests that informed traders exploit superior information across trading days, and stocks with lower transparency exhibit stronger information diffusion effects when traded in blocks, thus informed block trading facilitates price discovery. ","SELECTED":null}
{"Title":"High frequency trading with complex event processing","keywords (cleaned)":"Theoretical framework;Financial markets;Commerce;Realtime processing;Big data;Complex event processing;Trading systems;High frequency trading;Real-time streams;Big data analytics;Electronic trading;Data analytics;Equity trading;High-frequency trading;Real time systems","Abstract":"High frequency trading is steadily taking overthe equity trading world. High frequency trading involves veryhigh speed systems placing trades at sub millisecond speedsacross multiple stock exchanges. HFT is a good example forBig Data analytics - especially the velocity aspect of big data. For HFT strategies to be profitable, real time processing of bigdata is essential. In this paper we discuss the challenges facedby HFT systems and the opportunity for big data processingwith low latency in the field. Most HFT systems are designedusing real time stream processing, which have certaindrawbacks. We present a theoretical framework for buildinghigh frequency trading systems using the complex eventprocessing paradigm which could overcome the drawbacks ofstream processing. Complex event processing enables detectingpatterns of events from disparate events streams and respondsto the detected pattern. The applicability of the framework forHFT applications is discussed. ","SELECTED":null}
{"Title":"Quantitative trading: Algorithms, analytics, data, models, optimization","keywords (cleaned)":"r|risk analysis|transaction cost|market microstructure|models|nonlinear filtering|high-frequency data|risk|Quantitative trading|algorithms|electronic market|market impact models|management|stylized facts|microstructure|execution strategies|trading|algorithmic trading|trading strategies|market|multi-asset trading|systemic risk|strategies|market making|machine learning|market impact|Algorithm|Algorithms|costs|liquidity|algorithm|machine learning techniques|dynamics|transaction costs|optimization|learning techniques|order book|high-frequency","Abstract":"The first part of this book discusses institutions and mechanisms of algorithmic trading, market microstructure, high-frequency data and stylized facts, time and event aggregation, order book dynamics, trading strategies and algorithms, transaction costs, market impact and execution strategies, risk analysis, and management. The second part covers market impact models, network models, multi-asset trading, machine learning techniques, and nonlinear filtering. The third part discusses electronic market making, liquidity, systemic risk, recent developments and debates on the subject. ","SELECTED":null}
{"Title":"A convolutional neural network based approach for stock forecasting","keywords (cleaned)":"Convolution;Economics;Classification (of information);Binary classification problems;Convolutional neural network;Financial data processing;Time series;Commerce;Forecasting;Neural networks;Mathematical tools;Learning systems;Statistical tests;Stock forecasting;Electronic trading;Financial time series;Hodrick-prescott filters;Short-term forecasting;Image recognition;High-frequency trading","Abstract":"The Artificial Neural Network is widely applied to the forecasting of financial time series, and has got certain effects. The problem, however, is that most of these methods is based on daily trading data. When facing the high frequency trading data which is more powerful in short-term forecasting, these methods become incapable. Inspired by the development of Convolutional Neural Network in image recognition tasks, this paper tries to apply the Convolutional Neural Network on the high frequency financial trading data, and has achieved good results. We collect the close price of Shanghai Composite Index from 2006 to 2008 and from 2014 to 2015, about 1000 trading days in total. The frequency of the trading data is 5 min. The paper intends to forecast the stock\u2019s future trend of the next day from historical data, namely, go up or down. First, we apply Hodrick-Prescott decomposition [1] on the original time series. The Hodrick-Prescott filter(decomposition) is a mathematical tool used in macroeconomics to separate the cyclical component ct and trend component \u03c4t from a time series. After preprocessing, the proposed method novelly transforms the time series into pictures. We apply GASF, GADF and MTF algorithm [2] on the time series respectively, forming the R, G and B channels of a picture. By transforming each of the time series into picture mode, the trading time series produces a picture library. Finally, the proposed method uses AlexNet architecture as feature extractor and classifier. The net has five convolution layers, three pooling layers, and two fully-connected layers. In data set of time series from 2006 to 2008, we choose the training data and the testing data randomly, and the proposed method can keep the accuracy at about 0.55 on the binary classification problem. In data set of time series from 2014 to 2015, we choose the last 16 training days of the data set as testing data and the accuracy can reach up to 0.60. ","SELECTED":null}
{"Title":"Adaptive GP agent-based trading system under intraday seasonality model","keywords (cleaned)":"Financial data processing;Seasonality;Artificial intelligence;Financial forecasting;Electronic trading;Decision support systems;High-frequency trading;Planning;Pattern recognition;Financial forecasting;Decision support systems;Automation;Agent-based model;High-frequency trading;Trading systems;Financial markets;Commerce;intraday seasonality model;Agent-based model;Genetic algorithms;Finance;Autonomous agents;Adaptive GP trading system;Costs;Genetic programming","Abstract":"The development of computational intelligence based trading strategies for financial markets has been the focus of research over the last few years. To develop efficient and effective automated trading strategies, we need to understand the workings of the market and the patterns emerging as a result of the traders interactions. In this paper, we develop an adaptive Genetic Programming (GP) agent-based trading system under Intraday Seasonality Model (ISM), which is abbreviated as GP-ISM trading system. ISM is used for creating maps and visualizing the dynamic price evolution of the asset during the day. This new model permits the recognition of periodic patterns and seasonalities in the price time series and hence eliminates any unnecessary data input. We use a high-frequency dataset of historical price data from Saudi Stock Market, which enables us to run multiple market simulation runs and draw comparisons and conclusions for the developed trading strategies. The goal of our work is to develop automated computational intelligence-based strategies for real markets, and this study facilitates a more thorough understanding of a specific market's workings and constitutes the basis for further exploration into such strategies designed for the stock market. We evaluate the intelligence of the GP-ISM trading system through agent-based simulation market index trading. For comparison, we also include four other types of trading agents in this contest, namely, zero-intelligence agents, Buy-and-Hold agents, fundamental agents and technical analysis agents. As a result, GP-ISM performs the best, which provides a general framework for the further development of automated trading strategies and decision support systems. ","SELECTED":null}
{"Title":"Quantification machines and artificial agents in global finance: historical-phenomenological perspectives from philosophy and sociology of technology and money","keywords (cleaned)":"Artificial agents;Distance;Ethics of finance;Marcuse;Mathematics;Phenomenology;Philosophy of finance;Philosophy of technology and media;Quantification;Responsibility;Simmel;Sociology of finance","Abstract":"This paper raises questions regarding the societal, cultural and ethical significance of finance, mathematics, and financial-mathematical technologies, discussing in particular the phenomenon of quantification as mediated by contemporary electronic information and communication technologies (ICTS). It first relates the history of mathematics to the history of financial technologies, and argues, inspired by Simmel and Marcuse, that from ancient times to now there seems to be an evolution towards increasing quantification not only in finance, accounting etc., but in modern society in general. It shows that scientific and technological changes have social and ethical consequences, as quantification creates more distance between people. The paper then analyzes and discusses current shifts of financial agency that exemplify what seems to be a moment of hyper-quantification through the use of ICTs: experiences of \u201cthe market\u201d as an independent agent and money machines as artificial agents in high frequency trading\u2014perhaps the only agents still able to cope with the data-loaded and hyper-quantified world we live in. Under these conditions it becomes more difficult to exercise responsibility. The paper concludes that while we must acknowledge the human character of finance and mathematics, there are real human and social consequences of quantification, in ancient times and today, for society and responsibility. It is therefore misleading to assume that financial technologies and mathematics are ethically neutral; more analysis of ethical and societal aspects is needed, also from an \u201coutside\u201d perspective. ","SELECTED":null}
{"Title":"Hong Kong and Singapore exchanges confront high frequency trading","keywords (cleaned)":"infrastructure;international trade;Hong Kong;latency;technology providers;public sector;Singapore exchange;trade relations;High frequency trading;Singapore [Southeast Asia];China;Hong Kong exchanges","Abstract":"The Hong Kong (HKEx) and Singapore (SGX) exchanges remain conflicted about high frequency trading (HFT), reflecting the environment of private and public sector actors in which the HKEx and SGX operate. Neither exchange has resolved these conflicts, leaving the HFT controversy simmering and limiting the amounts of such trading occurring on their exchanges. Competitor exchanges in Asia, however, are more supportive of HFT. With the aid of technology providers which enable HFT, the HKEx and SGX significantly improved their trading infrastructures. At the same time, these providers developed data centres at other exchanges and built fibre-optic connections which permit low-latency trading across Asia. Traders in Hong Kong and Singapore access these exchanges, potentially undermining the HKEx and SGX. ","SELECTED":null}
{"Title":"A PIN per day shows what news convey: the intraday probability of informed trading","keywords (cleaned)":"Capital markets;High-frequency trading;Information asymmetry;Insider trading regulation;Probability of informed trading","Abstract":"This paper develops a new intraday estimation procedure for the sequential microstructure trading model initially proposed by Easley et al. (Rev Financ Stud 10:805\u2013835, 1997a). Using a full year of intraday trading data for the top 100 German stocks, we demonstrate how the new estimation procedure eliminates or significantly reduces the shortcomings of the original approach in recent, high-frequency trading environments. We slice a trading day in buckets of several minutes\u2019 length to obtain one estimate of the composite variable probability of informed trading (PIN) per day. This approach makes PIN applicable in short horizon event studies. Convergence rates are above 95\u00a0% even for the most liquid stocks and the model\u2019s underlying assumptions of independence for the arrival of traders and information events are fulfilled to a much higher degree than in the original approach. An empirical application in an event study type setting demonstrates how official announcements stipulated in German insider trading legislation significantly reduce information asymmetry upon public disclosure. ","SELECTED":null}
{"Title":"Trading orders algorithm development: Expert system approach","keywords (cleaned)":"Commerce;Expert systems;Financial markets;Algorithm development;Architectural element;Automated trading systems;Currency markets;Historical data;Historical periods;System approach;Trading systems;Electronic trading","Abstract":"The chapter puts into the business of financial markets area in greater detail at FOREX currency market. It describes the main methods used for in currencies trade. The main goal of this paper is to explain the principle of creating an Automated Trading System (ATS) with the MQL4 language. The chapter shows concrete architectural elements of the program on the demonstration examples and it is a guide for the development of an ATS. The main benefit is creation of the original trading system, which optimizes an ATS usage on the base of historical data in practice. Optimization of the trading parameters is based on the equity performance in the historical periods. ","SELECTED":null}
{"Title":"Self-aware hardware acceleration of financial applications on a heterogeneous cluster","keywords (cleaned)":"point process|data feed|r|hardware|low-latency|configurable hardware|hill climbing|article|acceleration|speed|high-frequency trading|reliability|hardware-accelerated|particle swarm optimisation|reconfigurable hardware|latency|trading|function evaluation|design|software|applications|heterogeneous cluster|FPGA|hardware acceleration|genetic programming|valuation|high-frequency","Abstract":"This chapter describes self-awareness in four financial applications. We apply some of the design patterns of Chapter 5 and techniques of Chapter 7. We describe three applications briefly, highlighting the links to self-awareness and self-expression. The applications are (i) a hybrid genetic programming and particle swarm optimisation approach for high-frequency trading, with fitness function evaluation accelerated by FPGA; (ii) an adaptive point process model for currency trading, accelerated by FPGA hardware; (iii) an adaptive line arbitrator synthesising high-reliability and low-latency feeds from redundant data feeds (A\/B feeds) using FPGA hardware. Finally, we describe in more detail a generic optimisation approach for reconfigurable designs automating design optimisation, using reconfigurable hardware to speed up the optimisation process, applied to applications including a quadrature-based financial application. In each application, the hardware-accelerated self-aware approaches give significant benefits: up to 55\u00d7 speedup for hardware-accelerated design optimisation compared to software hill climbing. ","SELECTED":null}
{"Title":"High frequency trading strategy evaluation system based on grey relational analysis and power spectral estimation","keywords (cleaned)":"Commerce;Electronic trading;Information management;Power spectrum;Profitability;Risk assessment;Risk perception;Software testing;Spectrum analysis;Comprehensive evaluation;Gray relational analysis;Grey relational analysis;High-frequency trading;Performance parameters;Power spectral estimation;Strategy development;Strategy evaluations;Frequency estimation","Abstract":"In order to make a comprehensive evaluation of the profit, risk prevention, profit and loss coefficient and other performance parameters of high frequency trading strategies, this paper proposes High Frequency Trading Strategy Evaluation System (SES) model based on gray relational analysis and power spectral estimation algorithm. This model uses gray relational analysis to determine the main strategy factors and power spectral estimation algorithm for strategy evaluation. Based on historical data simulation test, SES provides strategy performance evaluation report, and it also has functions of data management, strategy development, regression testing and clearing implementation. Copyright ","SELECTED":null}
{"Title":"Natural time analysis in financial markets","keywords (cleaned)":"complex systems;Financial time series;market's energy;Natural time;Trading strategies;Trend prediction","Abstract":"In this paper we introduce natural time analysis in financial markets. Due to the remarkable results of this analysis on earthquake prediction and the similarities of earthquake data to financial time series, its application in price prediction and algorithmic trading seems to be a natural choice. This is tested through a trading strategy with very encouraging results. ","SELECTED":null}
{"Title":"Short-term stock price analysis based on order book information","keywords (cleaned)":"Financial markets;Analysis method;Technical analysis;Commerce;Market fluctuations;Time series analysis;Simple analysis;Finance;Data representations;Electronic trading;Efficient market hypothesis;Stock market prediction;Costs;High-frequency trading","Abstract":"Efficient market hypothesis is widely accepted in financial market studies and entails the unpredictability of future stock prices. In this study, we show that a simple analysis can classify short-term stock price changes with an 82.9% accuracy. Our analysis uses the order book information of high-frequency trading. The volume of high-frequency trading, which is responsible for short-term stock price changes, is increasing dramatically; therefore, our study suggests the importance of analyzing short-term market fluctuations, an aspect that is not well studied in conventional market theories. The experimental results also suggest the importance of the new data representation and analysis methods we propose, neither of which have been thoroughly investigated in conventional financial studies. ","SELECTED":null}
{"Title":"Handbook of High Frequency Trading","keywords (cleaned)":"Economics;Statistics;Automated trading systems;Comprehensive examination;Computing infrastructures;Current technology;Hardware and software;High-frequency trading;Technological advances;Trading strategies;Commerce","Abstract":"This comprehensive examination of high frequency trading looks beyond mathematical models, which are the subject of most HFT books, to the mechanics of the marketplace. In 25 chapters, researchers probe the intricate nature of high frequency market dynamics, market structure, back-office processes, and regulation. They look deeply into computing infrastructure, describing data sources, formats, and required processing rates as well as software architecture and current technologies. They also create contexts, explaining the historical rise of automated trading systems, corresponding technological advances in hardware and software, and the evolution of the trading landscape. Developed for students and professionals who want more than discussions on the econometrics of the modelling process, The Handbook of High Frequency Trading explains the entirety of this controversial trading strategy. \u2022 Answers all questions about high frequency trading without being limited to mathematical modelling \u2022 Illuminates market dynamics, processes, and regulations \u2022 Explains how high frequency trading evolved and predicts its future developments. ","SELECTED":null}
{"Title":"Estimating risk of dynamic trading strategies from high frequency data flow","keywords (cleaned)":"Commerce;Data flow analysis;Data mining;Data transfer;Electronic trading;Risk management;Risk perception;Value engineering;High frequency data;High-frequency trading;Low latency;Massive data;Real time analysis;Risk measures;Test examples;Trading strategies;Risk assessment","Abstract":"We consider the problem of risk management in the framework of low latency trading. We suggest an efficient method of real-time analysis of massive data flow from the market. The result of the analysis is a new risk measure Dynamic VaR (DVaR) for risk management of low latency trading robots. The work of DVaR is illustrated on a test example and compared with Traditional VaR and ex-post measure commonly used in high frequency trading. ","SELECTED":null}
{"Title":"Why robots failed: Demonstrating the superiority of multiple-order trading agents in experimental human-agent financial markets","keywords (cleaned)":"Trading agent;Order management;Financial markets;Financial data processing;Continuous double auction;Trading agent;Agent-based computational economics;Commerce;Continuous double auction;Artificial intelligence;Finance industries;Automated trading systems;Finance;Open systems;Algorithmic trading system;Robots;Electronic trading;Experimental economics;Experimental economics;Agent-based computational economics","Abstract":"In the past decade there has been a rapid growth of the use of adaptive automated trading systems, commonly referred to in the finance industry as \"robot traders\": AI applications replacing highly-paid human traders in the global financial markets. The academic roots of this industry-changing deployment of AI technologies can be traced back to research published by a team of researchers at IBM at IJCAI 2001, which was subsequently replicated and extended by De Luca and Cliff at IJCAI 2011 and ICAART 2011. Here, we focus on the order management policy enforced by Open Exchange (OpEx), the open source algorithmic trading system designed by De Luca, for both human and robot traders: while humans are allowed to manage multiple orders simultaneously, robots only deal with one order at the time. We hypothesise that such unbalance may have strongly influenced the victory of human traders over robot traders, reported in past studies by De Luca et al., and by Cartlidge and Cliff. We employed OpEx to implement a multiple-order policy for robots as well as humans, and ran several human vs. robot trading experiments. Using aggregated market metrics and time analysis, we reached two important conclusions. First, we demonstrated that, in mixed human-robot markets, robots dealing multiple simultaneous orders consistently outperform robots dealing one order at a time. And second, we showed that while human traders outperform single-order robot traders under specific circumstances, multiple-order robot traders are never outperformed by human traders. We thus conclude that the performance of robot traders in a human-robot mixed market is strongly influenced by the order management policy they employ. Copyright ","SELECTED":null}
{"Title":"Forecasting intraday volume distributions","keywords (cleaned)":"Financial markets;Financial data processing;Exponentially weighted moving average;Trading efficiency;Moving averages;Commerce;Exponentially weighted moving average;Algorithmic trading;Forecasting;Volume Profile;Moving averages;Volume distributions;Volume-weighted averages;Finance;Algorithmic trading;Algorithms;Maximum absolute deviations;Mean square error","Abstract":"Over the past twenty years, trading in financial markets has evolved from a human-oriented process to one that is highly automated. One of the most influential and revolutionary processes in financial markets is algorithmic trading. The focus of this work is to increase trading efficiency by improving the accuracy of intraday volume forecasts, which are used in algorithmic trading. An intraday volume forecast predicts the distribution of trading volume throughout the day, and allows traders to make better decisions regarding the timing and quantity of their trades. An accurate intraday volume forecast is an important input to trading decisions and will ultimately improve traders' ability to meet benchmarks, such as volume weighted average price. This work seeks to understand the performance of three classes of models: moving average, exponentially weighted moving average, and average exponentially weighted moving average, for intra-day volume prediction over a large sample of U.S. equities. For each model, we explore a broad range of parameterizations and seek to understand how various factors affect model performance. Models are evaluated based on a variety of different metrics, such as mean square error and maximum absolute deviation. We report on the best performing models for a variety of stocks and scenarios. Overall, the average exponentially weighted moving average model performed the best across the examined parameters. ","SELECTED":null}
{"Title":"Algorithm Trading in Asian Currency FX Markets","keywords (cleaned)":"Algorithmic trading;Carry trade;Cross-rate;Foreign exchange market;High frequency data","Abstract":"We investigate a unique EBS foreign exchange dataset that provides each individual order a distinct ID number with a time stamp on entry and another time stamp on exit. Using this dataset for the Australian dollar and the Japanese yen, we measure how long an individual limit order remains in the foreign exchange markets. A large number of limit orders are canceled within a split second, which is evidence for algorithmic trading in the foreign exchange market. While more than 80% of limit orders are canceled in the JPY\/USD spot market, the cancelation rates are even higher in the JPY\/AUD and AUD\/USD spot markets. At the minute frequency, we find weak evidence of correlated cancelation activities among three spot markets. We conclude that the cross-rate JPY\/AUD market is characterized by a mixture of algorithm trading and triangular arbitrage trading. ","SELECTED":null}
{"Title":"Enhancing financial decision-making using social behavior modeling","keywords (cleaned)":"Wikipedia;Financial risks;Online social behaviors;Query information;Social activity modeling;Commerce;Algorithmic trading;Social networking (online);Financial trading;Behavioral research;Finance;Financial decisions;World Wide Web;Social activities;Decision making","Abstract":"Financial trading is a social activity that involves every par- ticipant's decision making. Meanwhile, people's online be- havior collectively creates the public emotion which affects investors' reactions and hence market movements. This pro- cess can be modeled by connecting online social behavior and future trading behavior to better understand mechanisms of the stock movement so as to assist financial decision making. In this paper, we investigate the query information of financially related Wikipedia pages, and show that early signs of trading volume movements can be detected which expose financial risks. We embed this information into a classic pairs trading strategy acting on a large portfolio of stocks. Over 23% profits are seen when testing on the year of 2013 and 20% comes from the inclusion of online social data. Copyright 2014 ACM.","SELECTED":null}
{"Title":"Two parameter update schemes for recurrent reinforcement learning","keywords (cleaned)":"Artificial intelligence;Investments;Learning algorithms;Profitability;Reinforcement learning;Financial data;Financial trading;High-frequency trading;Highly-correlated;ITS applications;Recurrent reinforcement learning;Serial dependence;Trading systems;Commerce","Abstract":"Recurrent reinforcement learning (RRL) is a machine learning algorithm which has been proposed by researchers for constructing financial trading platforms. When an analysis of RRL trading performance is conducted using low frequency financial data (e.g. daily data), the weakening autocorrelation in price changes may lead to a decrease in trading profits as compared to its applications in high frequency trading. There therefore is a need to improve RRL for the purposes of daily equity trading. This paper presents two parameter update schemes (the 'average elitist' and the 'multiple elitist') for RRL. The purpose of the first scheme is to improve out-of-sample performance of RRL-type trading systems. The second scheme aims to exploit serial dependence in stock returns to improve trading performance, when traders deal with highly correlated stocks. Profitability and stability of the trading system are examined by using four groups of S&P stocks for the period January 2009 to December 2012. It is found that the Sharpe ratios of the stocks increase after we use the two parameter update schemes in the RRL trading system. ","SELECTED":null}
{"Title":"The relative contribution of ask and bid quotes to price discovery","keywords (cleaned)":"Ask and bid quotes;Information shares;Limit order book;Market microstructure;Order imbalance;Price discovery","Abstract":"Using 2000-2010 data for 84 stocks listed in the Spanish Stock Exchange (SSE) and 2009-2010 data for 240 stocks listed in the New York Stock Exchange (NYSE), we provide robust evidence of daily asymmetries in the contribution of ask and bid quotes to price discovery. Asymmetries happen in 47.7% (62.8%) of the stock-day observations in our SSE (NYSE) sample, being larger in average among small cap stocks. These asymmetries are not driven by noise. Ask (bid) quotes lead in days with excessive buyer (seller) initiated trading, but the relationship weakened over time and with the advent of high-frequency trading. ","SELECTED":null}
{"Title":"MiFID II -A radical contribution to the development of data law?","keywords (cleaned)":"Data law;Blowing-in;European Parliament;Commerce;MiFID II;Asset class;Securities trading;Electronic trading;High-frequency trading;Markets in financial instruments","Abstract":"Away from the hubbub about HFT (High Frequency Trading) a quiet storm is blowing in to the EU that will radically change securities trading in bonds, OTC derivatives and other asset classes. The rules, called MiFID II,2 top off the alphabet soup of an extensive new rule book that, after the European Parliament's 'Super Tuesday' on 15 April 2014, is finally set to become law. Radical changes are afoot! ","SELECTED":null}
{"Title":"An intelligent market making strategy in algorithmic trading","keywords (cleaned)":"news impact analysis;Financial markets;Electronic trading;Directional predictions;Order book;Market simulation;Microstructure;Algorithmic trading;Commerce;Market simulation;Algorithmic trading;Forecasting;Design and implements;Shanghai stock exchanges;Supervised learning approaches;market making strategy;Impact analysis;order book microstructure;Profitability","Abstract":"Market making (MM) strategies have played an important role in the electronic stock market. However, the MM strategies without any forecasting power are not safe while trading. In this paper, we design and implement a twotier framework, which includes a trading signal generator based on a supervised learning approach and an event-driven MM strategy. The proposed generator incorporates the information within order book microstructure and market news to provide directional predictions. The MM strategy in the second tier trades on the signals and prevents itself from profit loss led by market trending. Using half a year price tick data from Tokyo Stock Exchange (TSE) and Shanghai Stock Exchange (SSE), and corresponding Thomson Reuters news of the same time period, we conduct the back-testing and simulation on an industrial near-to-reality simulator. From the empirical results, we find that 1) strategies with signals perform better than strategies without any signal in terms of average daily profit and loss (PnL) and sharpe ratio (SR), and 2) correct predictions do help MM strategies readjust their quoting along with market trending, which avoids the strategies triggering stop loss procedure that further realizes the paper loss. ","SELECTED":null}
{"Title":"Production of efficient wealth maximization using neuroeconomic behavioral drivers and continuous automated trading","keywords (cleaned)":"Automated trading;Cognitive science;Neuroeconomics;Production theory","Abstract":"For the better part of the last four decades academic research has focused on stock return predictability explained by asset pricing models. Financial economists approach finance theory by focusing on whether to reject a pricing model based on hypothesis tests. However, this approach often fails to capture aspects of the model and latent information in the data that could be potentially useful to the investor. Owing to these limitations, investors have increasingly turned to high-frequency trading as a means of achieving risk-mitigated wealth appreciation. High frequency trading is a strategy that relies on professional proprietary formula-driven trading algorithms to execute millisecond trades using high-speed computers that are placed near or in the proximity of the exchanges to reduce order latency time. The contemporary impact of this new approach has increased the speed and complexity of trading, and has had an impact on market liquidity and transparency. In turn, this has attracted the attention of regulators who are \"Clamping Down on Rapid Trades in the Stock Market\" [3] and the risk of such trades to create volatility by flooding the market with orders before canceling these same orders in fractions of seconds. Facing the possibility of regulatory oversight, many highfrequency automated traders are now turning to the alternative of near high-frequency trading. Instead of focusing on strategies to implement strategic runs in the millisecondenvironment, using the highest resolution data available, near high-frequency trading solves complex econometric models on an instrument by instrument basis to predict the profitability of a future trade. In this paper we present a theory of stochastic price formation to form a behavioral near high frequency automated trading system. The new system is actuated by a combining dual radial basis function artificial neural networks and two fundamental trading metrics. Additionally, because the trading signals are generated by cognitive models, the research calibrates a nonparametric behavioral production function to estimate the quasi elasticity coefficients of a four factor model. ","SELECTED":null}
{"Title":"Ontology-supported design of domain-specific languages: A complex event processing case study","keywords (cleaned)":"Ontology;Problem oriented languages;Security of data;Algorithmic trading;Complex event processing;Domain knowledge;Domain specific languages;Event Processing;Multiple contexts;Multiple domains;Robust monitoring;Digital subscriber lines","Abstract":"This chapter introduces a novel approach for design of Domain-Specific Languages (DSL). It is very common in practice that the same problems emerge in different application domains (e.g. the modeling support for complex event processing is desirable in the domain of algorithmic trading, IT security assessment, robust monitoring, etc.). A DSL operates in one single domain, but the above-mentioned cross-domain challenges raise the question: is it possible to automate the design of DSLs which are so closely related? This approach demonstrates how a family of domain-specific languages can be developed for multiple domains from a single generic language metamodel with generative techniques. The basic idea is to refine the targeted domain with separating the problem domain from the context domain. This allows designing a generic language based on the problem and customizing it with the appropriate extensions for arbitrary contexts, thus defining as many DSLs and as many contexts as one extends the generic language for. The authors also present an ontology-based approach for establishing contextspecific domain knowledge bases. The results are discussed through a case study, where a language for event processing is designed and extended for multiple context domains. ","SELECTED":null}
{"Title":"Input data organization for batch processing in time window based computations","keywords (cleaned)":"Time windows;Batch data processing;Event Processing;Vehicle routing;Continuous queries;Performance optimizations;Sliding time windows;Algorithmic trading;Batch event processing;Input output programs;Intrusion Detection Systems;Time window computations;Processing engine","Abstract":"Applications based on event processing are often designed to continuously evaluate set of events defined by sliding time windows. Solutions employing long-running continuous queries executed in-memory show their limits in applications characterized by a staggering growth of available sources that continuously produce new events at high rates (e.g. intrusion detection systems and algorithmic trading). Problems arise due to the complexities in maintaining large amounts of events in memory for continuous elaboration, and due to the difficulties in managing at run-time the network of elaborating nodes. A batch approach to this kind of computation provides a viable solution for scenarios characterized by non frequent computations of very large time windows. In this paper we propose a model for batch processing in time window event computations that allows the definition of multiple metrics for performance optimization. These metrics specifically take into account the organization of input data to minimize its impact on computation latency. The model is then instantiated on Hadoop, a batch processing engine based on the MapReduce paradigm, and a set of strategies for efficiently arranging input data is described and evaluated. Copyright 2013 ACM.","SELECTED":null}
{"Title":"Communication-efficient distributed online prediction using dynamic model synchronizations","keywords (cleaned)":"Social networking (online);Algorithmic trading;High frequency HF;Local prediction;Model synchronization;Network communications;Online prediction;Real time service;Social contents;Forecasting","Abstract":"We present the first protocol for distributed online prediction that aims to minimize online prediction loss and network communication at the same time. Applications include social content recommendation, algorithmic trading, and other scenarios where a configuration of local prediction models of high-frequency streams is used to provide a realtime service. For stationary data, the proposed protocol retains the asymptotic optimal regret of previous algorithms. At the same time, it allows to substantially reduce network communication, and, in contrast to previous approaches, it remains applicable when the data is non-stationary and shows rapid concept drift. The protocol is based on controlling the divergence of the local models in a decentralized way. Its beneficial properties are also confirmed empirically.","SELECTED":null}
{"Title":"Winning the Kaggle Algorithmic Trading Challenge with the composition of many models and feature engineering","keywords (cleaned)":"Liquidity shock;Feature selection;Kaggle challenge;Maximal information coefficient;Commerce;Model architecture;Boosting;Feature extraction;Models;Optimization;High frequency trading;Maximal information;Algorithms;Decision trees;High-frequency trading","Abstract":"This letter presents the ideas and methods of the winning solution * for the Kaggle Algorithmic Trading Challenge. This analysis challenge took place between 11th November 2011 and 8th January 2012, and 264 competitors submitted solutions. The objective of this competition was to develop empirical predictive models to explain stock market prices following a liquidity shock. The winning system builds upon the optimal composition of several models and a feature extraction and selection strategy. We used Random Forest as a modeling technique to train all sub-models as a function of an optimal feature set. The modeling approach can cope with highly complex data having low Maximal Information Coefficients between the dependent variable and the feature set and provides a feature ranking metric which we used in our feature selection algorithm. Copyright ","SELECTED":null}
{"Title":"Multi-agent system based portfolio management in prior-to-crisis and crisis period","keywords (cleaned)":"Profitability;Intelligent systems;Investments;Multi agent systems;Financial data processing;Multiagent systems;Automated trading;Sample sizes;Commerce;complexity;Pattern recognition systems;Systems analysis;Sample sizes;portfolio optimisation;Risks;portfolio optimisation;regularization;Risks;Automated trading","Abstract":"We analyze portfolio creation techniques in a high frequency trading domain and randomly changing environments. We aim to create the best risk\/reward portfolio based on thousands of profit histories of automated trading robots. We show that the effectiveness of standard portfolio weight calculation rules depends on the dimensionality, N, and the sample size, L, ratio. To resolve dimensionality \/ sample size dilemma we suggest designing a multistage feed-forward multi-agent system (MAS). At first we make simple 1\/N Portfolio based expert agents. Then we use them and the regularized mean-variance framework to form a large number of more complex fusion agents. Finally we use a trained cost sensitive set of perceptrons to recognize the most successful fusion agents for making a final 1\/N Portfolio based weights calculation. Experiments with 7708-dimensional 2004-2012 data confirm the effectiveness of the new approach. ","SELECTED":null}
{"Title":"Extending and evaluating agent-based models of algorithmic trading strategies","keywords (cleaned)":"Agent based;Simulation;Extended model;Market simulation;Bayesian adaptive agents;Evaluation methods;Historical data;Adaptive agents;Algorithmic trading;Commerce;Complex decision;Agent-based model;Algorithmic trading;Market model;Statistical tests;Algorithms;Execution strategies;Computer simulation;backtesting","Abstract":"Algorithmic trading (AT) strategies aim at executing large orders discretely, in order to minimize the order's impact, whilst also hiding the traders' intentions. Most AT evaluation methods range from running the AT strategies against historical data (back testing) to evaluating them on simulated markets. The contribution of the work presented in this paper is twofold. First we investigated different types of agent-based market simulations and suggested how to identify the most suitable market simulation type, based on the specific market model to be investigated. Then we proposed an extended model of the Bayesian execution strategy. We implemented and assessed this model using our tool AlTraSimBa (Algorithmic Trading Simulation Back testing) against the standard Bayesian execution strategy and naive execution strategies, for momentum markets and random markets. The results revealed useful insights on the trade-offs between the frequency of decision making and more complex decision criteria, on one side, and the negative outcome of lost trading on the agents' side due to them not participating actively in the market for some of the execution steps. ","SELECTED":null}
{"Title":"An integrated cross-volatility estimation for asynchronous noisy data","keywords (cleaned)":"asynchronous data;cross-volatility;integrated volatility;microstructure noise","Abstract":"Let \u03c3 t be the instantaneous cross-volatility of two continuous semimartingales X and Y. In this paper, we introduce some estimators for the class of integrated cross-volatilities of the form \u222b 1 0 g(X(t),Y(t)) \u03c3 t dt, where g is a continuous function and processes X and Y are sampled with microstructure noise and in an asynchronous way. In finance, it is widely accepted that the processes X and Y are reasonable models for the log return of price processes of stock and currency and our estimator is relevant in the context of intra-day high-frequency trading. ","SELECTED":null}
{"Title":"Multiplicative ICA algorithm for interaction analysis in financial markets","keywords (cleaned)":"ICA algorithms;Financial data processing;Time series;Soft computing;Artificial intelligence;Financial time series;Independent component analysis;Financial markets;Modeling environments;Independent component analysis;information representation;Independent components;Multivariate time series;Finance;Algorithms;Multiplicative version;information representation;Stock market;Interaction analysis;financial time series decomposition;Natural gradient;Automated trading","Abstract":"In this article we present a new method for the analysis of dependencies in case of multivariate time series. In this approach, we assume that the set of time series representing the various financial instruments creates a multidimensional variable. Such a multidimensional variable is decomposed into independent components which enable to analyze the morphology of given financial instruments and to identify the hidden interdependencies. We propose a new multiplicative version of the Natural Gradient ICA algorithm that could be used in automated trading systems or modeling environments. The presented method is tested on real stock markets data. ","SELECTED":null}
{"Title":"Program trading strategy based on Copula functions","keywords (cleaned)":"Coefficient of lower tail dependence;High frequency HF;Commerce;Program trading;Copula functions;Copula functions;Objective functions;Correlation theory;Empirical results;Tail dependence;High-frequency data;Sugars;Real-time trading;Trading strategies;Trading risks","Abstract":"This paper measured the coefficient of lower tail dependence between two series by using Copula functions, and established an objective function to capture the short sell signals in the real time trading process. We presented a new high-frequency trading strategy based on the signals and applied to Chinese sugar and cotton futures. The empirical results show that the proposed trading strategy gains a high and stable return, and it also manages the trading risk properly.","SELECTED":null}
{"Title":"Emerging markets unidirectional sensitivity coefficient as an indicator in portfolio investors' decision making","keywords (cleaned)":"Emerging markets;Market comovement;Market correlations;Unidirectional sensitivity coefficient","Abstract":"This paper examines linear correlations between six emerging European stock market indices and the world's most significant index Standard & Poor's (S & P) 500. Using a unique dataset within three years of data on the indices from the markets in Bulgaria, Croatia, Hungary, Romania, Serbia and Slovenia we compare movements of the indices, find out the influences on their regularity in day-to-day movements and calculate their unidirectional correlation. We introduce movement relative sensitivity to changes in S & P 500 values for each regional index, in order to better describe unidirectional changes. Results for unidirectional correlation and movement relative sensitivity show that these two indicators can be important factor in an investor decision making process. Furthermore, we explain how these two indicators can be used as practical tool in algorithmic trading system development. Finally, we show that average unidirectional sensitivity coefficient can be used as volatility indicator.","SELECTED":null}
{"Title":"Computational Economics","keywords (cleaned)":"Abstract modeling;Black-scholes partial differential equation;Computational economics;Data analysis;Heterogeneous cluster","Abstract":"Computational economics is a relatively new research technique in economics, but it is inexorably taking its place alongside the more traditional methods of general theory, abstract modeling, data analysis, and the more recent experimental economics. Perhaps because of its relative newness, the term computational economics currently has no determinate meaning. In contemporary use, it refers to a heterogeneous cluster of techniques implemented on concrete digital computers ranging from the numerical solution of the Black-Scholes partial differential equation for pricing options through automated trading strategies to agent-based computer simulations of the evolution of cooperation. Because of this heterogeneity, it is not possible to provide a comprehensive coverage of the topic in this article. Another reason for this restricted scope is that many of the methods used in computational economics have considerable technical interest but no particular philosophical relevance. ","SELECTED":null}
{"Title":"On the legal status of electronic agent in international trade","keywords (cleaned)":"Commerce;Government data processing;Industrial economics;Intelligent agents;international trade;Laws and legislation;America;Automated trading;Automated trading;E commerces;Electronic agents;High efficiencies;International businesses;Legal problems;Legal regulations;Legal statuses;Low costs;Electronic commerce","Abstract":"In international business, more and more E-business personnel come toadopt the various intelligent, automated trade systems based on the Internet. This online automated trading system is named legally as E-agent. (electronic agent), which has contributed a great deal to the low cost and high efficiency of international trade by means of its technique and unique function. Meanwhile, it also brings about some legal problems due to the lack of legislation in this realm in China. In oder to better guarantee its greater advantage in international business,this paper ventures to point out some relevant legal regulations of E-agent in those of the UN, America and Canada; analyze three representative views of scholars;finally, put forward some suggestions concerning legislation in E-commerce. ","SELECTED":null}
{"Title":"Intraday stock returns and performance of a simple market model","keywords (cleaned)":"hypothesis testing;numerical model;performance assessment;Probability;Stock market","Abstract":"With enhancements in information technology, increased institutional and automated trading, and previously unmatched availability of online trading grew the attention to intraday movements of security prices and tests associated with them. This study analyses the properties of both observed 5-minute stock returns and of excess returns obtained by using a variety of alternative models common to event studies. Using observed 5-minute returns, various event study methodologies are simulated and repeatedly applied to samples which have been created by random selection of securities and random assignment of event dates to each security. The study examines the probability of rejecting the null hypothesis of no average abnormal performance when it is true and the probability of detecting a given abnormal performance, which is introduced to the data at the randomly selected event dates.","SELECTED":null}
{"Title":"Using information gain to analyze and fine tune the performance of Supply Chain Trading Agents","keywords (cleaned)":"Agent performance;Procurement decisions;Commerce;Electronic commerce;Artificial intelligence;Behavioral features;Dynamic supply chains;TAC SCM;Amount of information;Agent performance analysis;Trading Agent Competition;Automated trading;Supply chain management","Abstract":"The Supply Chain Trading Agent Competition (TAC SCM) was designed to explore approaches to dynamic supply chain trading. During the course of each year's competition historical data is logged describing more than 800 games played by different agents from around the world. In this paper, we present analysis that is focused on determining which features of agent behavior, such as the average lead time requested for supplies or the average selling price offered on finished products, tend to differentiate agents that win from those that do not. We present a visual inspection of data from 16 games played in one bracket of the 2006 TAC SCM semi-final rounds. Plots of data from these games help isolate behavioral features that distinguish top performing agents in this bracket. We then introduce a metric based on information gain to provide a more complete analysis of the 80 games played in the 2006 TAC SCM quarter-final, semi-final and final rounds. The metric captures the amount of information that is gained about an agent's performance by knowing its value for each of 20 different behavioral features. Using this metric we find that, in the final rounds of the 2006 competition, winning agents distinguished themselves by their procurement decisions, rather than their customer bidding decisions. We also discuss how we used the analysis presented in this paper to improve our entry for the 2007 competition, which was one of the six finalists that year. ","SELECTED":null}
{"Title":"Why effective spreads on NASDAQ were higher than on the New York stock exchange in the 1990s","keywords (cleaned)":"Bid-ask spreads;Collusion;Day trading;Market microstructure;NASDAQ;NYSE;Odd-eighth avoidance;SOES bandits","Abstract":"Two hypotheses have been advanced to explain why spreads on NASDAQ were substantially higher than those on the NYSE in the 1990s: \"collusion\" and \"preferencing and payment for order flow.\" We present data on all actively traded stocks in these markets of relative effective spreads (RES), aggregated monthly over 1987-1999 and advance a third hypothesis: NASDAQ \"SOES-day-trading.\" We estimate NASDAQ and NYSE informed-trade losses and gains to market makers and other liquidity providers on six trade sizes, and find that losses on trades we ascribe to SOES day traders were substantially greater than those on other trades, offset somewhat by gains from small-trade-size investors. NASDAQ market makers' response to these losses and additional operations costs incurred to reduce the losses resulted in greater RES and increased trading within the best quotes, predominantly on larger trade sizes. The data are consistent with the \"SOES-day-trading\" hypotheses, but not with the other two. Furthermore, the mandatory SOES \"experiment\" provides insights into the negative effects of automated trading systems (such as ECNs, which now dominate NASDAQ) when their design does not adequately consider opportunistic traders. ","SELECTED":null}
{"Title":"Algorithmic tradings: The use of algorithms in automated trading","keywords (cleaned)":"algorithm|mining|real-time market|algorithms|technology|Event Processing|trading|trading algorithms|market|Algorithmic trading|r|Algorithm|trade|market data|Complex Event Processing|automated trading","Abstract":"An advanced technology approach, Complex Event Processing (CEP), allows to build, deploy, and manage trading algorithms quickly and easily that describe a sequence of steps in which real-time market data can be recognized and responded to detect trading opportunities in the market. Two main parts in a trading algorithm are sequence of steps that determine the time to trade and the other describing the way to trade. The first one is the analytic part of the strategy that is centered around watching the changing market data and detecting opportunities within the market. However, determining how to trade, centers on placing and managing orders in the market that helps large orders to get poor price. CEP is a new approach of computing that allows organizations to quickly respond to data that is continuously changing and allows to determine the queries in advance by setting certain parameters to select relevant data.","SELECTED":null}
{"Title":"Automated trading - Making it happen","keywords (cleaned)":"Agent-mediated electronic commerce;Supplier brokering;Real time control;Electronic commerce;Identification (control systems);Intelligent agents;e-procurement;Virtual reality;Data mining","Abstract":"Fully automated trading, such as e-procurement, is virtually unheard of today. Trading involves the maintenance of effective business relationships, and is the complete process of: need identification, product brokering, supplier brokering, offer-exchange, contract negotiation, and contract execution. Three core technologies are needed to fully automate the trading process. First, real-time data mining technology to tap information flows and to deliver timely information at the right granularity. Second, intelligent agents that are designed to operate in tandem with the real-time information flows from the data mining systems. Third, virtual institutions in which informed trading agents can trade securely both with each other and with human agents in a natural way. This discussion focusses on the second technology - the design of \"information driven\" trading agents.","SELECTED":null}
{"Title":"Empowering automated trading in multi-agent environments","keywords (cleaned)":"RuleML;URML;Multi agent systems;Web services;Collaboration agents;Semantic Web;Multi-agent collaboration;Financial services;data acquisition;Intelligent agents;Semantics;Real-time agents;Computer simulation;World Wide Web;Real time systems;Decision theory","Abstract":"Trading in the financial markets often requires that information be available in real time to be effectively processed. Furthermore, complete information is not always available about the reliability of data, or its timeliness - nevertheless, a decision must still be made about whether to trade or not. We propose a mechanism whereby different data sources are monitored, using Semantic Web facilities, by different agents, which communicate among each other to determine the presence of good trading opportunities. When a trading opportunity presents itself, the human traders are notified to determine whether or not to execute the trade. The Semantic Web, Web Services, and URML technologies are used to enable this mechanism. The human traders are notified of the trade at the optimal time so as not to either waste their resources or lose a good trading opportunity. We also have designed a rudimentary prototype system for simulating the interaction between the intelligent agents and the human beings, and show some results through experiments on this simulation for trading of the Chicago Board Options Exchange (CBOE) options.","SELECTED":null}
{"Title":"Time-driven feature-aware jointly deep reinforcement learning for financial signal representation and algorithmic trading","keywords (cleaned)":"Historical information;Environment perceptions;Iterative methods;temporal attention;Commerce;Gate;Reinforcement learning models;deep reinforcement learning;Algorithmic trading;Reinforcement learning;Finance;Decision-making problem;Electronic trading;Feature representation;Machine learning;Deep learning;Decision making","Abstract":"Algorithmic trading is a continuous perception and decision making problem, where environment perception requires to learn feature representation from highly nonstationary and noisy financial time series, and decision making requires the algorithm to explore the environment and simultaneously make correct decisions in an online manner without any supervised information. To address these two problems, we propose a time-driven feature-aware jointly deep reinforcement learning model (TFJ-DRL) that integrates deep learning model and reinforcement learning model to improve the financial signal representation learning and action decision making in algorithmic trading. Concretely, we learn the environmental representation by adaptively selecting and reweighting various features of financial signals and summarize the attention values between historical information and changing trend depending on the current state. Besides, the supervised deep learning and reinforcement learning are jointly and iteratively trained to make full use of the supervised signals in the training data, and obtain more update information and stricter loss function constraints, thereby increasing investment returns. TFJ-DRL is evaluated on real-world financial data with different price trends (rising, falling and no obvious direction). A series of analysis show the robust superiority and the extensive applicability of the proposed method. ","SELECTED":null}
{"Title":"When spread bites fast \u2013 Volatility and wide bid-ask spread in a mixed high-frequency and low-frequency environment","keywords (cleaned)":"Bad deals;High-frequency trading;Market stability;Na\u00efve orders;Volatility","Abstract":"This research focuses on the impact High-Frequency Trading has on price volatility when bid-ask spread is wide. The theoretical part introduces a set of equations and presents an Agent Based Model implemented via a computer-based simulation. The wide spread leads to the appearance of unusual phenomena caused by the relative speed difference between the fast and slow traders. The latter agents tend to quote limit orders that look irrational, as they are distant more than one tick from the top-of-book. The same relative speed difference causes slow traders to post market orders that execute at price worse than originally intended. Both these abnormal orders tend to increase local volatility. Other results found by the simulation are an increase in global volatility (computed both as the difference of maximum less minimum price and as standard deviation of price distribution) and in volatility at sub-second timescales. These occurrences penalise slower traders and affect market stability. All the results are consistent both under quiet and stressed market conditions. The results found are then compared with audit trail data to verify the soundness of theory against practice. ","SELECTED":null}
{"Title":"Identification of technical analysis patterns with smoothing splines for bitcoin prices","keywords (cleaned)":"Algorithmic trading, trading strategy;bitcoin cryptocurrency;Pattern recognition;smoothing splines;technical analysis patterns","Abstract":"This research studies automatic price pattern search procedure for bitcoin cryptocurrency based on 1-min price data. To achieve this, search algorithm is proposed based on nonparametric regression method of smoothing splines. We investigate some well-known technical analysis patterns and construct algorithmic trading strategy to evaluate the effectiveness of the patterns. We found that method of smoothing splines for identifying the technical analysis patterns and that strategies based on certain technical analysis patterns yield returns that significantly exceed results of unconditional trading strategies. ","SELECTED":null}
{"Title":"Asymmetric news responses of high-frequency and non-high-frequency traders","keywords (cleaned)":"financial crisis;High-frequency trading;information processing;news","Abstract":"Using NASDAQ trade and Reuters news data, I show that the response of aggressive non-high-frequency traders (nHFTs) to news is stronger than that of aggressive high-frequency traders (HFTs). Classifying news into quantitative (\u201chard\u201d) and less quantitative (\u201csofter\u201d) news, the trading response of aggressive nHFTs to softer news exceeds HFTs\u2019 response. Positive news elicits greater return and nHFT responses than negative news during the 2008 financial crisis period. As this phenomenon persists even after excluding the 2008 short-sale ban, the results support the hypothesis of nHFTs exhibiting stronger asymmetric responses during crisis periods. ","SELECTED":null}
{"Title":"A method to get a more stationary process and its application in finance with high-frequency data of Chinese index futures","keywords (cleaned)":"Mean reverting process;Ornstein\u2013Uhlenbeck process;Technical indicator;BIAS;Commerce;Stationary process;Finance;Electronic trading;Error process;High-frequency trading","Abstract":"Technical indicators have been widely used in financial markets for a long time. Wang and Zheng (2014) proposed in their book that the technical indicators can be transformed into the stationary process and investigated the profitability and availability. But in fact, we can only test that a data series form a weakly stationary process but a strongly stationary process. Nevertheless, the convergence of a more stationary process will vanish faster, thus it is much better if we can get a more stationary process. In this paper, we propose a method to get a more strongly (or weakly) process named mean reverting process that based on the original strongly (or weakly) stationary process. We particularly give some examples based on high-frequency data of CSI300 Stock Index Futures to show that some technical indicators are mean reverting process. We talk about its advantage and application in high frequency trading. ","SELECTED":null}
{"Title":"Applying Independent Component Analysis and Predictive Systems for Algorithmic Trading","keywords (cleaned)":"Algorithmic trading;Financial time series;Independent component analysis;Mean reverting portfolio;Neural networks;Support vector machines","Abstract":"In this paper, a Nonlinear AutoRegressive network with eXogenous inputs and a support vector machine are proposed for algorithmic trading by predicting the future value of financial time series. These architectures are capable of modeling and predicting vector autoregressive VAR(p) time series. In order to avoid overfitting, the input is pre-processed by independent component analysis to filter out the most noise like component. In this way, the accuracy of the prediction and the trading performance is increased. The proposed algorithms have a small number of free parameters which makes fast learning and trading possible. The method is not only tested on single asset price series, but also on predicting the value of mean reverting portfolios obtained by maximizing the predictability parameter of VAR(1) processes. The tests were first performed on artificially generated data and then on real data selected from exchange traded fund time series including bid\u2013ask spread. In both cases the proposed method could achieve positive returns. ","SELECTED":null}
{"Title":"Statistical arbitrage with optimal causal paths on high-frequency data of the S&P\u00a0500","keywords (cleaned)":"cryptocurrency;Finance;High-frequency trading;Lead\u2013lag structure;Optimal causal path;Statistical arbitrage","Abstract":"This paper develops the optimal causal path algorithm and applies it within a fully-fledged statistical arbitrage framework to minute-by-minute data of the S&P\u00a0500 constituents from 1998 to 2015. Specifically, the algorithm efficiently determines the optimal non-linear mapping and the corresponding lead\u2013lag structure between two time series. Afterwards, this study explores the use of optimal causal paths as a means for identifying promising stock pairs and for generating buy and sell signals. For this purpose, the established trading strategy exploits information about the leading stock to predict future returns of the following stock. The value-add of the proposed framework is assessed by benchmarking it with variants relying on classic similarity measures and a buy-and-hold investment in the S&P\u00a0500 index. In the empirical back-testing study, the trading algorithm generates statistically and economically significant returns of 54.98% p.a. and an annualized Sharpe ratio of 3.57 after transaction costs. Returns are well superior to the benchmark approaches and do not load on any common sources of systematic risk. The strategy outperforms in the context of cryptocurrencies even in recent times due to the fact that stock returns contain substantial information about the future bitcoin returns. ","SELECTED":null}
{"Title":"Deep learning in exchange markets","keywords (cleaned)":"Classification (of information);Theoretical points;Exchange markets;Neural networks (NNS);Commerce;Market depth;classification;Equidae;Time series analysis;Fully automated;Automated trading systems;Short-term forecasting;Betting exchange;Predictive power;Long short-term memory;Deep learning;Multivariate time series analysis","Abstract":"We present the implementation of a short-term forecasting system of price movements in exchange markets using market depth data and a systematic procedure to enable a fully automated trading system. Three types of Deep Learning (DL) Neural Network (NN) methodologies are trained and tested: Deep NN Classifier (DNNC), Long Short-Term Memory (LSTM) and Convolutional NN (CNN). Although the LSTM is more suitable for multivariate time series analysis from a theoretical point of view, test results indicate that the CNN has on average the best predictive power in the case study under analysis, which is the UK to Win Horse Racing market during pre-live stage in the world's most relevant betting exchange. Implications from the generalized use of automated trading systems in betting exchange markets are discussed. ","SELECTED":null}
{"Title":"Grid trading system robot (GTSbot): A novel mathematical algorithm for trading FX market","keywords (cleaned)":"Data forecasting;Financial time-series;Machine learning","Abstract":"Grid algorithmic trading has become quite popular among traders because it shows several advantages with respect to similar approaches. Basically, a grid trading strategy is a method that seeks to make profit on the market movements of the underlying financial instrument by positioning buy and sell orders properly time-spaced (grid distance). The main advantage of the grid trading strategy is the financial sustainability of the algorithm because it provides a robust way to mediate losses in financial transactions even though this also means very complicated trades management algorithm. For these reasons, grid trading is certainly one of the best approaches to be used in high frequency trading (HFT) strategies. Due to the high level of unpredictability of the financial markets, many investment funds and institutional traders are opting for the HFT (high frequency trading) systems, which allow them to obtain high performance due to the large number of financial transactions executed in the short-term timeframe. The combination of HFT strategies with the use of machine learning methods for the financial time series forecast, has significantly improved the capability and overall performance of the modern automated trading systems. Taking this into account, the authors propose an automatic HFT grid trading system that operates in the FOREX (foreign exchange) market. The performance of the proposed algorithm together with the reduced drawdown confirmed the effectiveness and robustness of the proposed approach. ","SELECTED":null}
{"Title":"Deep learning for spatio-temporal modeling: Dynamic traffic flows and high frequency trading","keywords (cleaned)":"Classification (of information);Forecasting;prediction;nonparametric regression;Electronic trading;High-frequency trading;Pollution control;traffic flows;regularization;Mean square error;Deep learning;Commerce;Stochastic systems;Learning architectures;nonparametric regression;Spatio-temporal models;classification;Dynamic traffic flow;High frequency trading;Stochastic gradient descent;traffic flows","Abstract":"Deep learning applies hierarchical layers of hidden variables to construct nonlinear high dimensional predictors. Our goal is to develop and train deep learning architectures for spatio-temporal modeling. Training a deep architecture is achieved by stochastic gradient descent and dropout for parameter regularization with a goal of minimizing out-of-sample predictive mean squared error. To illustrate our methodology, we first predict the sharp discontinuities in traffic flow data, and secondly, we develop a classification rule to predict short-term futures market prices using order book depth. Finally, we conclude with directions for future research. ","SELECTED":null}
{"Title":"Toward Semantic Social Network Analysis for Business Big Data","keywords (cleaned)":"Big data;Commerce;Electronic trading;Semantic Web;Business contracts;Explicit semantics;High-frequency trading;Recent researches;Semantic social network analysis;Stock data;Stock level;Semantics","Abstract":"This paper first presents results of our three recent research projects on using social network analysis (SNA) techniques to analyze business big data involving stock data, trading data, and business contract data. The analysis on historical stock data identifies alternative representative indexing stock groups. The analysis on high frequency trading data establishes new algorithms for more effective high frequency trading. The analysis on business contract networks studies relationships between companies' contracts and their performance in profits and stock levels. The paper then discusses approaches to incorporating explicit semantics into conventional social networks and extending standard social network analysis techniques to more effective semantics-based analysis. ","SELECTED":null}
{"Title":"Big Data Algorithmic Trading Systems Based on Investors\u2019 Mood","keywords (cleaned)":"Algorithmic trading system;Artificial intelligence;Behavioral finance;Big data;Investors\u2019 mood","Abstract":"Traditional automated trading systems use rules and filters based on Chartism to send orders to the market, aiming to beat the market and obtain positive returns in bullish or bearish contexts. However, these systems do not consider the investors\u2019 mood that many studies have demonstrated its effects over the evolution of financial markets. The authors describe 2 \"big data\" algorithmic trading systems over Ibex 35 future. These systems send orders to the market to open long or short positions, based on an artificial intelligence model that uses investors\u2019 mood. To measure the investors' mood, the authors use semantic analysis algorithms that qualify as good, bad, or neutral any communication related to Ibex 35 made on social media (Twitter) or news media. After 1.5 years of research, conclusions are: First, the authors observe positive returns, demonstrating that investors\u2019 mood has predictive capacity on the evolution of the Ibex 35. Second, these systems have beaten the Ibex 35 index, showing the imperfect efficiency of the financial markets. Third, big data algorithmic trading systems numbers are better in Sharpe ratio, success rate, and profit factor than traditional trading systems on the Ibex 35, listed in the Trading Motion platform. ","SELECTED":null}
{"Title":"Exploiting social media with higher-order Factorization Machines: statistical arbitrage on high-frequency data of the S&P 500","keywords (cleaned)":"Factorization Machine;Finance;High-frequency trading;Machine learning;social media;Statistical arbitrage","Abstract":"Over the past 15 years, there have been a number of studies using text mining for predicting stock market data. Two recent publications employed support vector machines and second-order Factorization Machines, respectively, to this end. However, these approaches either completely neglect interactions between the features extracted from the text, or they only account for second-order interactions. In this paper, we apply higher-order Factorization Machines, for which efficient training algorithms have only been available since 2016. As Factorization Machines require hyperparameters to be specified, we also introduce a novel adaptive-order algorithm for automatically determining them. Our study is the first one to make use of social media data for predicting minute-by-minute stock returns, namely the ones of the S&P 500 stock constituents. We show that, unlike a trading strategy employing support vector machines, Factorization-Machine-based strategies attain positive returns after transactions costs for the years 2014 and 2015. Especially the approach applying the adaptive-order algorithm outperforms classical approaches with respect to a multitude of criteria, and it features very favorable characteristics. ","SELECTED":null}
{"Title":"An investigation on factors affecting stock valuation using text mining for automated trading","keywords (cleaned)":"Algorithms;questionnaire survey;Data mining;The LDA model;Matthiola incana;model validation;automated trading systems (ATS);software;numerical model;qualitative analysis;valuation;Stock valuation;P\/BV ratio;accuracy assessment","Abstract":"Predicted price-to-book value ratios (P\/BV) are widely used for the valuation of listed common stocks. However, with the application of automated trading system (ATS), the existing indicators that are applied in the method are losing their effectiveness in the Chinese market. Combining qualitative research with the text mining method, this study explores and validates those ignored factors to improve the accuracy of the stock valuation. On the basis of the principal of the existing valuation method, we clarify the scope of the factors that affects the P\/BV ratio prediction. Through semi-structured interviews that are designed with six first-level factors which are taken from the literature, we then excavate some second-level factors. After that, with three corpuses including samples form Sina.com.cn, Xueqiu.com, and CSDN.net, four first-level factors and thirteen second-level factors have been verified step by step through the Latent Dirichlet Allocation (LDA) model. In the process, two other new factors and three sub-factors are also found. Furthermore, based on the factor correlation that was found in a data analysis, a factor relationship model was built. The results can be used in a stock valuation in future work as the basis of the indicator system for the prediction of P\/BV ratio. ","SELECTED":null}
{"Title":"Optimal Mean-Reverting Portfolio With Leverage Constraint for Statistical Arbitrage in Finance","keywords (cleaned)":"leverage constraint;Problem solving;Algorithmic trading;Cointegration;Mean reversion;Electronic trading;sparse optimization;Portfolio optimization;Cointegration;asset selection;successive convex approximation;Pairs trading;quantitative trading;nonconvex optimization;mean reversion strategy;Financial markets;Algorithmic trading;Commerce;Pairs trading;sparse optimization;Investments;nonconvex optimization;successive convex approximation","Abstract":"The optimal mean-reverting portfolio (MRP) design problem is an important task for statistical arbitrage, also known as pairs trading, in the financial markets. The target of the problem is to construct a portfolio of the underlying assets (possibly with an asset selection target) that can exhibit a satisfactory mean reversion property and a desirable variance property. In this paper, the optimal MRP design problem is studied under an investment leverage constraint representing the total investment positions on the underlying assets. A general problem formulation is proposed by considering the design targets subject to a leverage constraint. To solve the problem, a unified optimization framework based on the successive convex approximation method is developed. The superior performance of the proposed formulation and the algorithms are verified through numerical simulations on both synthetic data and real market data. ","SELECTED":null}
{"Title":"Functional GARCH models: The quasi-likelihood approach and its applications","keywords (cleaned)":"High-frequency volatility models;High frequency HF;Time series;Functional time series;Stationarity of functional GARCH;Commerce;Time series analysis;Intraday returns;Electronic trading;Stationarity;Functional QMLE","Abstract":"The increasing availability of high frequency data has initiated many new research areas in statistics. Functional data analysis (FDA) is one such innovative approach towards modelling time series data. In FDA, densely observed data are transformed into curves and then each (random) curve is considered as one data object. A natural, but still relatively unexplored, context for FDA methods is related to financial data, where high-frequency trading currently takes a significant proportion of trading volumes. Recently, articles on functional versions of the famous ARCH and GARCH models have appeared. Due to their technical complexity, existing estimators of the underlying functional parameters are moment based\u2014an approach which is known to be relatively inefficient in this context. In this paper, we promote an alternative quasi-likelihood approach, for which we derive consistency and asymptotic normality results. We support the relevance of our approach by simulations and illustrate its use by forecasting realised volatility of the S&P100 Index. ","SELECTED":null}
{"Title":"High-frequency trading from an evolutionary perspective: Financial markets as adaptive systems","keywords (cleaned)":"Market efficiency;Malus x domestica;Efficient market hypothesis;Stock market;adaptive management;price dynamics;Evolutionary computation;Genetic programming;adaptive market hypothesis;High-frequency trading;Financial markets;hypothesis testing","Abstract":"The recent rapid growth of algorithmic high-frequency trading strategies makes it a very interesting time to revisit the long-standing debates about the efficiency of stock prices and the best way to model the actions of market participants. To evaluate the evolution of stock price predictability at the millisecond timeframe and to examine whether it is consistent with the newly formed adaptive market hypothesis, we develop three artificial stock markets using a strongly typed genetic programming (STGP) trading algorithm. We simulate real-life trading by applying STGP to millisecond data of the three highest capitalized stocks: Apple, Exxon Mobil, and Google and observe that profit opportunities at the millisecond time frame are better modelled through an evolutionary process involving natural selection, adaptation, learning, and dynamic evolution than by using conventional analytical techniques. We use combinations of forecasting techniques as benchmarks to demonstrate that different heuristics enable artificial traders to be ecologically rational, making adaptive decisions that combine forecasting accuracy with speed. ","SELECTED":null}
{"Title":"Trading co-integrated assets with price impact","keywords (cleaned)":"Algorithmic trading;Cointegration;co-movements;cross-price impact;Optimal execution;Price impact","Abstract":"Executing a basket of co-integrated assets is an important task facing investors. Here, we show how to do this accounting for the informational advantage gained from assets within and outside the basket, as well as for the permanent price impact of market orders (MOs) from all market participants, and the temporary impact that the agent's MOs have on prices. The execution problem is posed as an optimal stochastic control problem and we demonstrate that, under some mild conditions, the value function admits a closed-form solution, and prove a verification theorem. Furthermore, we use data of five stocks traded in the Nasdaq exchange to estimate the model parameters and use simulations to illustrate the performance of the strategy. As an example, the agent liquidates a portfolio consisting of shares in Intel Corporation and Market Vectors Semiconductor ETF. We show that including the information provided by three additional assets (FARO Technologies, NetApp, Oracle Corporation) considerably improves the strategy's performance; for the portfolio we execute, it outperforms the multiasset version of Almgren\u2013Chriss by approximately 4\u20134.5 basis points. ","SELECTED":null}
{"Title":"Evaluating the effectiveness of candlestick analysis in forecasting U.S. stock market","keywords (cleaned)":"Buy-and-hold strategy;Financial markets;Stock price;Expected return;Investments;Technical analysis;Commerce;Trading strategies;Trading strategies;Forecasting;Information analysis;Data handling;Candlestick;Learning systems;Electronic trading;Prior experience;Profitability;High-frequency trading","Abstract":"Predicting the stock market trend has drawn wide attention from the public, industry, and academia. There are various strategies for investment, ranging from the buy-and-hold strategy that let time makes money to high-frequency trading based on complicated models of machine learning. For the general public, the candlestick analysis is a simple and straightforward way to predict the market trend and help make decisions on buy and sell. However, the general effectiveness and validity of such methodology are understudied, and many previous guidebooks on candlestick analysis were based on specific cases and the summary of prior experiences, but lack unbiased and rigorous confirmation. Here, we aim at evaluating the effectiveness of multiple famous candlestick patterns based on recent data of 20 U.S. stocks. By estimating the fraction of correct guesses and expected return, we were able to show that three of the four patterns under scrutiny were not effective in generating profitable outcomes, but one of them do provide valuable information on market trend reversal. Although testing more candlestick patterns and using more test data may lead to the discoveries of more patterns being significantly profitable, our results call for scrutiny and scientific guidance for the general public when applying the candlestick analysis, and questions the general validity and applicability of such analyses. ","SELECTED":null}
{"Title":"Gaussian mixture and kernel density-based hybrid model for volatility behavior extraction from public financial data","keywords (cleaned)":"Algorithmic trading;Foreign exchange market;Gaussian mixture model;Kernel density estimation","Abstract":"This paper carried out a hybrid clustering model for foreign exchange market volatility clustering. The proposed model is built using a Gaussian Mixture Model and the inference is done using an Expectation Maximization algorithm. A mono-dimensional kernel density estimator is used in order to build a probability density based on all historical observations. That allows us to evaluate the behavior\u2019s probability of each symbol of interest. The computation result shows that the approach is able to pinpoint risky and safe hours to trade a given currency pair. ","SELECTED":null}
{"Title":"Regulating high-frequency trading: The case for individual criminal liability","keywords (cleaned)":"computers|r|high-frequency traders|securities trading|High-frequency trading|data centers|algorithms|trading volume|high-frequency trading|R|pin|trading|supercomputers|market|High-frequency|enforcement|stock exchange|regulation|algorithm|technology|stock|trading strategy|edi|ace|trade|high-frequency","Abstract":"The popular imagination of securities trading is a chaotic, physical stock exchange\u2014a busy floor with hurried traders yelling, \u201cbuy, buy, buy!\u201d While this image is a Hollywood and media favorite, it is no longer accurate. In 2019, most securities trading is conducted electronically on digital markets. One type of trading strategy, high-frequency trading, utilizes algorithms, data centers, fiber optic cables, and supercomputers to obtain an edge in the market. High-frequency trading has leveraged advancements in technology to constitute over half of all trading volume in a given day. High-frequency trading, however, has come under scrutiny in recent years as it has increased market susceptibility to certain forms of criminal conduct. In 2017, the U.S. Court of Appeals for the Seventh Circuit upheld the first conviction of a high-frequency trader for spoofing, a type of trader misconduct that is made more susceptible by high-frequency trading. While scholars have debated whether high-frequency trading should be regulated more than other types of trading and if so, what the regulations might look like, no one has analyzed criminal law as a vehicle to regulate high-frequency trading. This Comment makes the case that individual criminal liability is an ideal tool to regulate misconduct in the high-frequency trading space. Two features of high-frequency trading make the strategy particularly challenging to regulate: 1) it is difficult to draw a line between legitimate and illegitimate behavior in high-frequency trading; and 2) it is difficult to pinpoint an exact definition of what high-frequency trading is. Criminal liability has several advantages over civil liability with respect to these challenges. First, the mens rea component and higher standard of proof required in criminal liability will ensure that high-frequency traders found criminally liable engaged in illegitimate behavior with a higher degree of certainty. Second, the threat of criminal prosecution will better serve the goal of deterring high-frequency trader misconduct. Within the context of criminal liability, individual criminal liability is preferable to corporate criminal liability because the former better furthers the goal of deterrence. The identity problem that corporate liability helps to solve\u2014in some corporate contexts it is impossible to pinpoint culpability on any single individual\u2014is not an issue in high-frequency trading; and individual criminal liability is socially more preferable as a matter of policy. Accordingly, the government should increase criminal enforcement of high-frequency traders to promote its goal of safeguarding market integrity. ","SELECTED":null}
{"Title":"Liquidity withdrawal in the FX spot market: A cross-country study using high-frequency data","keywords (cleaned)":"Algorithmic trading;Foreign exchange;High-frequency trading;Limit order book;Liquidity;Market microstructure","Abstract":"This paper studies short-term liquidity withdrawal in the FX spot market for eight currency pairs. We include over 3 million limit order submissions, worth more than $5 trillion, and investigate the drivers of two different measures of volume-based liquidity. Overall, we find that market participants react differently to changes in the state of the market for different currency pairs. Moreover, the liquidity withdrawal process also differs depending on the perceived information content of new limit orders submitted. Finally, we document that a \u2018liquidity illusion\u2019 might exist in FX spot markets electronic trading platforms where algorithmic and high-frequency trading is prominent. ","SELECTED":null}
{"Title":"Improving financial trading decisions using deep Q-learning: Predicting the number of Shares, action Strategies, and transfer learning","keywords (cleaned)":"Q-learning;Financial markets;Deep Q-learning;Deep neural networks;Commerce;Trading strategies;Stock trading;Automation;Forecasting;Trading strategies;Automated trading systems;Financial trading;Automated systems;Electronic trading;Transfer learning;Reinforcement learning models;Reinforcement learning;Profitability","Abstract":"We study trading systems using reinforcement learning with three newly proposed methods to maximize total profits and reflect real financial market situations while overcoming the limitations of financial data. First, we propose a trading system that can predict the number of shares to trade. Specifically, we design an automated system that predicts the number of shares by adding a deep neural network (DNN) regressor to a deep Q-network, thereby combining reinforcement learning and a DNN. Second, we study various action strategies that use Q-values to analyze which action strategies are beneficial for profits in a confused market. Finally, we propose transfer learning approaches to prevent overfitting from insufficient financial data. We use four different stock indices\u2014the S&P500, KOSPI, HSI, and EuroStoxx50\u2014to experimentally verify our proposed methods and then conduct extensive research. The proposed automated trading system, which enables us to predict the number of shares with the DNN regressor, increases total profits by four times in S&P500, five times in KOSPI, 12 times in HSI, and six times in EuroStoxx50 compared with the fixed-number trading system. When the market situation is confused, delaying the decision to buy or sell increases total profits by 18% in S&P500, 24% in KOSPI, and 49% in EuroStoxx50. Further, transfer learning increases total profits by twofold in S&P500, 3 times in KOSPI, twofold in HSI, and 2.5 times in EuroStoxx50. The trading system with all three proposed methods increases total profits by 13 times in S&P500, 24 times in KOSPI, 30 times in HSI, and 18 times in EuroStoxx50, outperforming the market and the reinforcement learning model. ","SELECTED":null}
{"Title":"MiFID II key concerns","keywords (cleaned)":"Dark pools;High frequency trading;MiFID II;MiFIR;Organised trading facility;Transparency reporting","Abstract":"Purpose: This paper aims to discuss key concerns surrounding the recent implementation of the Markets in Financial Instruments Directive (MIFID II). It focuses on the UK regime. The insights derived are envisaged to be helpful guides for participants and regulators in financial markets. Design\/methodology\/approach: This paper used the legal-economics perspective. It relied on primary data from statutes and regulations and secondary data from the public domain to analyze the phenomenon. The analytical framework comprised the following sections: Introduction, MiFID I review, MiFID II scope, MiFID II key concerns and concluding remarks. Findings: Only half of the EU Member States including the UK managed to transpose MiFID II within the 3rd January 2018 effective date. At this early stage of implementation, various teething problems were encountered. These pertained to costs and charges reporting, firm governance, product governance, transaction reporting, best execution and research. Owing to the sheer scale and complexity of MIFID II, most entities barely coped with their reporting obligations. Noting the situation, the Financial Conduct Authority assured firms taking all sufficient steps that they would be treated fairly. Research limitations\/implications: The paper was not sufficiently empirical. However, the study benefited reasonably from triangulation of data and perspectives to provide good insights on the implementation effects of the complex and voluminous EU rules for governing financial markets with global implications. Practical implications: Investors could gain from the enhanced transparency and best execution rules. Investment banks could gain from the emerging resilient, integrated and efficient financial markets. Regulators with better access to more and higher quality reporting could intervene more effectively when required. Originality\/value: This paper assembled and critically analyzed currently available research insights in these areas so as to provide useful guidance to those needing to work and comply with MiFID II rules and academics teaching financial services law. ","SELECTED":null}
{"Title":"An approach based on heterogeneous multiagent system for stock market speculation","keywords (cleaned)":"Fundamental factor;Metaheuristic;Multi agent systems;Particle swarm optimization;Smart agent;Speculation;Stock market;Technical indicator;Text mining","Abstract":"Foreign Exchange market (FOREX) is the global and most liquid market interested in buying, selling and exchanging currencies. The price of these currencies changes rapidly. However, we need to have a good trading strategy to take advantage of these variations. As Forex is a dynamic market, it becomes more difficult to control trading behavior and it becomes very complex to predict the events that can occur. Due to the chaotic, noisy, and nonstationary nature of the data, many algorithmic approaches were adopted in the aim to help traders and make FOREX speculation successful. Algorithmic trading offers the ability to have a strategy in advance. It helps traders to take the final decision. When the decision is so wide and complex, that one algorithm cannot possess all rules to take it. It becomes necessary to call upon several agents, who must work together in pursuing a common objective. These agents co-operate with one another to solve these decision problems. Coordinating between agents in a multi-agent system gives more flexibility and performance to problem solving. Each agent simultaneous results are combined by using a super-agent who helps to make the final decision. In our paper, we propose a theoretical Multi Agent System for stock market Speculation. We use four agents working in one system. The first one is a Metaheuristic Algorithm agent, the second one is based on technical indicators, the third one is a Text Mining agent, and the fourth one is a Fundamental Factor agent. The final decision should be made based on the combination of the four agents results. We think that working with Metaheuristics can improve speculation results compared to the use of classical algorithms. In perspective work, we test our system on a multiagent systems platform. ","SELECTED":null}
{"Title":"Using Stock Prices as Ground Truth in Sentiment Analysis to Generate Profitable Trading Signals","keywords (cleaned)":"Bayesian classifier;Artificial intelligence;Forecasting;Sentiment analysis;Electronic trading;Financial engineering;Automated trading;Stock predictions;Financial prediction;Financial markets;Costs;Commerce;Financial markets;Learning systems;Ground truth;Social media datum;Machine learning;Financial engineering;Large volumes;Sentiment analysis;Automated trading;Data mining","Abstract":"The increasing availability of 'big' (large volume) social media data has motivated a great deal of research in applying sentiment analysis to predict the movement of prices within financial markets. Previous work in this field investigates how the true sentiment of text (i.e., positive or negative opinions) can be used for financial predictions, based on the assumption that sentiments expressed online are representative of the true market sentiment. Here we consider the converse idea, that using the stock price as the ground-truth in the system may be a better indication of sentiment. Tweets are labelled as Buy or Sell dependent on whether the stock price discussed rose or fell over the following hour, and from this, stock-specific dictionaries are built for individual companies. A Bayesian classifier is used to generate stock predictions, which are input to an automated trading algorithm. Placing 468 trades over a 1 month period yields a return rate of 5.18%, which annualises to approximately 83% per annum. This approach performs significantly better than random chance and outperforms two baseline sentiment analysis methods tested. ","SELECTED":null}
{"Title":"Neural Network based Trading Signal Generation in Cypto-Currency Markets","keywords (cleaned)":"Financial markets;Investments;Currency markets;Technical indicator;Time-periods;Commerce;Historical data;Signals generation;Algorithmic trading;Signals generation;Neural networks;Data handling;Neural network estimators;Electronic trading;Trading Signal Generation;Learning algorithms;Digital assets;Machine learning;Technical indicator","Abstract":"Machine learning algorithms are commonly used to automate stock market trading. Crypto-currencies are novel digital assets that attracted investors all over the world. In this paper, we developed a neural network estimator to generate trading signals. Unlike previous methods, our method uses more volatile and uses historical data divided five minutes intervals. First, our method uses technical indicators and optimizes their time periods then we developed artificial neural network (ANN) architectures to predict asset future directions. Classification and regression networks are developed and their results are compared. Our results are promising but need improvements in order to make more profitable trading. ","SELECTED":null}
{"Title":"Revealing high-frequency trading provision of liquidity with visualization","keywords (cleaned)":"Data visualization;Aggregates;Investments;High-frequency trading;High frequency HF;Network diagrams;Visualization;Commerce;Large data;Big data;Liquidity provisions;Risk management;Tick datum;Large datasets;High frequency trading;Software engineering;Electronic trading;Information management;Tick data;adverse selection","Abstract":"Liquidity is crucial for successful financial markets. It ensures that all investors are able to buy and sell assets quickly at a fair price. High Frequency Traders (HFTs) utilize sophisticated algorithms operating with extreme speed and are frequently cited as liquidity providers. The objective of this paper is to investigate the liquidity provision of a number of HFTs to determine their effects on aggregate marketplace liquidity. We consider a large data set collected from the Australian Securities Exchange throughout 2013, providing a near complete picture of all trading activity. Our method is to consider temporal bar charts, association scatterplots, faceted plots and network diagrams to provide visualizations that yield both novel and conventional insights into how HFTs are operating in the market, specifically with respect to liquidity provision. Consistent with HFTs avoiding adverse selection, our results show that on aggregate, HFTs often consume rather than provide liquidity. Furthermore, liquidity consumption often occurs very quickly over intra-millisecond time periods. We conclude that HFTs are not exclusively focused on liquidity provision. ","SELECTED":null}
{"Title":"Programmatic trading: the future of audience economics","keywords (cleaned)":"Advertising;audience economics;audience measurement;programmatic","Abstract":"Communication researchers examining how audiences are commoditised in the audience marketplace by commercial media providers and advertisers have been using audience economic models that were applied to commercial television, radio and print audiences. New digital technologies are transforming the way audiences are commoditised and previous models of the audience marketplace are not applicable to the new audience marketplace. This paper examines how audience economy and audience marketplace theories evolved and describes how programmatic advertising\u2014the automated trading of the audience commodity\u2014differs from previous audience theories requiring a need to re-examine these theories. New digital technology is being used to not only find audiences online but also uses data to create profiles of individuals audience members. The datafied audience plays a central role in the programmatic trading of the evolving audience commodity and the new audience marketplace that has emerged. ","SELECTED":null}
{"Title":"Forecasting Financial Markets Using High-Frequency Trading Data: Examination with Strongly Typed Genetic Programming","keywords (cleaned)":"Algorithmic trading;Artificial intelligence;Big data analytics;Evolutionary computation;financial econometrics;High-frequency trading","Abstract":"Market regulators around the world are still debating whether high-frequency trading (HFT) plays a positive or negative role in market quality. We develop an artificial futures market populated with high-frequency traders (HFTs) and institutional traders using Strongly Typed Genetic Programming (STGP) trading algorithm. We simulate real-life futures trading at the millisecond time frame by applying STGP to E-Mini S&P 500 data stamped at the millisecond interval. A direct forecasting comparison between HFTs and institutional traders indicate the superiority of the former. We observe that the negative implications of high-frequency trading in futures markets can be mitigated by introducing a minimum resting trading period of less than 50 milliseconds. Overall, we contribute to the e-commerce literature by showing that minimum resting trading order period of less than 50 milliseconds could lead to HFTs facing a queuing risk resulting in a less harmful market quality effect. One practical implication of our study is that we demonstrate that market regulators and\/or e-commerce practitioners can apply artificial intelligence tools such as STGP to conduct trading behavior-based profiling. This can be used to detect the occurrence of new HFT strategies and examine their impact on the futures market. ","SELECTED":null}
{"Title":"High-frequency trading: Order-based innovation or manipulation?","keywords (cleaned)":"High-frequency trading;Instability;Monopoly;Order-based manipulation;Regulation","Abstract":"High-frequency trading (HFT) is a financial innovation that focuses on order flow and relies on quickly evolving information and communication technology. The innovation is successful, and HFT is highly and consistently profitable. However, the Flash Crash on 6 May 2010 exposed the unfamiliar side of HFT, thus illuminating the emergent need to unveil the negative impact that HFT has on other investors and the market. This paper examines data regarding quote-stuffing, spoofing, and market making provided by high-frequency (HF) traders, based on the increasing empirical literature. It first defines order-based manipulation (OBM) as the framework under which quote-stuffing, spoofing, and HF market making find common ground. It then provides details regarding how OBM is displayed in the three manipulation tactics. In essence, they all seek and exercise monopoly power in trading albeit through different ways of achieving it. The shared purpose is to gain monopolistic profit. The essence and common purpose explain why HF traders are not net liquidity providers, contrary to some proponents\u2019 conclusions. Rather, this paper points out the three consequences that HF traders have brought to the market, i.e. increased volatility, increased frequency of unfairness, and instability potential. Recent regulatory improvement and completed prosecutions against manipulative HFT strategies justify the analysis. ","SELECTED":null}
{"Title":"Deep Neural Trading: Comparative Study with Feed Forward, Recurrent and Autoencoder Networks","keywords (cleaned)":"Experimental comparison;Stock value prediction;LSTM;Time series;Electronic trading;trade;Dow Jones Industrial averages;Information management;Long short-term memory;Deep learning;Financial markets;Commerce;Multivariate time series;Network architecture;quantitative finance;Autoencoder;Auto encoders;Feed forward neural network;Stock market prediction;Feedforward neural networks;Recurrent neural networks;Deep neural networks;Social networking (online);Recurrent neural networks","Abstract":"Algorithmic trading approaches based on news or social network posts claim to outperform classical methods that use only price time series and other economics values. However combining financial time series with news or posts, requires daily huge amount of relevant text which are impracticable to gather in real time, even because the online sources of news and social networks no longer allow unconditional massive download of data. These difficulties have renewed the interest in simpler methods based on financial time series. This work presents a wide experimental comparisons of the performance of 7 trading protocols applied to 27 component stocks of the Dow Jones Industrial Average (DJIA). The buy\/sell trading actions are driven by the stock value predictions performed with 3 types of neural network architectures: feed forward, recurrent and autoencoder. Each architecture types in turn has been experimented with different sizes and hyperparameters over all the multivariate time series. The combinations of trading protocols with variants of the 3 neural network types have been in turn applied to time series, varying the input variables from 4 to 17 and the training period from 8 to 16\u00a0years while the test period from 1 to 2\u00a0years. ","SELECTED":null}
{"Title":"Feature Engineering for Mid-Price Prediction with Deep Learning","keywords (cleaned)":"Economics;U.S. data;Electronic trading;Financial markets;Multilayer neural networks;Statistics;Limit order book;Automation;Forecasting;econometrics;mid-price;Correlation theory;Multiobjective optimization;Motion estimation;High-frequency trading;Brain;Long short-term memory;Limit order book;Deep learning;High-frequency trading","Abstract":"Mid-price movement prediction based on the limit order book data is a challenging task due to the complexity and dynamics of the limit order book. So far, there have been very limited attempts for extracting relevant features based on the limit order book data. In this paper, we address this problem by designing a new set of handcrafted features and performing an extensive experimental evaluation on both liquid and illiquid stocks. More specifically, we present an extensive set of econometric features that capture the statistical properties of the underlying securities for the task of mid-price prediction. The experimental evaluation consists of a head-to-head comparison with other handcrafted features from the literature and with features extracted from a long short-term memory autoencoder by means of a fully automated process. Moreover, we develop a new experimental protocol for online learning that treats the task above as a multi-objective optimization problem and predicts: 1) the direction of the next price movement and; 2) the number of order book events that occur until the change takes place. In order to predict the mid-price movement, features are fed into nine different deep learning models based on multi-layer perceptrons, convolutional neural networks, and long short-term memory neural networks. The performance of the proposed method is then evaluated on liquid and illiquid stocks (i.e., TotalView-ITCH US and Nordic stocks). For some stocks, results suggest that the correct choice of a feature set and a model can lead to the successful prediction of how long it takes to have a stock price movement. ","SELECTED":null}
{"Title":"How rigged are stock markets? Evidence from microsecond timestamps","keywords (cleaned)":"High-frequency trading;Latency arbitrage;Market structure;SIP","Abstract":"Using new data from the two U.S. securities information processors (SIPs) between August 6, 2015 and June 30, 2016, we examine claims that high-frequency trading (HFT) firms use direct feeds to exploit traders who rely on SIP prices. Across $3.7 trillion of trades in the Dow Jones 30, the SIPs report quote updates from exchanges 1,128 \u03bcs after they occur. However, the SIP-reported National Best Bid and Offer (NBBO) matches the NBBO calculated without reporting latencies in 97% of all SIP-priced trades. Liquidity-taking orders gain on average $0.0002\/share when priced at the SIP-reported NBBO rather than the instantaneous NBBO, but aggregate gross profits are just $14.4 million. These findings indicate that direct feed arbitrage is not a meaningful source of HFT profits, nor can it explain the arms race for trading speed. ","SELECTED":null}
{"Title":"Fractional risk process in insurance","keywords (cleaned)":"Convex risk measures;Fractional Poisson process;Renewal process;Risk process","Abstract":"The Poisson process suitably models the time of successive events and thus has numerous applications in statistics, in economics, it is also fundamental in queueing theory. Economic applications include trading and nowadays particularly high frequency trading. Of outstanding importance are applications in insurance, where arrival times of successive claims are of vital importance. It turns out, however, that real data do not always support the genuine Poisson process. This has lead to variants and augmentations such as time dependent and varying intensities, for example. This paper investigates the fractional Poisson process. We introduce the process and elaborate its main characteristics. The exemplary application considered here is the Carm\u00e9r\u2013Lundberg theory and the Sparre Andersen model. The fractional regime leads to initial economic stress. On the other hand we demonstrate that the average capital required to recover a company after ruin does not change when switching to the fractional Poisson regime. We finally address particular risk measures, which allow simple evaluations in an environment governed by the fractional Poisson process. ","SELECTED":null}
{"Title":"Portfolio Selection Based on Hierarchical Clustering and Inverse-Variance Weighting","keywords (cleaned)":"Financial markets;Hierarchical clustering algorithms;Portfolio selection;Portfolio selection models;Clustering algorithms;Algorithmic trading;Inverse problems;Inverse-variance weighting;Correlation coefficient;Hier-archical clustering;Electronic trading;Machine learning;Variance Weighting;Machine learning techniques;Portfolio construction","Abstract":"This paper presents a remarkable model for portfolio selection using inverse-variance weighting and machine learning techniques such as hierarchical clustering algorithms. This method allows building diversified portfolios that have a good balance sector exposure and style exposure, respect to momentum, size, value, short-term reversal, and volatility. Furthermore, we compare performance for seven hierarchical algorithms: Single, Complete, Average, Weighted, Centroid, Median and Ward Linkages. Results show that the Average Linkage algorithm has the best Cophenetic Correlation Coefficient. The proposed method using the best linkage criteria is tested against real data over a two-year dataset of one-minute American stocks returns. The portfolio selection model achieves a good financial return and an outstanding result in the annual volatility of 3.2%. The results suggest good behavior in performance indicators with a Sharpe ratio of 0.89, an Omega ratio of 1.16, a Sortino ratio of 1.29 and a beta to S&P of 0.26. ","SELECTED":null}
{"Title":"Automated Negotiations Under User Preference Uncertainty: A Linear Programming Approach","keywords (cleaned)":"Commerce;Economic and social effects;Electric power transmission networks;Electronic trading;Information use;Linear programming;Smart power grids;Automated negotiations;High-frequency trading;Limited information;Linear optimization;Pair-wise comparison;Partial information;Partial information model;Preference uncertainty;Autonomous agents","Abstract":"Autonomous agents negotiating on our behalf find applications in everyday life in many domains such as high frequency trading, cloud computing and the smart grid among others. The agents negotiate with one another to reach the best agreement for the users they represent. An obstacle in the future of automated negotiators is that the agent may not always have a priori information about the preferences of the user it represents. The purpose of this work is to develop an agent that will be able to negotiate given partial information about the user\u2019s preferences. First, we present a new partial information model that is supplied to the agent, which is based on categorical data in the form of pairwise comparisons of outcomes instead of precise utility information. Using this partial information, we develop an estimation model that uses linear optimization and translates the information into utility estimates. We test our methods in a negotiation scenario based on a smart grid cooperative where agents participate in energy trade-offs. The results show that already with very limited information the model becomes accurate quickly and performs well in an actual negotiation setting. Our work provides valuable insight into how uncertainty affects an agent\u2019s negotiation performance, how much information is needed to be able to formulate an accurate user model, and shows a capability of negotiating effectively with minimal user feedback. ","SELECTED":null}
{"Title":"Absolute vs. relative speed in high-frequency trading","keywords (cleaned)":"High-frequency trading;Order cancellation;Speed;Sub-second scale","Abstract":"This paper addresses the little investigated topic of the relationship between the speed of exchange servers, an absolute reference for the system, and trading speed, considered relative to the former. This is a major issue, as trading speed overwhelming the capability of the server to cope with the incoming orders might jeopardise the orderly functioning of the markets. It will be shown how, by raising the speed of trading and increasing the number of the agents operating in the market, it is possible to generate a crisis, no matter how performing the exchange server is. The paper presents a theoretical framework and then verifies its occurrence by analysing audit trail data. The theoretical framework shows a scenario in which under certain, heavy but by no means uncommon, conditions, the excess speed of the trading agents with respect to servers is capable of exacerbating price volatility, leading to vicious feedback loops capable of potentially creating a financial crisis. The empirical part analyses data taken from a particularly volatile day and compares them with much less volatile days. It results that, because of excessive speed, one of the most widely used techniques for minimising risk, order churning, can cause a major crisis. ","SELECTED":null}
{"Title":"Structural changes in exchange rate-stock returns dynamics in South Africa: examining the role of crisis and new trading platform","keywords (cleaned)":"Emerging economies;Exchange rates;Flexible Fourier form unit root test;High frequency trading;Millennium trading platform;N-ARDL model;South Africa;Stock returns","Abstract":"The 2007 sub-prime crisis and the adoption of Millennium trading platform represent two of the most important recent structural developments for the Johannesburg Stock Exchange (JSE). Under an environment of flexible and volatile exchange rates, this study seeks to examine the effects of these two structural events on the exchange rate-equity returns nexus for 4 JSE indices using the nonlinear autoregressive distributive lag cointegration. We use monthly data collected from 2000:M01 to 2017:M12, and conduct our empirical analysis over sub-periods corresponding to breaks caused by the crisis and the use of a new trading platform. We find prior the crisis exchange rates appreciations generally cause stock returns whereas depreciations are unlikely to cause stock returns to decrease. However, during crisis period this relationship entire disappears whilst resurfacing subsequent to the adoption of a new trading platform although the dynamics of the time series differs between sectors. Our overall empirical results caution regulatory authorities to closely monitor stock market developments as the new trading platform offers market participants opportunities of using the exchange rate to beat the market. ","SELECTED":null}
{"Title":"International Conference on Digital Science, DSIC 2018","keywords (cleaned)":"America|automated trading system|implementation|information|r|education|real time|fuzzy logic|models|trading system|automated trading|innovation|information system|financial reporting|management|quantification|neural network|R|trading|information systems|industry|market|cryptocurrency|estimation|artificial intelligence|automated trading systems|expert systems|trading systems|commerce|edi|efficiency|wire|ace|mes|valuation|deep learning","Abstract":"The proceedings contain 53 papers. The special focus in this conference is on Digital Science. The topics include: Self-adaptive intelligent system for mass evaluation of real estate market in cities; comparability of financial reporting under different tax regimes; methodology of construction management models of actors of nature; digital innovations in the global exhibition industry \u2013 Synergy of new digital technologies and live communication measures; expert systems of real time as key tendency of artificial intelligence in tax administration; fuzzy logic model for the selection of applicants to university study programs according to enrollment profile; the digitization of the Russian higher education; m-Learning didactic strategy for children diagnosed with dyslexia; a comparative study on maturity models for information systems in higher education institutions; collecting the database for the neural network deep learning implementation; automatic test generation on the basis of a semantic network; data literacy as a compound competence; particularities of language classes in a multi-cultural context; intuition and quantification of mental variables of cognition subjects in the processes of comprehension of the surrounding world; two-diode model parameter evaluation from dark characteristics of back-contact back-junction solar cells; efficiency of back contact-back junction solar cells with variable contact in the emitter; recommendation system for material scientists based on deep learn neural network; digital food product traceability: Using blockchain in the international commerce; a reliable wireless communication system for Hazardous environments; approach of estimation of the fair value of assets on a cryptocurrency market; optimizing automated trading systems; are Latin American youtubers influential?.","SELECTED":null}
{"Title":"Optimizing automated trading systems","keywords (cleaned)":"Generic strategies;Trading systems;Optimization framework;Commerce;Economic analysis;Trading systems;Optimization algorithms;Computer scientists;Automated trading systems;Electronic trading;Binary Options;Optimization algorithms","Abstract":"In 2016, more than the 80% of transactions in the Forex market (where the world\u2019s currencies trade) have been directed by robots. The design of profitable automatic trading systems is becoming a challenging process. This requires a strong synergy of economists and computer scientists. Our aim is to provide an optimization framework for trading systems that starting from a generic strategy, enhances its performances by exploiting mathematical constraints. Moreover, the growth of new markets requires suitable solutions integrating computer science tools with economic analysis. In this work, we mainly refer to an emerging market known as Binary Options. Starting from basic strategies used every day in the stock markets by professional traders, we show how optimization issues enhance the outcoming performances. Tests on the optimized algorithms are conducted on both historical and real time data. ","SELECTED":null}
{"Title":"Big Data Analytics: A Trading Strategy of NSE Stocks Using Bollinger Bands Analysis","keywords (cleaned)":"Financial markets;Bollinger bands;Distributed computer systems;Chartists;Commerce;NSE;Big data;Algorithmic trading;Trading strategies;Spark;Blue chip companies;Electronic trading;Big data analytics;Standard deviation;Electric sparks;Intraday trading;Profitability","Abstract":"The availability of huge distributed computing power using frameworks like Hadoop and Spark has facilitated algorithmic trading employing technical analysis of Big Data. We used the conventional Bollinger Bands set at two standard deviations based on a band of moving average over 20 minute-by-minute price values. The Nifty 50, a portfolio of blue chip companies, is a stock index of National Stock Exchange (NSE) of India reflecting the overall market sentiment. In this work, we analyze the intraday trading strategy employing the concept of Bollinger Bands to identify stocks that generates maximum profit. We have also examined the profits generated over one trading year. The tick-by-tick stock market data has been sourced from the NSE and was purchased by Amrita School of Business. The tick-by-tick data being typically Big Data was converted to a minute data on a distributed Spark platform prior to the analysis. ","SELECTED":null}
{"Title":"MAVIS: A multiagent value investing system","keywords (cleaned)":"Multi agent systems;Automated trading systems;Financial data processing;Cluster analysis;Portfolio optimization;Portfolio managements;Portfolio optimization;Automation;Intelligent systems;Financial markets;Value investing;Commerce;Portfolio managements;Utility functions;Financial health;Investments;Return value;Multiagent systems;Cluster analysis;Automated trading systems;Financial distress prediction model","Abstract":"Portfolio management is a challenging task where humans have to make decisions under uncertainty. Since usually humans tend to avoid unknown risk, in general they don't maximize their utility function when managing a portfolio. This fact favours using an automated trading system for portfolio management. In this work, we propose an automated trading system using multiagent systems. We use fundamental and cluster analysis to select the stocks, and additionally we employ a financial distress prediction model to estimate companies financial health. We also optimize the portfolio for different investor's utility functions. Comparing our approach's results to a benchmark, we have obtained higher return values and lower risks; moreover, the approach was profitable even when we have added brokerage fees. ","SELECTED":null}
{"Title":"Prediction and portfolio optimization in quantitative trading using machine learning techniques","keywords (cleaned)":"quantitative trading;Investments;Stock predictions;Linear regression models;Commerce;Support vector regression models;Trading strategies;Artificial intelligence;Forecasting;Stock predictions;Automated trading systems;Learning systems;Regression analysis;Electronic trading;Portfolio optimization;Learning algorithms;Machine learning;Trading strategies;Machine learning techniques","Abstract":"Quantitative trading is an automated trading system in which the trading strategies and decisions are conducted by a set of mathematical models. Quantitative trading applies a wide range of computational approaches such as statistics, physics, or machine learning to analyze, predict, and take advantage of big data in finance for investment. This work studies core components of a quantitative trading system. Machine learning offers a number of important advantages over traditional algorithmic trading. With machine learning, multiple trading strategies are implemented consistently and able to adapt to real-time market. To demonstrate how machine learning techniques can meet quantitative trading, linear regression and support vector regression models are used to predict stock movement. In addition, multiple optimization techniques are used to optimize the return and control risk in trading. One common characteristic for both prediction models is they effectively performed in short-term prediction with high accuracy and return. However, in short-term prediction, the linear regression model is outperform compared to the support vector regression model. The prediction accuracy is considerably improved by adding technical indicators to dataset rather than adjusted price and volume. Despite the gap between prediction modeling and actual trading, the proposed trading strategy achieved a higher return than the S&P 500 ETF-SPY. ","SELECTED":null}
{"Title":"Forecasting volatility trend of INR USD currency pair with deep learning LSTM techniques","keywords (cleaned)":"LSTM;Chemical detection;Volatility forecasting;Innovative approaches;Commerce;Learning techniques;Forecasting;Forecasting problems;Volatility forecasting;Finance;Volatility forecasting;Electronic trading;Volatility;Learning algorithms;Long short-term memory;Decision trees;Deep learning","Abstract":"Volatility is an important and most discussed topic in finance. Many of financial trades and applications are based on the volatility. In the recent times currency pair conversion trades are new found interest among financial traders due to high instability in the financial market. The advancement in technologies, increased computing speed and capability to handle large data has given rise to deep learning techniques. In this paper, deep learning LSTM techniques have been used to solve volatility forecasting problem of INR USD currency pair. The research uses an innovative approach in arrangement of data to make use of recent 25 values for forecasting volatility trend. The algorithm forecasts uptrend or downtrend movement of volatility a day ahead. The experiments were conducted to forecast volatility using machine learning and deep learning techniques. The LSTM technique is experimented with several epochs and configurations to yield better accuracy. The results show that LSTM techniques produced better accuracy compared with neural networks, SVM, random forest, regression, decision trees and boosting techniques. One of the main application of this research paper is forecasting the rise and fall of INR versus USD. The approach can also be applied to forecasting problems in algorithmic trading, churn predictions, Lead optimization and Fraud detections. ","SELECTED":null}
{"Title":"BV-VPIN: Measuring the impact of order flow toxicity and liquidity on international equity markets","keywords (cleaned)":"Bulk volume classification;International equities;Liquidity;Order flow toxicity;VPIN","Abstract":"Order flow toxicity is the measure of a trader\u2019s exposure to the risk that counterparties possess private information or other informational advantages. High levels of order flow toxicity can culminate in market makers providing liquidity at a loss or in the suboptimal execution of trades. From a regulatory perspective, high levels of toxicity can be harmful to overall market liquidity and precede precipitous drops in asset prices. The bulk volume-volume-synchronized probability of informed trading (BV-VPIN) model is one way of measuring the \u201ctoxicity\u201d component of order flow, and it has been successfully applied in high-frequency trading environments. We apply the BV-VPIN to daily data from a range of international indexes in order to extend previous analyses of its properties.We find that a rise in BV-VPIN effectively foreshadows high levels of volatility in the equity indexes of several countries. If a BV-VPIN futures contract were to exist, we show that it would exhibit safe haven characteristics during market downturns. In particular, a simple active portfolio management strategy that times investments in equities (risk-free assets) when BV-VPIN levels are low (high) outperforms a buy-and-hold strategy. Thus, we find support for the application of BV-VPIN in international equity markets as a risk monitoring and management tool for portfolio managers and regulators. ","SELECTED":null}
{"Title":"Forecasting Price Movements in Betting Exchanges Using Cartesian Genetic Programming and ANN","keywords (cleaned)":"Algorithmic trading;Betting exchange;Financial series forecasting","Abstract":"Since the introduction of betting exchanges in 2000, there has been increased interest of ways to monetize on the new technology. Betting exchange markets are fairly similar to the financial markets in terms of their operation. Due to the lower market share and newer technology, there are very few tools available for automated trading for betting exchanges. The in-depth analysis of features available in commercial software demonstrates that there is no commercial software that natively supports machine learned strategy development. Furthermore, previously published academic software products are not publicly obtainable. Hence, this work concentrates on developing a full-stack solution from data capture, back-testing to automated Strategy Agent development for betting exchanges. Moreover, work also explores ways to forecast price movements within betting exchange using new machine learned trading strategies based on Artificial Neuron Networks (ANN) and Cartesian Genetic Programming (CGP). Automatically generated strategies can then be deployed on a server and require no human interaction. Data explored in this work were captured from 1st of January 2016 to 17th of May 2016 for all GB WIN Horse Racing markets (total of 204 GB of data processing). Best found Strategy agent shows promising 83% Return on Investment (ROI) during simulated historical validation period of one month (15th of April 2016 to 16th of May 2016). ","SELECTED":null}
{"Title":"High-frequency trading strategies based on deep learning algorithms and their profitability","keywords (cleaned)":"Convolutional neural network;Deep learning;High-frequency trading;LSTM neural network;Quantitative investment","Abstract":"As an important algorithm, deep learning has been applied successfully io image processing, speech recognition, machine translation and other fields. Here, deep learning algorithms were applied to high-frequency trading. Convolutional neural network (CNN) and long short-term memory (LSTM) neural network were selected io build up and down classification models, respectively. Based on the models, high-frequency trading strategies were proposed. Then the data of bitumen futures contract was used for back-testing and empirically analyzing the superiority of the strategies. In back-testing, deep learning algorithms were compared with artificial neural network (ANN). The results show that both strategies based on CNN and LSTM neural network exhibit better profitability and generalization ability. In addition, the winning rates and expected returns of the two strategies are also better. ","SELECTED":null}
{"Title":"A scalable circular pipeline design for multi-way stream joins in hardware","keywords (cleaned)":"Parallel algorithms;Query optimization;Computer hardware description languages;Hardware acceleration;Real-time analytics;FPGA;Targeted advertising;Algorithmic trading;Stream Joins;Pipelines;Field programmable gate arrays (FPGA);Pipeline processing systems;Intermediate results;Management applications;Information management;Heterogeneous hardware;Hardware","Abstract":"Efficient real-Time analytics are an integral part of a growing number of data management applications such as computational targeted advertising, algorithmic trading, and Internet of Things. In this paper, we primarily focus on accelerating stream joins, arguably one of the most commonly used and resource-intensive operators in stream processing. We propose a scalable circular pipeline design (Circular-MJ) in hardware to orchestrate multi-way join while minimizing data flow disruption. In this circular design, each new tuple (given its origin stream) starts its processing from a specific join core and passes through all respective join cores in a pipeline sequence to produce final results. We further present a novel two-stage pipeline stream join (Stashed-MJ) that uses a best-effort buffering technique (stash) to maintain intermediate results. In a case that an overwrite is detected in the stash, our design automatically resorts to recomputing intermediate results. Our experimental results demonstrate a linear throughput scaling with respect to the number of execution units in hardware. ","SELECTED":null}
{"Title":"Analysis of Stock Market Behaviour by Applying Chaos Theory","keywords (cleaned)":"Financial markets;Stock Market Movements;Fractals;Behaviour models;Commerce;Pattern mining;Chaos theory;Behaviour models;Algorithmic trading;Pattern mining;Learning systems;Chaos theory;Market behaviours;Electronic trading;Price index;Normalisation;Computation theory;Data mining","Abstract":"Today, the results obtained from machine learning represent a substantial increase in returns over existing algorithmic trading engines. What we are proposing is an approach that is modeled around Chaos Theory. As we know, it is the study of surprises, of randomness. We aim to find occurrences of fractal data in huge chunks of stock market behaviour thereby effectively looking to have a better understanding of a certain stock's unpredictable behaviour. The existing approaches aim at maximising the gains and minimising the losses only, which does not enable us to comprehend the movement of the price index. This paper puts forth a novel idea that makes an effort in just that direction. We use normalisation-denormalisation and pattern mining to achieve our purpose. ","SELECTED":null}
{"Title":"Designing Financial Strategies based on Artificial Neural Networks Ensembles for Stock Markets","keywords (cleaned)":"Financial markets;Electronic trading;Economy sectors;artificial neural network;Technical indicator;Market trends;Commerce;Investments;Algorithmic trading;Stock trading;Neural networks;Learning systems;Financial strategies;Financial Computing;Data Science;Machine learning;Stock market","Abstract":"Before the advent of computers and Internet, the stock market investors perform their operations based mainly on intuition. With the growth of investments and online stock trading, a continued search for better tools to improve the prediction of stock market trends has become necessary in order to increase profits and reduce risks. In this work we propose and evaluate some algorithmic trading (algotrading) strategies, based on an Ensemble of artificial neural networks (ANN), to support the decision of stock market's investors. Thirty different ANN models, using different input sets of price, volume and technical indicators, were analyzed for different stock symbols (i.e., companies from different economy sectors) of the main Brazilian Stock Market-BM& FBovespa. Moreover, ensembles that combine the best ANN models were modeled and validated through different experiments. The results confirm the potential of the proposed strategies for algotrading. ","SELECTED":null}
{"Title":"Optical TEMPEST","keywords (cleaned)":"Electronic trading;Internet of things;Population statistics;Product design;Security of data;Supply chains;Financial sectors;General data protection regulations;General population;High-frequency trading;Industrial settings;Internet of thing (IOT);Recent trends;Electromagnetic compatibility","Abstract":"Research on optical TEMPEST has moved forward since 2002 when the first pair of papers on the subject emerged independently and from widely separated locations in the world within a week of each other. Since that time, vulnerabilities have evolved along with systems, and several new threat vectors have consequently appeared. Although the supply chain ecosystem of Ethernet has reduced the vulnerability of billions of devices through use of standardised PHY solutions, other recent trends including the Internet of Things (IoT) in both industrial settings and the general population, High Frequency Trading (HFT) in the financial sector, the European General Data Protection Regulation (GDPR), and inexpensive drones have made it relevant again for consideration in the design of new products for privacy. One of the general principles of security is that vulnerabilities, once fixed, sometimes do not stay that way. ","SELECTED":null}
{"Title":"Big Data Automatic System of Analysis and Trading on Financial Markets","keywords (cleaned)":"International currencies;Financial markets;Automated trading systems;Investments;Decision support systems;Data flow;Commerce;artificial neural network;Decision supports;Logistic regressions;Big data;Neural networks;Life cycle;Automated trading systems;Automatic systems;Information products;Regression analysis;Electronic trading;international currency markets;Logistic regressions","Abstract":"The paper considers the main tendencies of the international currency markets and opportunities for individual investments. The methods of decision support of assets purchasing were provided. A mechanism of intellectual trade with the use of a significant data flow is proposed. Open and closed types of information products were combined to automate the system. Guided by the effectiveness of trade signals assessment, recommendations were made for the use of methods of artificial neural networks or logistic regressions. A set of criteria has been formed to support life cycle of the automated trading system. ","SELECTED":null}
{"Title":"MODELING INTERNATIONAL STOCK PRICE COMOVEMENTS with HIGH-FREQUENCY DATA","keywords (cleaned)":"ADCC-GARCH;HFD;MES;Price Comovements;Systemic Risk","Abstract":"This paper studies stock price comovements in two key regions [the United States and Europe, which is represented by three major European developed countries (France, Germany, and the United Kingdom)]. Our paper uses recent high-frequency data (HFD) and investigates price comovements in the context of normal times and crisis periods. To this end, we applied a non-Gaussian Asymmetrical Dynamic Conditional Correlation (ADCC)-GARCH (Generalized Autoregressive Conditional Heteroscedasticity) model and the Marginal Expected Shortfall (MES) approach. This choice has three advantages: (i) With the development of high-frequency trading (HFT), it is more appropriate to use HFD to test price linkages for overlapping and nonoverlapping data. (ii) The ADCC-GARCH model captures further asymmetry in price comovements. (iii) The use of the MES enables to measure systemic risk contributions around the distribution tails. Accordingly, we offer two interesting findings. First, while the hypothesis of asymmetrical and time-varying stock return linkages is not rejected, the MES approach indicates that both European and US indices make a considerable contribution to each other's systemic risk, with significant input from Frankfurt to the French and US markets, especially following the collapse of Lehman Brothers. Second, we show that the propagation of systemic risk is higher during the crisis period and overlapping trading hours than during nonoverlapping hours. Thus, the MES test is recommended as an indicator to help monitor market exposure to systemic risk and to gauge expected losses for other markets. Copyright ","SELECTED":null}
{"Title":"Modeling calendar spread options","keywords (cleaned)":"Calendar spreads;Corn;Futures;Options;Panel unit root tests","Abstract":"Purpose: The purpose of this paper is to derive a new option pricing model for options on futures calendar spreads. Calendar spread option volume has been low and a more precise model to price them could lead to lower bid-ask spreads as well as more accurate marking to market of open positions. Design\/methodology\/approach: The new option pricing model is a two-factor model with the futures price and the convenience yield as the two factors. The key assumption is that convenience follows arithmetic Brownian motion. The new model and alternative models are tested using corn futures prices. The testing considers both the accuracy of distributional assumptions and the accuracy of the models\u2019 predictions of historical payoffs. Findings: Panel unit root tests fail to reject the unit root null hypothesis for historical calendar spreads and thus they support the assumption of convenience yield following arithmetic Brownian motion. Option payoffs are estimated with five different models and the relative performance of the models is determined using bias and root mean squared error. The new model outperforms the four other models; most of the other models overestimate actual payoffs. Research limitations\/implications: The model is parameterized using historical data due to data limitations although future research could consider implied parameters. The model assumes that storage costs are constant and so it cannot separate between negative convenience yield and mismeasured storage costs. Practical implications: The over 30-year search for a calendar spread pricing model has not produced a satisfactory model. Current models that do not assume cointegration will overprice calendar spread options. The model used by the Chicago Mercantile Exchange for marking to market of open positions is shown to work poorly. The model proposed here could be used as a basis for automated trading on calendar spread options as well as marking to market of open positions. Originality\/value: The model is new. The empirical work supports both the model\u2019s assumptions and its predictions as being more accurate than competing models. ","SELECTED":null}
{"Title":"Relative spread and price discovery","keywords (cleaned)":"High-frequency trading;Market design;Market microstructure","Abstract":"We establish the importance of relative minimum price increments for price discovery in the context of a single asset trading at diverse venues. Our model relates relative spreads to directed information flows and begets a set of testable implications. Although conventional wisdom dictates that futures prices lead equities, our model predicts the opposite should be true when particular relative price conditions hold for the bids and offers of each asset. We develop an empirical measure of price discovery which is suited to asynchronous, high-frequency financial data and test the model predictions. Empirical evidence strongly supports the relative spread mechanism. ","SELECTED":null}
{"Title":"The Control Strategies for High Frequency Algorithmic Trading","keywords (cleaned)":"Air traffic control;Financial markets;Electronic trading;High frequency HF;Automated methods;Fundamental characteristics;Algorithmic trading;Commerce;Electronic document exchange;Algorithmic trading;control strategy;Deep learning;Security and reliabilities;Automated control;High frequency trading;control strategy;finanical market;High-frequency trading;Systems engineering","Abstract":"High Frequency Algorithmic Trading has developed rapidly in recent years as the disruptive information technology such as deep learning emerges. So, do the public concerns about the security and reliability of the algorithmic trading in stock markets as well. Taking advantage of the highly fluctuated trading data, the automated methods of trading have evolved accordingly. This article first reviews the existing literature on algorithmic trading, electronic trading, and high frequency trading. The most important underlying control strategy and mechanism of high frequency algorithmic trading is named 'market maker' in stock market. Accordingly, we analyze the fundamental characteristics and functions of market maker in trading and illustrate several control strategies in high frequency algorithmic trading. Our work open the door for the design of sophisticated automated control strategies for financial markets, which are fundamentally different from those for traditional physical systems such as air traffic control system. ","SELECTED":null}
{"Title":"CMBS market efficiency: The crisis and the recovery","keywords (cleaned)":"CMBS;CMBX;Credit crisis;Financial crisis of 2008;Market efficiency;Recovery","Abstract":"This paper presents a reduced form credit risk model to study CMBS pricing and CMBS market efficiency during and after the credit crisis with a comprehensive loan, bond and deal level data set. Using a model determined fair value, an automated trading strategy based on a newly determined risk ratio buys undervalued and sells overvalued CMBS. These strategies result in substantial trading profits between November 2007 and June 2015. Controlling for CMBS sector risk factors, we reject CMBS market efficiency over the entire sample period. When we split the sample into the Crisis and Recovery periods, we observe persistent abnormal returns over both subperiods, which is consistent with an inefficient CMBS market. Because the CMBS market appears to be inefficient, our results suggest that the approach presented in this paper may facilitate the increased financial stability of the CRE sector through the better pricing and risk management of CMBS. ","SELECTED":null}
{"Title":"Impact and recovery process of mini flash crashes: An empirical study","keywords (cleaned)":"article;empiricism;Marketing;Algorithms;commercial phenomena;Models, Economic;empirical research;Humans;Investments;Algorithms;Commerce;empirical research;Humans;Investments;Models, Economic","Abstract":"In an Ultrafast Extreme Event (or Mini Flash Crash), the price of a traded stock increases or decreases strongly within milliseconds. We present a detailed study of Ultrafast Extreme Events in stock market data. In contrast to popular belief, our analysis suggests that most of the Ultrafast Extreme Events are not necessarily due to feedbacks in High Frequency Trading: In at least 60 percent of the observed Ultrafast Extreme Events, the largest fraction of the price change is due to a single market order. In times of financial crisis, large market orders are more likely which leads to a significant increase of Ultrafast Extreme Events occurrences. Furthermore, we analyze the 100 trades following each Ultrafast Extreme Events. While we observe a tendency of the prices to partially recover, less than 40 percent recover completely. On the other hand we find 25 percent of the Ultrafast Extreme Events to be almost recovered after only one trade which differs from the usually found price impact of market orders. ","SELECTED":null}
{"Title":"Multivariate statistical model based currency market profitability binary classifier","keywords (cleaned)":"Profitability;Data analysis;Financial markets;Data reduction;Pattern recognition;Currency markets;Predictive models;Multivariate statistical modeling;Commerce;Predictive analytic;Predictive analytic;Algorithmic trading;Artificial intelligence;Binary classifiers;Multivariant analysis;Electronic trading;Stability indicators;Binary classification","Abstract":"In this paper, we carried out a detailed statistical predictive model to estimate the financial market stability based on historical data. The given stability indicator allows the trader to make the decision about which part of the day a financial instrument is profitable but not risky. This model is implemented as external indicator that can be integrated in the MetaTrader platform and uses historical and real time financial data coming, by streaming, from the trading broker. Using this custom indicator, the trader will save the lost time that he\/she spent in front of screens waiting for a valid signal to trade, and avoid the high volatility periods of the day during which the market is unpredictable and risky. The algorithm can be also implemented in algorithmic trading robot as a trigger to turn it on at the best and right moments. ","SELECTED":null}
{"Title":"Complex correlation approach for high frequency financial data","keywords (cleaned)":"financial networks;network reconstruction;quantitative finance","Abstract":"We propose a novel approach that allows the calculation of a Hilbert transform based complex correlation for unevenly spaced data. This method is especially suitable for high frequency trading data, which are of a particular interest in finance. Its most important feature is the ability to take into account lead-lag relations on different scales, without knowing them in advance. We also present results obtained with this approach while working on Tokyo Stock Exchange intraday quotations. We show that individual sectors and subsectors tend to form important market components which may follow each other with small but significant delays. These components may be recognized by analysing eigenvectors of complex correlation matrix for Nikkei 225 stocks. Interestingly, sectorial components are also found in eigenvectors corresponding to the bulk eigenvalues, traditionally treated as noise. ","SELECTED":null}
{"Title":"Market microstructure in practice: Second edition","keywords (cleaned)":"market fragmentation|High Frequency|entropy|students|nse|trading algorithms|r|information|Trade|electronic trading|market microstructure|market design|models|Trading|intraday|Market microstructure|algorithms|fix|microstructure|expo|R|measurements|Dark Pools|trading|Order Routing|fragmentation|high frequency|market|design|market impact|Market|price impact|financial markets|data analysis|framework|liquidity|algorithm|infrastructure|dynamics|trade scheduling|prices|edi|Flash Crash|high frequency trading|profit|trade|Equity Trading|Policy makers|financial market","Abstract":"This book exposes and comments on the consequences of Reg NMS and MiFID on market microstructure. It covers changes in market design, electronic trading, and investor and trader behaviors. The emergence of high frequency trading and critical events like the\"Flash Crash\" of 2010 are also analyzed in depth. Using a quantitative viewpoint, this book explains how an attrition of liquidity and regulatory changes can impact the whole microstructure of financial markets. A mathematical Appendix details the quantitative tools and indicators used through the book, allowing the reader to go further independently. This book is written by practitioners and theoretical experts and covers practical aspects (like the optimal infrastructure needed to trade electronically in modern markets) and abstract analyses (like the use on entropy measurements to understand the progress of market fragmentation). As market microstructure is a recent academic field, students will benefit from the book's overview of the current state of microstructure and will use the Appendix to understand important methodologies. Policy makers and regulators will use this book to access theoretical analyses on real cases. For readers who are practitioners, this book delivers data analysis and basic processes like the designs of Smart Order Routing and trade scheduling algorithms. In this second edition, the authors have added a large section on orderbook dynamics, showing how liquidity can predict future price moves, and how High Frequency Traders can profit from it. The section on market impact has also been updated to show how buying or selling pressure moves prices not only for a few hours, but even for days, and how prices relax (or not) after a period of intense pressure. Further, this edition includes pages on Dark Pools, Circuit Breakers and added information outside of Equity Trading, because MiFID 2 is likely to push fixed income markets towards more electronification. The authors explore what is to be expected from this change in microstructure. The appendix has also been augmented to include the propagator models (for intraday price impact), a simple version of Kyle's model (1985) for daily market impact, and a more sophisticated optimal trading framework, to support the design of trading algorithms. ","SELECTED":null}
{"Title":"Using tensorflow to solve the problems of financial forecasting for high-frequency trading","keywords (cleaned)":"Financial data;Financial markets;Distributed computer systems;Financial forecasting;Futures contract;TensorFlow;Commerce;Market forecasting;Financial market forecasting;Artificial intelligence;Quality indicators;Silicon compounds;Recurrent neural networks;Electronic trading;Artificial intelligence;Recurrent neural network (RNNs);High-frequency trading","Abstract":"The use of neural networks significantly expands the possibilities of analyzing financial data and improves the quality indicators of the financial market. In article we examine various aspects of working with neural networks and Frame work TensorFlow, such as choosing the type of neural networks, preparing data and analyzing the results. The work was carried out on the real data of the financial instrument Si-6.16 (futures contract on the US dollar rate). ","SELECTED":null}
{"Title":"BSE: A minimal simulation of a limit-order-book stock exchange","keywords (cleaned)":"Program code;Financial markets;University levels;Stock exchange;Hands-on learning;Commerce;Automation;Correlation theory;Automated trading systems;Open systems;Automated trading;Simulation for education;Limit order book;Teaching and researches","Abstract":"This paper describes the design, implementation, and successful use of the Bristol Stock Exchange (BSE) a novel minimal simulation of a centralized financial market, based on a Limit Order Book (LOB) such as is commonly in major stock exchanges. Construction of BSE was motivated by the fact that most of the world's major financial markets have automated, with trading activity that previously was the responsibility of human traders now being performed by high-speed autonomous automated trading systems. Research aimed at understanding the dynamics of this new style of financial market is hampered by the fact that no operational real-world financial exchange is ever likely to allow experimental probing of that market while it is open and running live, forcing researchers to work primarily from time-series of past trading data. Similarly, university-level education of the engineers who can create next-generation automated-trading systems requires that they have hands-on learning experiences in a sufficiently realistic teaching environment. BSE as described here addresses both needs: it has been successfully used for teaching and research in a leading UK university since 2012, and the BSE program code is freely available as open-source on GitHub. ","SELECTED":null}
{"Title":"Automatic High-Frequency Trading: An Application to Emerging Chilean Stock Market","keywords (cleaned)":"Commerce;Financial markets;Particle swarm optimization (PSO);Profitability;Computer tools;Effective solution;High-frequency trading;Net return;NP Complete;Optimization algorithms;Real markets;Electronic trading","Abstract":"This research seeks to design, implement, and test a fully automatic high-frequency trading system that operates on the Chilean stock market, so that it is able to generate positive net returns over time. A system that implements high-frequency trading (HFT) is presented through advanced computer tools as an NP-Complete type problem in which it is necessary to optimize the profitability of stock purchase and sale operations. The research performs individual tests of the algorithms implemented, reviewing the theoretical net return (profitability) that can be applied on the last day, month, and semester of real market data. Finally, the research determines which of the variants of the implemented system performs best, using the net returns as a basis for comparison. The use of particle swarm optimization as an optimization algorithm is shown to be an effective solution since it is able to optimize a set of disparate variables but is bounded to a specific domain, resulting in substantial improvement in the final solution. ","SELECTED":null}
{"Title":"Theories for analysing innovation and technology in emerging financial markets: The case of algorithmic and high frequency trading","keywords (cleaned)":"Bounded rationality;Rational choice theory;Financial markets;High-frequency trading;Information use;Social constructionism;Commerce;Engineering research;Financial institution;Innovation and technology;Information management;Information systems;High frequency trading;Electronic trading;Disciplinary perspective;Regulation;Innovation;Technology","Abstract":"Theoretical and empirical analysis of complex innovation and technology from a multi-disciplinary perspective is often recommended, yet rarely undertaken in management and information systems scholarship. This paper examines high frequency trading as an innovation in financial markets enabled by sophisticated computer technology. Primary data from the US and UK is combined with secondary sources from academic studies, government agencies, financial institutions and the media on the ideologies and practices governing high frequency trading. Theoretical perspectives of rational choice theory, bounded rationality, social constructionism and Marxism are rooted in different ontological and empirical positions. Each theory prioritises distinct levels and units of analysis for HFT which, by extension, shape the research agendas for examining the regulatory processes, market structures, firm strategies and technologies of the HFT phenomenon. ","SELECTED":null}
{"Title":"High frequency trading strategies, market fragility and price spikes: an agent based model perspective","keywords (cleaned)":"Agent-based model;Algorithmic trading;Limit order book;MiFID II;Stylised facts","Abstract":"Given recent requirements for ensuring the robustness of algorithmic trading strategies laid out in the Markets in Financial Instruments Directive II, this paper proposes a novel agent-based simulation for exploring algorithmic trading strategies. Five different types of agents are present in the market. The statistical properties of the simulated market are compared with equity market depth data from the Chi-X exchange and found to be significantly similar. The model is able to reproduce a number of stylised market properties including: clustered volatility, autocorrelation of returns, long memory in order flow, concave price impact and the presence of extreme price events. The results are found to be insensitive to reasonable parameter variations. ","SELECTED":null}
{"Title":"A hybrid alpha-stable model development for high frequency trading markets [Formulaci\u00d3n de un modelo h\u00cdbrido alfa-estable para mercados con operaci\u00d3n de alta frecuencia]","keywords (cleaned)":"?-stable processes;Big data;High frequency markets;Multilayer Perceptron;Non-stationary compound Poisson processes","Abstract":"Business activities require to obtain, organize and manage information from large amounts of data. In hedge funds, short selling trade and derivatives valuation, agents change their strategies to improve profits, and therefore to increase their possibilities to remain in the market, as a result of finding more accurate methods to process ever larger volume of information, considering that the information is not evenly distributed among markets participants. In this paper, a hybrid three stage model is formulated consisting of: a high frequency market model through a non-stationary Compound Poisson Process, a multilayer perceptron trained by backpropagation and, finally, estimators based on alpha-stable distributions, as an initial overview to develop a high frequency trading market operating system. ","SELECTED":null}
{"Title":"CPU and GPU implementations for high frequency trading in algorithmic finance","keywords (cleaned)":"Parallel processing systems;Electronic trading;Graphics processing unit;Commerce;Parallel computing;Matlab- software;Algorithmic trading;Statistical arbitrage;Trading strategies;Computational speed;High frequency trading;MATLAB;Program processors;GPU;Application programs;High performance computing;GPU implementation;High-frequency trading","Abstract":"Today algorithmic trading and High Frequency Trading (HFT) account for a dominant part of overall trading volume in financial markets. The trade execution time has grown from daily trading to microseconds and nanoseconds.. A modern GPU allows hundreds of operations to be performed in parallel, leaving the CPU free to execute other jobs. The main objective of this research was to test the possibility and quantify how much higher speedups the use of GPUs can bring in calculations of HFT statistical arbitrage algorithms. In the research MATLAB software was applied for GPU application and computations. The statistical arbitrage- pair trading algorithm was parallelized in order to adapt it to GPU application. The effectiveness was measured according to time CPU and GPU did spent working on historical data using pair trading strategy. In the paper the final results of the research are presented and discussed. The results have proven up to 30% increase in computational speed with the application of statistical arbitrage algorithm in HFT. Copyright held by the author(s).","SELECTED":null}
{"Title":"Extreme Market Prediction for Trading Signal with Deep Recurrent Neural Network","keywords (cleaned)":"Random forests;High frequency HF;Time series;Deep neural networks;Commerce;Logistic regressions;Network architecture;Prediction markets;Forecasting;Recurrent neural networks;High frequency trading;Financial time series;Electronic trading;Motion estimation;Movement prediction;Deep architectures;Decision trees;Deep learning;High-frequency trading;Decision making","Abstract":"Recurrent neural network are a type of deep learning units that are well studied to extract features from sequential samples. They have been extensively applied in forecasting univariate financial time series, however their application to high frequency multivariate sequences has been merely considered. This paper solves a classification problem in which recurrent units are extended to deep architecture to extract features from multi-variance market data in 1-minutes frequency and extreme market are subsequently predicted for trading signals. Our results demonstrate the abilities of deep recurrent architecture to capture the relationship between the historical behavior and future movement of high frequency samples. The deep RNN is compared with other models, including SVM, random forest, logistic regression, using CSI300 1-minutes data over the test period. The result demonstrates that the capability of deep RNN generating trading signal based on extreme movement prediction support more efficient market decision making and enhance the profitability. ","SELECTED":null}
{"Title":"Direct multiperiod forecasting for algorithmic trading","keywords (cleaned)":"lag length selection;lag length selection;High frequency HF;Multi-step forecasting;Non-linear transformations;Commerce;Algorithmic trading;Forecasting;volume weight average price;volume weight average price;Volume-weighted averages;Electronic trading;intraday forecasting;Mathematical transformations;direct multistep forecasting;Trading strategies","Abstract":"This paper examines the performance of iterated and direct forecasts for the number of shares traded in high-frequency intraday data. Constructing direct forecasts in the context of formulating volume weighted average price trading strategies requires the generation of a sequence of multistep-ahead forecasts. I discuss nonlinear transformations to ensure nonnegative forecasts and lag length selection for generating a sequence of direct forecasts. In contrast to the literature based on low-frequency macroeconomic data, I find that direct multiperiod forecasts can outperform iterated forecasts when the conditioning information set is dynamically updated in real time. Copyright ","SELECTED":null}
{"Title":"Gaussian process kernel crossover for automated forex trading system","keywords (cleaned)":"Gaussian noise (electronic);Forex;Financial data processing;Crossover strategies;Exponential moving averages;Commerce;Gaussian Processes;Gaussian distribution;Traditional technical analysis;Gaussian Process Kernel;Time series analysis;Objective functions;Financial time series;Automated trading","Abstract":"Due to time varying volatility of forex financial time series price data, the traditional technical analysis such as simple moving average (SMA) and exponential moving average (EMA) cannot capture the dynamic time varying in the market. The combination of Gaussian Process kernel framework and classical trendline crossover strategy is proposed with new objective function for optimization trading performance. The experimental results demonstrate better trading performance for both price prediction and accumulated profit return. ","SELECTED":null}
{"Title":"Modeling the High-Frequency FX Market: An Agent-Based Approach","keywords (cleaned)":"Agent-based simulation;Intelligent agents;Electronic trading;Agent based simulation;Software agents;Systematic exploration;Agent-based model;Stylized facts;Electronic market;Electronic market;Commerce;FX markets;Statistical pattern;Autonomous agents;Economics;Electronic commerce;Computational methods;Constituent elements;Agent-based model;Stylized facts;Agent-based approach","Abstract":"The development of computational intelligence-based strategies for electronic markets has been the focus of intense research. To be able to design efficient and effective automated trading strategies, one first needs to understand the workings of the market, the strategies that traders use, and their interactions as well as the patterns emerging as a result of these interactions. In this article, we develop an agent-based model of the foreign exchange (FX) market, which is the market for the buying and selling of currencies. Our agent-based model of the FX market comprises heterogeneous trading agents that employ a strategy that identifies and responds to periodic patterns in the price time series. We use the agent-based model of the FX market to undertake a systematic exploration of its constituent elements and their impact on the stylized facts (statistical patterns) of transactions data. This enables us to identify a set of sufficient conditions that result in the emergence of the stylized facts similarly to the real market data, and formulate a model that closely approximates the stylized facts. We use a unique high-frequency data set of historical transactions data that enables us to run multiple simulation runs and validate our approach and draw comparisons and conclusions for each market setting. ","SELECTED":null}
{"Title":"A multimodal asymmetric exponential power distribution: Application to risk measurement for financial high-frequency data","keywords (cleaned)":"Financial markets;Signal processing;Asymmetric distributions;Risk measures;Maximum likelihood;Expected shortfall;Stochastic systems;Value-at-risk;Value at Risk;Risk perception;Asymmetric distributions;Risk assessment;Multimodality;Maximum likelihood estimation;Electronic trading;Multimodality;Value engineering","Abstract":"Interest in risk measurement for high-frequency data has increased since the volume of high-frequency trading stepped up over the two last decades. This paper proposes a multimodal extension of the Exponential Power Distribution (EPD), called the Multimodal Asymmetric Exponential Power Distribution (MAEPD). We derive moments and we propose a convenient stochastic representation of the MAEPD. We establish consistency, asymptotic normality and efficiency of the maximum likelihood estimators (MLE). An application to risk measurement for high-frequency data is presented. An autoregressive moving average multiplicative component generalized autoregressive conditional heteroskedastic (ARMA-mcsGARCH) model is fitted to Financial Times Stock Exchange (FTSE) 100 intraday returns. Performances for Value-at-Risk (VaR) and Expected Shortfall (ES) estimation are evaluated. We show that the MAEPD outperforms commonly used distributions in risk measurement. ","SELECTED":null}
{"Title":"BigData and regulation in high frequency trading","keywords (cleaned)":"Financial data;Financial markets;Intelligent robots;Proposed architectures;Autoregulations;Commerce;Knowledge based systems;Artificial intelligence;Big data;Knowledge base;High frequency trading;Maturity levels;Multiple markets;Electronic trading;Regulation;Robot trader;High-frequency trading;Decision making","Abstract":"In this article, we are defining a proposed architecture that can resolve the main problem of high frequency trading which is the maturity level of financial data used by robot trader. The volume of markets data which comes every fragment of second, has caused a fatal problem in decision making on market. Any engendered market anomaly of robot trader can be propagated on the multiple markets due to the interaction between them and can be the cause of several problems in financial markets. The concept of our solution is based on endowing artificial intelligence of robot trader with auto regulation capacity following financial knowledge base.. ","SELECTED":null}
{"Title":"A Highly Efficient Quantitative Transaction Data Mining Model","keywords (cleaned)":"Probability generating functions;Stationary waiting time;Discrete-time geo\/g\/1 queues;Financial markets;Bernoulli gated service;Commerce;Data mining;Queueing theory;Automated trading;Geo\/G\/1 queue;Electronic trading;Money flow;Trading strategies;Digital devices","Abstract":"This paper focused on discrete time Geo\/G\/1 queue with Bernoulli gated service. We proposed a new algorithm to calculate stock's money flow by the probability generating function (P.G.F.) of stationary waiting time and stationary queue length. We did program by data mining of a variety of stock trading. The new algorithm could get any stock's money flow. We have established a series of quantitative trading strategies based on the new money flow model. These strategies could guide investors to grasp an upward trend or avoid a down trend. The quantitative investment trading strategies can be automated trading. It could reduce investors' subjective operability and increase return rate. The new algorithm was suitable for index trading, futures trading, stock trading, and ETF trading. ","SELECTED":null}
{"Title":"A parallel fuzzy rule discovery algorithm and future goods automated trading system","keywords (cleaned)":"Big data;Future goods data;Fuzzy rule based classification system;MapReduce","Abstract":"Nowadays, with the rapid development of information technology, many business fields gradually usher in the era of big data. Extracting valuable information from the large data is very urgent. However, because of some limitations such as memory restrictions, data complexity or time complexity, the majority of traditional data mining methods are time consuming and low efficiency working on big data. In contrast, parallel computing is a common and reliable choice to solve that problem. In this paper, we propose a fuzzy rule discovery algorithm - fuzzy rule based classification system in the framework of MapReduce (FRBCS-MR). The proposed FRBCS-MR algorithm applies parallel computing technology to extract fuzzy rules and build fuzzy rule-base, which combines the advantage of dealing with uncertainty of fuzzy system and the ability of MapReduce in parallel computing. Future goods trading has been gradually transformed from the original manual trading to stylized trading and the stylized trading is considered to be the forefront and the most scientific mode of investment. In the experimental studies, the rules extracted by FRBCS-MR algorithm using future goods data are turned into trading strategies which are to be used in the real automated trading platform TradeBlazer (TB) to compute the profit and loss situation. The result shows that the FRBCS-MR algorithm has good profit ability and possesses a certain guiding significance for investors. ","SELECTED":null}
{"Title":"Robust Decision Support System for Asset Assessment and Management","keywords (cleaned)":"Data envelopment analysis;Artificial intelligence;Portfolio managements;Managers;Peer evaluations;Electronic trading;High-frequency trading;Portfolio selection;Market volatility;Decision support systems;Cross-efficiency analysis;Classification process;Numerical experiments;uncertainty;Commerce;Finance;Cross efficiency;Efficiency;Investments;data envelopment analysis (DEA);Portfolio selection","Abstract":"We address asset classification and portfolio selection in this paper. Surprisingly, money managers find that the market volatility becomes more frequent as more advanced innovations are applied in the financial system. For example, the high-frequency trading may amplify the deviation on U.S. stock market [1], [2]. Therefore, a reliable method to appraise the asset performance is extremely important to portfolio managers, regulators, and individual investors. One alternative approach to achieve this goal is data envelopment analysis (DEA). Asset performance was ranked from both self- and peer-evaluation perspectives. Specifically, we extended the cross-efficiency analysis in DEA that uses row and column means to portfolio selection and identify different types of asset set. This classification process can help investors to construct a more robust portfolio. Numerical experiments based on S&P500 showed that the portfolio with cross-efficiency analysis can generate better Sharpe ratios during the period of financial crisis in 2008. ","SELECTED":null}
{"Title":"Decision support system for real-time trading based on on-line learning and parallel computing techniques","keywords (cleaned)":"Parallel computing techniques;Artificial intelligence;Algorithmic trading;Adaptive boosting;Computer architecture;Electronic trading;Decision support systems;Parallel computing;Decision support systems;Cloud computing;Financial decision support systems;Learning algorithms;Open Computing Language;Machine learning techniques;Algorithmic languages;Network function virtualization;Parallel computing platform;Commerce;Computer architecture;Finance;Learning systems;joint-AdaBoost;E-learning;Parallel processing systems;Economics;Investment opportunities;Big data;Program processors","Abstract":"A novel intraday algorithmic trading strategy is developed based on various machine learning techniques and paralleled computing architectures in this paper. The proposed binary classification framework can predict the price trends of Taiwan stock index futures after thirty minutes. Traditional learning-based approaches collect all samples during the training period as the learning materials. The major contribution of this paper is to collect a subset of similar historical financial data to train the real-time trading model. This goal can be achieved by an on-line learning technique which is required to calculate an accurate model with training time limitation. In addition, the proposed joint-AdaBoost algorithm is to improve the system performance based on the concept of paired feature learning and planar weak classifier design. The core execution components in this algorithm can be further accelerated with the aid of Open Computing Language (OpenCL) parallel computing platform. The experimental results show that the proposed learning algorithm can improve the prediction accuracy of final classifier from 53.8% to 61.68%. Compared to the pure CPU implementation, the OpenCL version which uses CPU and GPGPU simultaneously can reduce the calculation time around 83.02 times. The efficiency improvement can decrease the delay of investment opportunity which is a critical issue in real-time financial decision support system application. To sum up, this paper proposed a novel learning framework based on joint-AdaBoost algorithm with similar learning samples and OpenCL parallel computation. The extended financial decision support system is also proven to work effectively and efficiently in our simulation experiments to trade the Taiwan stock index futures. ","SELECTED":null}
{"Title":"Minimisation of parameters for optimisation of Algorithmic Trading Systems","keywords (cleaned)":"Commerce;Electric machinery;Electronic trading;Oscillators (electronic);Power electronics;Algorithmic trading system;Market forecasting;Minimisation;Optimisations;electronic equipment","Abstract":"Many existing Algorithmic Trading Systems (ATS) use a large number of parameters to improve the results (i.e. accurate market forecasting and maximisation of profit). However, the use of superfluous parameters can often mask underlying trends leading to result instability and an excessive risk. This paper proposes a method for choosing the minimum number of parameters that are necessary and sufficient for the ATS to demonstrate a more reliable outcome. An ATS system is implemented and tested with the method and is shown to be successful for a number of case data scenarios. ","SELECTED":null}
{"Title":"GeniBux-event based intelligent Forex trading strategy enhancer","keywords (cleaned)":"Forex;Significant patterns;Artificial intelligence;Algorithmic trading;Data handling;Electronic trading;Prediction and decision;Computer programming languages;Sustainable development;Automation;Domain specific languages;Domain specific languages;Intelligent systems;Complex event processing;Commerce;Complex event processing;Machine learning;Learning systems;Profitability;Graph mining;Interactive interfaces;Education;Interoperability;Data mining","Abstract":"The Foreign Currency Exchange market (Forex) is the largest financial market in the world with the highest daily trading volume. A highly volatile complex behavior is seen during the time the market is open and understanding market trend patterns solves an enormous amount of problems pertaining to prediction and decision making. Many often traders write trading strategies to identify the significant patterns they have recognized. This concept directly involves trading automation or algorithmic trading but building such a versatile algorithm is quite challenging unless you have a person to suggest improvements in your own algorithm. Therefore, in this paper we propose an intelligent system called Genibux will assist Forex traders to improve their strategies. The system suggests improvements with justifications for the maximum gain of profit. Genibux is mainly based on Complex Event Processing and it has been implemented with highly comprehensible Genibux Strategy Language (GSL) together with Machine Learning and classifying algorithms enclosed in a set of highly interactive interfaces. Most importantly, it performs exceedingly well and depicts more profit gains through Genibux improved trading strategies. ","SELECTED":null}
{"Title":"Parallel Optimization of Sparse Portfolios with AR-HMMs","keywords (cleaned)":"Financial time series;GPGPU;Hidden Markov models;High frequency algorithmic trading;Mean reversion;Portfolio optimization","Abstract":"In this paper we optimize mean reverting portfolios subject to cardinality constraints. First, the parameters of the corresponding Ornstein\u2013Uhlenbeck (OU) process are estimated by auto-regressive Hidden Markov Models (AR-HMM), in order to capture the underlying characteristics of the financial time series. Portfolio optimization is then performed by maximizing the return achieved with a predefined probability instead of optimizing the predictability parameter, which provides more profitable portfolios. The selection of the optimal portfolio according to the goal function is carried out by stochastic search algorithms. The presented solutions satisfy the cardinality constraint in terms of providing a sparse portfolios which minimize the transaction costs (and, as a result, maximize the interpretability of the results). In order to use the method for high frequency trading (HFT) we utilize a massively parallel GPGPU architecture. Both the portfolio optimization and the model identification algorithms are successfully tailored to be running on GPGPU to meet the challenges of efficient software implementation and fast execution time. The performance of the new method has been extensively tested both on historical daily and intraday FOREX data and on artificially generated data series. The results demonstrate that a good average return can be achieved by the proposed trading algorithm in realistic scenarios. The speed profiling has proven that GPGPU is capable of HFT, achieving high-throughput real-time performance. ","SELECTED":null}
{"Title":"Equities","keywords (cleaned)":"Capital markets;Common stock;Equities;High-frequency trading (HFT);Initial public offering (IPO);Preferred stock;Regulation","Abstract":"This chapter discusses equities, including common stock, preferred shares, depository receipts, and mutual funds. It begins with a discussion of equity market participants and the processes of the primary (i.e., initial public offerings) and secondary markets. An example of the sequence of orders sent to a stock exchange illustrates the mechanics of order-driven markets. This chapter also explores the impact of a number of historical events, including the Crash of 1929 and the introduction of the Regulation National Market System (Reg NMS). It then goes on to explain the modern market structure, including order types, complexity, and market data, and the impact of high-frequency trading (HFT). ","SELECTED":null}
{"Title":"Ultra tight relative timing in finance trading","keywords (cleaned)":"Finance;Optical fibers;Timing circuits;Automated trading;Latency control;Network element;Optical fiber networks;Relative timing;Service Level Agreements;Specific segments;Time synchronization;Commerce","Abstract":"A huge number of news, data and trading actions are generated at a global scale every microsecond. And all this data and trading activity has impact among themselves, thus a tight relative timing between different sites in finance trading is mandatory. In the Finance sector the requirements of different actors may be different: transferring legal time from certified institutions to trading centers, time synchronization at a data center scale, deterministic timing for finely coordinated trading actions at a global scale, etc. Our contribution describes how to distribute deterministically a global notion of time across an optical fiber network to arrive at a tight relative timing (accurate time synchronization among the network elements). Another use case is related to low latency which is a highly demanded feature in specific segments (e.g. reduction in exchange latency improves market quality). Latency is a necessary condition for the implementation of various types of strategies and low latency automated trading is associated with lower quoted and effective spreads, lower volatility and greater liquidity. We describe how we can use latency control mechanisms for evaluating it in the framework of a service level agreement. In summary, actors in the Finance sector are adapting to the new situation, the millisecond (soon nanosecond) framework and we present an ultra-accurate solution for this target. ","SELECTED":null}
{"Title":"Parallel heterogeneous multi-classifier system for decision making in algorithmic trading","keywords (cleaned)":"Genetic algorithms;Parallel environment;Technical analysis;Multiclassifier system;Commerce;Algorithmic trading, trading strategy;Algorithmic trading;Genetic algorithms;Time series analysis;Decision-making systems;Optimal feature sets;Base classifiers;Learning systems;Character recognition;Electronic trading;Multiclassifier system;Trading strategies;Decision making","Abstract":"The most important factors of successful trading strategy are the decisions to sell or buy. We propose multi-classifier system for decision making in algorithmic trading, whose training is carried out in three stages. At the first stage, features set is calculated based on historical data. These can be oscillators and moments that used in technical analysis, other characteristics of time series, market indexes, etc. At the second stage, base classifiers are trained using genetic algorithms, and optimal feature set for each of them is selected. At the third stage, a voting ensemble is designed, weights of base classifiers are selected also using genetic algorithms. However, the usage of genetic algorithms requires considerable time for computing, so the proposed system is implemented in a parallel environment. Testing on real data confirmed that the proposed approach allows to build a decision-making system, the results of which significantly exceed the trading strategies based on indicators of technical analysis and other techniques of machine learning. ","SELECTED":null}
{"Title":"The gap effect on the Brazilian exchange","keywords (cleaned)":"algorithm|simulation|gate|tick-by-tick data|trading|algorithmic trading|stock|index|trading strategies|r|market|tick data|strategies|exchange rate|day trading|asset class|intraday trading|large data|auction|intraday","Abstract":"We spotted a market anomaly, related to the opening gap of three asset classes, the exchange rate, Bovespa blue chips and the Ibovespa, the major stock index in Brazil; and further investigated through algorithmic trading simulation to verify our initial hunch. Our assumption, that turned out to be correct, we called the Gap Effect, and it is that big slumps or spikes in the opening gap on the beginning of the trading day, tend to a reversal, or a significant come back in the first fifteen minutes of the trading day, creating great opportunities for intraday trading. Using a large dataset of tick-by-tick data, we found a pattern which can spot striking opportunities to develop algorithmic trading strategies (long or short), based on the early movements of a security. Moreover, we confirm through Data Panel with Thresholds that the larger is the opening gap (up or down), the larger is the chance to a price reversal in the early minutes of the trading day, just after the initial auction is over.","SELECTED":null}
{"Title":"Evolving directional changes trading strategies with a new event-based indicator","keywords (cleaned)":"Economics;Financial forecasting;Technical analysis;Commerce;Algorithmic trading;Genetic algorithms;Forecasting methods;Trading strategies;Finance;Electronic trading;Directional changes;Decision making process;Price fluctuation;Decision making","Abstract":"The majority of forecasting methods use a physical time scale for studying price fluctuations of financial markets, making the flow of physical time discontinuous. An alternative to this is event-based summaries. Directional changes (DC), which is a new event-based summary method, allows for new regularities in data to be discovered and exploited, as part of trading strategies. Under this paradigm, the timeline is divided in directional change events (upwards or downwards), and overshoot events, which follow exactly after a directional change has been identified. Previous work has shown that the duration of overshoot events is on average twice the duration of a DC event. However, this was empirically observed on the specific currency pairs DC was tested with, and only under the specific time periods the tests took place. Thus, this observation is not easily generalised. In this paper, we build on this regularity, by creating a new event-based indicator. We do this by calculating the average duration time of overshoot events on each training set of each individual dataset we experiment with. This allows us to have tailored duration values for each dataset. Such knowledge is important, because it allows us to more accurately anticipate trend reversal. In order to take advantage of this new indicator, we use a genetic algorithm to combine different DC trading strategies, which use our proposed indicator as part of their decision-making process. We experiment on 5 different foreign exchange currency pairs, for a total of 50 datasets. Our results show that the proposed algorithm is able to outperform its predecessor, as well as other well-known financial benchmarks, such as a technical analysis. ","SELECTED":null}
{"Title":"Threshold convergence between the federal fund rate and South African equity returns around the colocation period","keywords (cleaned)":"Colocation;Equity returns;Federal fund rates;Global financial crisis;High frequency trading;Johannesburg stock exchange (JSE);Threshold cointegration","Abstract":"Using weekly data collected from 20.09.2008 to 09.12.2016, this paper uses dynamic threshold adjustment models to demonstrate how the introduction of high-frequency and algorithmic trading on the Johannesburg Stock Exchange (JSE) has altered convergence relations between the federal fund rate and equity returns for aggregate and disaggregate South African market indices. We particularly find that for the post-crisis period, the JSE appears to operate more efficiently, in the weak-form sense, under high frequency trading platforms. ","SELECTED":null}
{"Title":"Statistical arbitrage trading strategy in commodity futures market with the use of nanoseconds historical data","keywords (cleaned)":"Commerce;Contracts;Costs;Financial markets;MATLAB;Software testing;Commodity futures;Futures contract;High-frequency trading;Matlab- software;Price difference;Selection algorithm;Statistical arbitrage;Trading strategies;Electronic trading","Abstract":"This paper confirms the existence of statistical arbitrage opportunities by employing the nanosecond historical data in high frequency trading (HFT). When considering the possible options, the Daniel Herlemont pairs trading strategy has been selected. In order pairs trading could operate, the pair selection algorithm had to be developed. Herlemont pairs trading strategy has not been tested before by using the nanosecond information and the proposed pair selection algorithm. The main objective of the given research is to test the pairs trading strategy in HFT by calculating the returnability in commodity futures market. The statistical arbitrage strategy attempts to achieve profit by exploiting price differences of the futures contracts. The strategy takes long\/short positions when the spread between the prices widens with an expectation that the prices will converge in the future. In the given paper, the nanosecond historical data was provided by the Nanotick Company. The applied strategy has been subsequently tested with MatLab software. ","SELECTED":null}
{"Title":"Algorithmic trading using deep neural networks on high frequency data","keywords (cleaned)":"High frequency data;Computational finance;Deep neural networks;Commerce;Algorithmic trading;Linear modeling;Deep neural networks;Electronic trading;Standard deviation;Costs;Short-term forecasting;High-frequency trading;Transaction price","Abstract":"In this work, a high-frequency trading strategy using Deep Neural Networks (DNNs) is presented. The input information consists of: (i). Current time (hour and minute); (ii). The last n one-minute pseudo-returns, where n is the sliding window size parameter; (iii). The last n one-minute standard deviations of the price; (iv). The last n trend indicator, computed as the slope of the linear model fitted using the transaction prices inside a particular minute. The DNN predicts the next one-minute pseudo-return, this output is later transformed to obtain a the next predicted one-minute average price. This price is used to build a high-frequency trading strategy that buys (sells) when the next predicted average price is above (below) the last closing price. ","SELECTED":null}
{"Title":"Discovery of jump breaks in joint volatility for volume and price of high-frequency trading data in China","keywords (cleaned)":"Matrix singular values;Multivariate statistical;Financial markets;High-frequency trading;Transaction data;Realized trading volatility;Commerce;Multivariant analysis;Finance;Matrix norms;Electronic trading;Real time monitors;Singular values;Matrix norms;Matrix algebra;Case-studies","Abstract":"Recent years have witnessed more and more frequent abnormal fluctuations in stock markets and thus it is important to real-time monitor dynamically such fluctuations. To that end, this paper first proposes a realized trading volatility (RTV) model and analyzes its properties. Next, based on the RTV model, it develops a critical jump point test for the joint volatility of volume and price using matrix singular values. Finally, the proposed models are evaluated on the minute transaction data of China\u2019s Shanghai and Shenzhen A-share stock markets over 2009.01.05\u20132009.03.31. With the PV, VV and RTV sequence values extracted from the transaction data, case studies are performed on certain stocks and empirical suggestions are offered for the maintenance of the stability of the market index. ","SELECTED":null}
{"Title":"Reordering transaction execution to boost high frequency trading applications","keywords (cleaned)":"Big data;Commerce;Electronic trading;Information management;Pipelines;Application performance;High-frequency trading;Parallel executions;Portfolio selection;Social welfare;State of the art;Transaction execution;Transaction throughput;Concurrency control","Abstract":"High frequency trading (HFT) has always been welcomed because it benefits not only personal interests but also the whole social welfare. While the recent advance of portfolio selection in HFT market generates more profit, it yields much contended OLTP workloads. Featuring in exploiting the abundant parallelism, transaction pipeline, the state-of-the-art concurrency control (CC) mechanism, however suffers from limited concurrency confronted with HFT workloads. Its variants that enable more parallel execution by leveraging find-grained contention information also take little effect. To solve this problem, we for the first time observe and formulate the source of restricted concurrency as harmful ordering of transaction statements. To resolve harmful ordering, we propose PARE, a pipeline-aware reordered execution, to improve application performance by rearranging statements in order of their degrees of contention. In concrete, two mechanisms are devised to ensure the correctness of statement rearrangement and identify the degrees of contention of statements respectively. Experiment results show that PARE can improve transaction throughput and reduce transaction latency on HFT applications by upto an order of magnitude than the state-of-the-art CC mechanism. ","SELECTED":null}
{"Title":"10th International Conference on Knowledge Science, Engineering and Management, KSEM 2017","keywords (cleaned)":"neural network model|nse|r|high-frequency trading data|fuzzy logic|distance|gate|Knowledge|knowledge base|Management|Engineering|management|web|high-frequency trading|china|neural network|mining|pin|trading|vectors|software|classification|sentiment|multi-objective|algorithm|edi|wire|feature selection|ace|knowledge|negotiation|volatility|sentiment analysis|high-frequency","Abstract":"The proceedings contain 47 papers. The special focus in this conference is on Knowledge Science, Engineering and Management. The topics include: Learning sparse overcomplete word vectors without intermediate dense representations; a study of distributed semantic representations for automated essay scoring; weakly supervised feature compression based topic model for sentiment classification; an effective gated and attention-based neural network model for fine-grained financial target-dependent sentiment analysis; a hidden astroturfing detection approach base on emotion analysis; leveraging term co-occurrence distance and strong classification features for short text feature selection; a fuzzy logic based policy negotiation model; a fuzzy spatio-temporal description logic; r-calculus for the primitive statements in description logic alc; a multi-objective attribute reduction method in decision-theoretic rough set model; a behavior-based method for distinction of flooding DdoS and flash crowds; analyzing customer\u2019s product preference using wireless signals; improved knowledge base completion by the path-augmented transr model; balancing between cognitive and semantic acceptability of arguments; discovery of jump breaks in joint volatility for volume and price of high-frequency trading data in china; device-free intruder sensing leveraging fine-grained physical layer signatures; understanding knowledge management in agile software development practice; multi-view unit intact space learning; a novel blemish detection algorithm for camera quality testing; learning to infer API mappings from API documents; super-resolution for images with barrel lens distortions and mining schema knowledge from linked data on the web.","SELECTED":null}
{"Title":"Algorithmic daily trading based on experts\u2019 recommendations","keywords (cleaned)":"Feature selection;Classification (of information);Intelligent systems;K-nn;Investments;High-frequency trading;Classification models;Commerce;Machine learning models;classification;Electronic data interchange;Algorithmic trading;Feature extraction;Gradient boosting;Sparse features;Electronic trading;Expert recommendations;Decision trees;Trading strategies;Gradient boosting decision trees","Abstract":"Trading financial products evolved from manual transactions, carried out on investors\u2019 behalf by well informed market experts to automated software machines trading with millisecond latencies on continuous data feeds at computerised market exchanges. While high-frequency trading is dominated by the algorithmic robots, mid-frequency spectrum, around daily trading, seems left open for deep human intuition and complex knowledge acquired for years to make optimal trading decisions. Banks, brokerage houses and independent experts use these insights to make daily trading recommendations for individual and business customers. How good and reliable are they? This work explores the value of such expert recommendations for algorithmic trading util-ising various state of the art machine learning models in the context of ISMIS 2017 Data Mining Competition. We point at highly unstable nature of market sentiments and generally poor individual expert performances that limit the utility of their recommendations for successful trading. However, upon a thorough investigation of different competitive classification models applied to sparse features derived from experts\u2019 recommendations, we identified several successful trading strategies that showed top performance in ISMIS 2017 Competition and retrospectively analysed how to prevent such models from over-fitting. ","SELECTED":null}
{"Title":"Ensemble trend classification in the foreign exchange market using class variable fitting","keywords (cleaned)":"Intelligent systems;Financial markets;Price prediction;Technical analysis;Foreign exchange market;Commerce;Trend classification;Artificial intelligence;Algorithmic trading;Analysis approach;Data mining;Historical performance;Electronic trading;Ensemble classification;Ensemble classifiers;Ensemble classifiers;Trading strategies;Data mining","Abstract":"We present a method for ensemble classification of trends in the foreign exchange market using historical data, technical analysis and class variable fitting. We have implemented a complete closed source algorithmic trading platform in Java and MQL. In contradiction to standard concrete price prediction or trend classification, we apply ensemble trend classification and search for optimal class variable. We use single timeframe in contradiction to multiple timeframes analysis approach. We show substantial profitably applying the trading strategies derived by our approach. This paper has two main objectives. The first, to present a new trend definition by expanding the search space for more efficient trading strategies. The second, is to present a new algorithmic trading platform and provide a live trading historical performance rather than back testing results. While previous works in the field tend to incorporate single trading strategy, we show a method for finding multiple trading strategies for various assets. ","SELECTED":null}
{"Title":"Leveraged exchange trated funds\u2019s emerging markets: A practical application of statistical arbitrage based on cointegration","keywords (cleaned)":"Cointegration;Exchange traded funds (ETF);Mean reversion;Pairs trading;Statistical arbitrage","Abstract":"Pair Trading is a Neutral Market Strategy descendant of Statistical Arbitrage. Its objective is to identify pairs of assets whose historical prices or variations have high correlation between them. To attain this, pairs trading takes advantage of overvalued assets sales and purchases undervalued assets. To identify the goal pair, we performed back-testing using historical log returns (from December 31, 2008 through April 16, 2013). With the goal pair identified we run the daily strategy using historical adjusted at closed price data and historical log returns (from December 31, 2010 through September 11, 2015). Herein, we consider two inverse Exchange Trade Funds versus benchmark EEM (iShares MSCI Emerging Markets ETF) index. The objective of this work is to demonstrate that automated trading strategy built under the co-integration approach in moving windows of 60 and 180 days is able to beat a buy and hold strategy on the EEM benchmark. ","SELECTED":null}
{"Title":"MatTrader: An automated trading and financial data analysis framework for Matlab and Java","keywords (cleaned)":"r|Trade|research|real time|financial data analysis|financial assets|automated trading|data structures|real-time trading|historical data|simulation|pin|trading|trading strategies|arch|strategies|and simulation|data analysis|quantitative investment|framework|financial data|Matlab|investment","Abstract":"This paper introduces MatTrader, a framework for real-time trading, historical data analysis and simulation, aimed at giving researchers and investors an instrument for developing and testing quantitative investment strategies and paradigms. MatTrader is built to grant maximum flexibility and control, providing data structures for representing financial assets and data, methods for trading, managing orders and retrieving historical data, and a set of events for implementing real time investment strategies. It also includes Matlab-specific classes for automatic validation of trading strategies and for feedback-driven trading. The framework is fully documented, open source, and it contains several ready-to-use examples. It is freely available for use by the community, and the Java library allows to easily port the framework to other programming languages. ","SELECTED":null}
{"Title":"Optimal execution in high-frequency trading with Bayesian learning","keywords (cleaned)":"Optimal solutions;Financial data processing;Commerce;Optimal execution;Optimal trading strategy;Correlation theory;Terminal wealth;Two-step procedure;Electronic trading;Dynamic programming;Limit order book;Bayesian learning;High-frequency trading","Abstract":"We consider optimal trading strategies in which traders submit bid and ask quotes to maximize the expected quadratic utility of total terminal wealth in a limit order book. The trader's bid and ask quotes will be changed by the Poisson arrival of market orders. Meanwhile, the trader may update his estimate of other traders\u2019 target sizes and directions by Bayesian learning. The solution of optimal execution in the limit order book is a two-step procedure. First, we model an inactive trading with no limit order in the market. The dealer simply holds dollars and shares of stocks until terminal time. Second, he calibrates his bid and ask quotes to the limit order book. The optimal solutions are given by dynamic programming and in fact they are globally optimal. We also give numerical simulation to the value function and optimal quotes at the last part of the article. ","SELECTED":null}
{"Title":"Automated trading based on biclustering mining and fuzzy modeling","keywords (cleaned)":"Technical indicator;Technical analysis;Forecasting;Forecasting modeling;Electronic trading;Technical indicator;fuzzy logic;biclustering algorithm;Trading patterns;Trading patterns;Robots;fuzzy logic;biclustering algorithm;Fuzzy inference;Comparative methods;Financial markets;Commerce;Fuzzy inference method;Finance;Profitability;Inference engines;Automated trading","Abstract":"More and more records or charts of historical financial data are used for technical analysis, hoping to identify patterns that can be exploited to achieve excess profits. Technical analysis has been widely used in the real stock market to forecast stock price or stock trading points. The good association of technical indicators can obtain good prediction results in stock markets. But the selection of technical indicators is also a tough problem. In this paper, we introduce a forecasting model incorporating biclustering algorithm with a new fuzzy inference method. Biclustering algorithm discover biclusters which are regarded as trading patterns. And a new fuzzy inference method is used for determining trading points. The proposed forecasting model (called BM-FM) was used for predicting three real-world stock data. The experiment is designed by comparing the profit ratio in TPP-based strategy, IPLR and IPLR-ANN with the profit ratio in our forecasting model. According to experimental results, it is indicated that our model obtains more earnings and higher profit ratio than other comparative methods. ","SELECTED":null}
{"Title":"How do individual investors trade?","keywords (cleaned)":"foreign exchange|r|Trade|market microstructure|data set|models|investment decisions|monitoring effect|high-frequency trading|order flow|price movement|microstructure|finance|trading|market|price changes|behavioral finance|individual investors|edi|trading platform|trade|investment|high-frequency","Abstract":"This paper examines how high-frequency trading decisions of individual investors are influenced by past price changes. Specifically, we address the question as to whether decisions to open or close a position are different when investors already hold a position compared with when they do not. Based on a unique data set from an electronic foreign exchange trading platform, OANDA FXTrade, we find that investors' future order flow is (significantly) driven by past price movements and that these predictive patterns last up to several hours. This observation clearly shows that for high-frequency trading, investors rely on previous price movements in making future investment decisions.We provide clear evidence that market and limit orders flows are much more predictable if those orders are submitted to close an existing position than if they are used to open one. We interpret this finding as evidence for the existence of a monitoring effect, which has implications for theoretical market microstructure models and behavioral finance phenomena, such as the endowment effect. ","SELECTED":null}
{"Title":"Kelly model and its application in high frequency trading","keywords (cleaned)":"Investments;Electronic trading;Financial data processing;Quadratic programming;Kelly;Capital growth;Continuous time systems;Portfolio managements;Optimization;Law of large numbers;Algorithms;Portfolio optimization;Asset allocation;Optimization algorithms;High-frequency trading","Abstract":"We investigate the problem of dynamic optimal capital growth of a portfolio under transaction costs constrained. A general framework that one strives to maximize the long term growth rate of its expected log utility was developed. However, when applying to portfolio management with many assets, optimization algorithms such as quadratic programming run into difficulties. In our research, we get the fraction for a portfolio in continuous time by combining law of large numbers and the additivity of the logarithm utility functions. Empirical research indicate that the approach is inspiring for this class of problems. ","SELECTED":null}
{"Title":"A hybrid approach to exchange rates: How do macro news and order flow affect exchange rate volatility?","keywords (cleaned)":"Exchange rate volatility;G1;Information interaction;Macro news;Microstructure;order flow;Private information","Abstract":"Purpose \u2013 This study aims to investigate the impact of information, both public macro news and private information, on exchange rate volatility in an integrated framework. Design\/methodology\/approach \u2013 The authors apply real-time data of macro announcements and high-frequency trading data (German Deutsche Mark to US dollar, DEM\/USD, from 1 May to 31August 1996) to GARCH models and examine various model specifications. Findings \u2013 Data analysis demonstrates real-time macro news and market makers\u2019 private information both have a significant impact on exchange rate volatility, but there is no interaction between macro and micro information in the information transmission process. Originality\/value \u2013 This study contributes to empirical hybrid studies of examining exchange rates volatility, which is in line with literature that combine both macro and micro fundamentals in examining exchange rates variation. Particularly, a key element of this study is to use a microstructure fundamental variable, namely, order flow, to capture private information in an exchange rate volatility study. ","SELECTED":null}
{"Title":"A distributed game-theoretic approach for IaaS service trading in an auction-based cloud market","keywords (cleaned)":"Nash equilibrium state;Data privacy;Resource allocation;Optimal bidding price;Distributed computer systems;Computer games;Nash equilibrium;Cloud computing;Noncooperative game;Game theory;Computation theory;Infrastructure as a Service;Nash equilibria;Commerce;Resource allocation;Infrastructure as a service (IaaS);Incomplete information;Economics;Auction;Experimental investigations;Big data;Costs","Abstract":"With the rapid development of IaaS market, how to efficiently trade between cloud providers and users is becoming a new challenge which attracts huge attentions from both industry and academia. Compared with traditional fixed-price model, market-oriented trading mechanism such as auction demonstrates greater promise for resource pricing and allocation in clouds due to its adaptability and flexibility. In this paper, we focus on the competitive and bidding scenario among independent users in an auction-based IaaS market. Participants have different composite service demands and they adjust their bidding prices during the trading with the aim of obtaining appropriate amount of resources to maximize their profits. We formulate this scenario as a dynamic noncooperative game with incomplete information and propose a feedback-based distributed bidding adjustment approach to find the approximated optimal bids (i.e. Nash Equilibrium) for each user so as to achieve a fair and multi-win resources allocation outcome in the whole market. Our experimental investigation showed that with the help of our agent-based automated trading model and distributed bidding algorithms, the Nash equilibrium state of the cloud resource allocation game can be reached efficiently within an acceptable time, and the optimal bidding prices of each user could be obtained at the same time. ","SELECTED":null}
{"Title":"Designing, implementing and testing an automated trading strategy based on dynamic Bayesian networks, the limit order book information, and the random entry protocol","keywords (cleaned)":"foreign exchange|Markov Model|information|r|Hierarchical Hidden Markov Model|risk|automated trading|wavelet|limit order book|Hidden Markov Model|order flow|market data|R|Design|trading|market|arch|design|sentiment|liquidity|training and testing|return|dynamics|order book information|complexity|trading strategy|edi|training|knowledge|trade|dynamic Bayesian networks|Bayesian networks|order book|Order Book|performance","Abstract":"This paper evaluates, using the Random Entry Protocol technique, a highfrequency trading strategy based on a Dynamic Bayesian Network (DBN) that can identify predictive trend patterns in foreign exchange orden-driven markets. The proposed DNB allows simultaneously to represent expert knowledge of skilled traders in a model structure and to learn computationally from data information that reflects relevant market sentiment dynamics. The DBN is derived from a Hierarchical Hidden Markov Model (HHMM) that incorporates expert knowledge in its design and learns the trend patterns present in the market data. The wavelet representation is used to produce compact representations of the LOB liquidity dynamics that simultaneously reduces the time complexity of the computational learning and improves its precision. In previous works, this trading strategy has been shown to be competitive when compared with conventional techniques. However, these works failed to control for unwanted dependencies in the return series used for training and testing that may have skewed performance results to the positive side. This paper constructs key trading strategy estimators based on the Random Entry Protocol over the USD-COP data. This technique eliminates unwanted dependencies on returns and order flow while keeps the natural autocorrelation structure of the LimitOrder Book (LOB). It is still concluded that the HHMM-based model results are competitive with a positive, statistically significant P\/L and a well-understood risk profile. Buy-and-Hold results calculated over the testing period are provided for comparison reasons. ","SELECTED":null}
{"Title":"Analysis of market trend regimes for March 2011 USDJPY exchange rate tick data","keywords (cleaned)":"Autonomous agents;Disasters;Electronic trading;Microstructure;Multi agent systems;Network layers;Spectrum analysis;Algorithmic trading;Exchange rates;Extreme events;Market microstructure;Market trends;Singular spectrum analysis;Tohoku earthquakes;Trend prediction;Commerce","Abstract":"This paper reports the analysis of the foreign exchange market for the USD and JPY currency pair in March 2011 for the period of 23 trading days comprising 3,774,982 transactions. On March 11, 2011 the disaster of the Great Tohoku Earthquake disaster accompanied by tsunami took place; the event was followed by a highly turbulent market with JPY appreciating without limits in the panic that ensued; major central banks of the world intervened since after to weaken the yen. We analyze the tick data set using the criteria of aggregate volatility, extreme-event distribution, and singular spectrum analysis to discover the market microstructure during the central bank interventions. In addition, a multi-layer neural network algorithm is designed to extract the causality regime on the microscale for each trading day. At the beginning of the month, the success ratios in the trend prediction hit levels as high as the order of 70 %, followed by about a 10-point decrease for the rest of the data set. Distribution of intra-trade times shows clear signs of algorithmic trading with the transaction clock ticking at the time intervals of 0.1, 0.25 and 10.0 s. The extracted trend prediction rates represent lower bounds with respect to other methods. The present work offers a useful insight into algorithmic trading and market microstructure during extreme events. ","SELECTED":null}
{"Title":"19th International Conference on Business Information Systems, BIS 2016","keywords (cleaned)":"Information|implementation|information|r|real time|models|risk|High-frequency trading|innovation|Information Systems|information system|speed|web|Information System|computational speed|architecture|profitability|credit scoring|big data|services|trading|internet|information systems|market|arch|life cycle|machine learning|risk assessment|High-frequency|edi|business process|ontologies|parallel r|profit","Abstract":"The proceedings contain 34 papers. The special focus in this conference is on Ecosystems and Big\/Smart Data. The topics include: High-frequency trading, computational speed and profitability; a methodology for quality-based selection of internet data sources in maritime domain; towards identifying the business value of big data in a digital business ecosystem; specification and implementation of a data generator to simulate fraudulent user behavior; quantitative analysis of art market using ontologies, named entity recognition and machine learning; search engine visibility indices versus visitor traffic on websites; situation awareness for push-based recommendations in mobile devices; a new perspective over the risk assessment in credit scoring analysis using the adaptive reference system; drivers and inhibitors for the adoption of public cloud services in Germany; parallel real time investigation of communication security changes based on probabilistic timed automata; batch processing across multiple business processes based on object life cycles; towards a methodology for industrie 4.0 transformation; discovering decision models from event logs; a formalization of multiagent organizations in business information systems; bridging the gap between independent enterprise architecture domain models; a usage control model extension for the verification of security policies in artifact-centric business process models; overcoming the barriers of sustainable business model innovations by integrating open innovation; an ontological matching approach for enterprise architecture model analysis; requirements on the functionality of a system for service self-customization and approach, realization and application to a payments transaction processing service.","SELECTED":null}
{"Title":"Portfolio of global futures algorithmic trading strategies for best out-of-sample performance","keywords (cleaned)":"Trend following;Financial markets;Electronic trading;Information use;Commerce;Algorithmic trading;Trading models;comgen;Information systems;Optimization;Mean reversion;Comsha;Sharpe ratio;Construction method;Sharpe ratio;Portfolio construction","Abstract":"We investigate two different portfolio construction methods for two different sets of algorithmic trading strategies that trade global futures. The problem becomes complex if we consider the out-of-sample performance. The Comgen method blindly optimizes the Sharpe ratio, and Comsha does the same but gives priority to strategies that individually have the better Sharpe ratio. It has been shown in the past that high Sharpe ratio strategies tend to perform better in out-of-sample periods. As the benchmark method, we use an equally weighted (1\/N, na\u00efve) portfolio. The analysis is performed on two years of out-of-sample data using a walk forward approach in 24 independent periods. We use the mean reversion and trend following datasets consisting of 22,702 and 36,466 trading models (time series), respectively. We conclude that Comsha produces better results with trend-following methods, and Comsha performs the same as Comgen with other type of strategies. ","SELECTED":null}
{"Title":"Building a type-2 fuzzy random support vector regrebion scheme in quantitative investment","keywords (cleaned)":"Financial data processing;Fuzzy systems;Forecasting;Prediction accuracy;Electronic trading;Support vector;Over the counter markets;Support vector regrebion model;Financial markets;Fuzzy random variable;Creditability theory;Type-2 fuzzy random variable;Type-reduction;Quantitative investment;Confidence interval;Financial markets;Type-2 fuzzy;Commerce;Finance;Random variables;Type reduction;Investments;Costs","Abstract":"Financial markets are connected well these days. One clab abets' price performance is usually affected by movements of other clabes of abets. However, the relationship between them is hard to trace and predict along with increase in complexity of markets' behaviors these days. Nothing like stock market, money or bond market is an over-the-counter market, where abets' prices are often presented in the form of clabes of discrete quotations by trader's subjective judgments, thus are hard to model and analyze. Given concern to this, we define the Type 2 fuzzy random variable (T2 fuzzy random variable) to quantify those bid\/offer behaviors in this paper. Moreover, we build a T2 fuzzy random support vector regrebion (T2-FSVR)scheme to study relationships between these markets, thus form an effective trading strategy to predict the trend of market prices. We use matlab platform to implement and test the effectiveneb of the new model, then train and test it with 2014 whole years price data of bond and money markets. We also compare T2-FSVRs prediction accuracy with type-2 fuzzy expected regrebion(T2-FER) and confidence-interval-based fuzzy random regrebion model(CI-FRRM). The result shows that T2-FSVR outperforms and has 98% accuracy while CI-FRRM has 81% accuracy and T2-FER has only 70% accuracy. Moreover,T2-FSVR can be developed into a automated trading strategy for practical busineb use, which is able to learn behaviors of different markets based on mab of available historical and real time data and earn profit automatically. ","SELECTED":null}
{"Title":"Data and Alpha Design","keywords (cleaned)":"Academic literature;Alpha design;Alpha ideas;Alpha researcher;Data validation;Data vendors;High-frequency trading firms;Simulation","Abstract":"Data plays a central role in alpha design. The basic data is required to run a simulation. Second, data itself can inspire alpha ideas. Finding new data has always been a critical skill for an alpha researcher. People always prefer good performance and low correlated alphas. A new dataset can serve both purposes. This chapter talks about sourcing data from academic literature and data vendors, validating the data, and understanding data before using it. Data grows in three areas: variety, volume, and velocity. Data is also created fast. Now the latency for high-frequency trading firms is single-digit microseconds.More data is always better, as long as we can handle it. When there is more data to play with, more alpha, faster alphas, and better alphas can be made. Big investment in big data means bigger returns. ","SELECTED":null}
{"Title":"Faster Convergence to the Estimation of Quadratic Variation with Microstructure Noise","keywords (cleaned)":"High frequency data;High frequency HF;Microstructure;Faster convergence;Realized variation;Quadratic variations;Quadratic variations;microstructure noise;Frequency estimation;Electronic trading;High-frequency;Recursive estimators;High-frequency trading;Batch estimators","Abstract":"The continuous quadratic variation of asset return plays a critical role for high-frequency trading. However, the microstructure noise could bias the estimation of the continuous quadratic variation. Zhang et al. (2005) proposed a batch estimator for the continuous quadratic variation of high-frequency data in the presence of microstructure noise. It gives the estimates after all the data arrive. This article proposes a recursive version of their estimator that outputs variation estimates as the data arrive. Our estimator gives excellent estimates well before all the data arrive. Both real high-frequency futures data and simulation data confirm the performance of the recursive estimator. ","SELECTED":null}
{"Title":"Automated pricing in a multiagent prediction market using a partially observable stochastic game","keywords (cleaned)":"Risk-averse traders;Stochastic game;Software agents;Correlated equilibria;Stochastic systems;Automated trading;Commerce;Extensive simulations;Automation;Forecasting;Market-based mechanisms;Prediction markets;Prediction markets;Distributed information;Stochastic models;Correlated equilibrium;Costs;Game theory;Risk averse;Computation theory","Abstract":"Prediction markets offer an efficient market-based mechanism to aggregate large amounts of dispersed or distributed information from different people to predict the possible outcome of future events. Recently, automated prediction markets where software trading agents perform market operations such as trading and updating beliefs on behalf of humans have been proposed. A challenging aspect in automated prediction markets is to develop suitable techniques that can be used by automated trading agents to update the price at which they should trade securities related to an event so that they can increase their profit. This problem is nontrivial, as the decision to trade and the price at which trading should occur depends on several dynamic factors, such as incoming information related to the event for which the security is being traded, the belief-update mechanism and risk attitude of the trading agent, and the trading decision and trading prices of other agents. To address this problem, we have proposed a new behavior model for trading agents based on a game-theoretic framework called partially observable stochastic game with information (POSGI). We propose a correlated equilibrium (CE)-based solution strategy for this game that allows each agent to dynamically choose an action (to buy or sell or hold) in the prediction market. We have also performed extensive simulation experiments using the data obtained from the Intrade prediction market for four different prediction markets. Our results show that our POSGI model and CE strategy produces prices that are strongly correlated with the prices of the real prediction markets. Results comparing our CE strategy with five other strategies commonly used in similar market show that our CE strategy improves price predictions and provides higher utilities to the agents compared to other existing strategies. ","SELECTED":null}
{"Title":"Adaptive universal trading strategy","keywords (cleaned)":"Numerical experiments;Financial markets;Stock price;Calibration;Algorithmic trading;Commerce;Algorithmic trading;Stock price;Forecasting;Forecasting methods;Trading strategies;Continuous functions;well-calibrated forecasts;Finance;Algorithms;Randomized Algorithms;Universal algorithm;Computation theory","Abstract":"In the first part of the paper, a universal method of algorithmic trading in stock markets, which provides the asymptotically highest profit as compared to any trading strategy that is not extremely complex, is proposed. The universal strategy is compared with the class of strategies that compute the amount of bought or sold units of a financial instrument made in each step of trading by means of continuous functions of input information. The universal strategy makes decisions based on the forecasts of future prices of financial instruments with the help of the randomized algorithm for computing well-calibrated forecasts in Dawid\u2019s sense. The given algorithm is the development of the Kakade and Foster theory and is combined with the similar forecasting method suggested by V.G. Vovk. In the second part, the results of numerical experiments performed with the proposed universal algorithm and historical data on financial series are presented. ","SELECTED":null}
{"Title":"Efficient Performance Evaluation for High-Frequency Traders","keywords (cleaned)":"Portfolios;Financial markets;Performance evaluation;Commerce;Aluminum alloys;Alpha;Electronic trading;Subordinators;Sharpe ratio;Subordinators;Sharpe ratio;High-frequency trading;Efficiency","Abstract":"Consider two traders A and B with identical Sharpe ratio (SR) and portfolio volatility. Suppose trader B trades less than trader A. Other things equal, trader B is more efficient than trader A because trader B attains the same SR as trader A with less trading time. Trader B's efficiency is not apparent when annualized SR (ASR) formulas are extended to high-frequency trading (HFT). In this chapter, we show how the pantheon of modified SRs in the literature could be adjusted by a single factor, which identifies trader efficiency via adaptive HFT stock price dynamics and cycle lengths for maximal alpha. Specifically, we derive a directing process for HFT trade strategy, which jumps positively only when the trader executes a successful trade or stays flat otherwise. We identify a simple intuitively appealing summary statistic, which can be extrapolated from publicly available data, and which serves as the single factor for modifying ASR to get an efficient Sharpe ratio (ESR). Application to risk and return data for HFT in Baron et al. (2014) finds that the ESR for aggressive HFT is 1.15, medium HFT is 2.88, and passive HFT is 1.43. Those ESRs for HFT numbers are equivalent to reported SR estimates for hedge funds multistrategy, convertible option arbitrage, and fund of fund strategies. ","SELECTED":null}
{"Title":"Data Characteristics for High-Frequency Trading Systems","keywords (cleaned)":"Economics;High frequency data;Commerce;Algorithmic trading;Time series analysis;Foreign exchange;Foreign exchange (FX);Electronic trading;High-frequency data;Statistical prerequisites;Stylized facts","Abstract":"Like all trading systems, high-frequency trading systems work by exploiting inefficiencies in the pricing process. Before embarking on designing a high-frequency trading system, it is important to confirm that the price data for the instrument you intend to trade exhibits inefficiencies at the time frame you intend to exploit. Tests for randomness and market efficiency should be conducted at the required time frame to confirm that the instrument is not efficient at that time frame. The results of these tests also give some direction to the future style of trading system that is likely to be successful in the required time frame. ","SELECTED":null}
{"Title":"High-Frequency Trading under Information Regimes","keywords (cleaned)":"Financial data processing;Market microstructure;Liquidity;Commerce;Trading strategies;Information regimes;Trading strategies;Correlation theory;Wavelets analysis;Limit order book;Electronic trading;Costs;Limit order book;Market microstructure","Abstract":"This chapter presents ways by which high-frequency trading can benefit from the identification of information regimes in limit order books. As introduced by Lehmann, in an information regime all the information is trade related, arrives via order flow, and the fundamental value that underlines the prices does not change, it is simply translated by the size of the executed market order and the backfilling adjustment. During an information regime the best quotes and the underlying values follow a path defined by the limit order book. This implies that algorithmic trading gains can be strengthened not only by identifying these information regimes but also by forecasting the transition periods between regimes that signal fundamental changes in assets' prices, trading behavior, and optimal trading strategies. Our results show that the discovery and identification of information regimes essentially uncover the mechanism by which latent demands are translated into realized prices and volumes and, from here, that they can be used by professional traders. ","SELECTED":null}
{"Title":"Visualizations for sense-making in financial market regulation","keywords (cleaned)":"Financial data;Financial Market Regulation;Electronic Order Book;Financial markets;Visual Anaylsis;Visualization;Commerce;Financial data;Big data;Market regulation;Regulatory analysis;Correlation theory;Finance;Electronic ordering;Exploratory data analysis;Market surveillance;Electronic market","Abstract":"Electronic markets and automated trading have resulted in a drastic increase in the quantity and complexity of regulatory data. Regulatory analysis now includes detailed analysis of all messaging and communications related to electronic limit order books. New order types, intra-market behavior and other exchange functionality further complicate analysis. Data visualizations have proven to be a fundamental tool for building intuition and enabling exploratory data analysis in many fields. In this paper, we propose the incorporation of visualizations in the workflow of multiple financial regulatory roles, including market surveillance, enforcement, and academic research. ","SELECTED":null}
{"Title":"Computational models of algorithmic trading in financial markets (doctoral consortium)","keywords (cleaned)":"Multi agent systems;Financial data processing;Agent-based simulation;Algorithmic trading;Computational agents;Electronic trading;Agent based simulation;High-frequency trading;Game theory;Frequent call market;Computational model;Allocative efficiency;Financial markets;Commerce;Finance;Game theoretic analysis;Algorithms;Autonomous agents;Efficiency;Investments;Societal implications;Computational methods;Market making","Abstract":"In today's financial markets, algorithmic trading, the use of quantitative algorithms to automate the submission of orders, is responsible for the majority of trading activity. To better understand the societal implications of algorithmic trading, I construct computational agent-based models comprised of investors and algorithmic traders. I examine two overlapping types of algorithmic traders: high-frequency traders, who exploit speed advantages for profit, and market makers, who facilitate trade and supply liquidity by simultaneously maintaining offers to buy and sell. I employ simulation and empirical game-theoretic analysis to study trader behavior in equilibrium, that is, when all traders best- respond to their environment and other agents' strategies. I focus on the impact of algorithmic trading on allocative efficiency, or overall gains from trade, and the potential for a call market, in which orders are matched to trade at periodic intervals, to mitigate the latency advantages of high- frequency traders. Copyright ","SELECTED":null}
{"Title":"Experimental Computational Simulation Environments for Big Data Analytic in Social Sciences","keywords (cleaned)":"Big data analytics;Computational simulation environments;Financial software systems","Abstract":"Experimental computational simulation environments (e.g., Galas, 2014) are increasingly being developed by major financial institutions to model their analytic algorithms; this includes evaluation of algorithm stability, estimation of its optimal parameters, and the expected risk and performance profiles. Such environments rely on big data analytics (e.g., McAfee and Brynjolfsson, 2012), as part of their software infrastructure, to enable large-scale testing, optimization, and monitoring of algorithms running in virtual or real mode.At UCL, we believe that such environments will have a profound impact on the way research is conducted in social sciences. Consequently, for the last few years, we have been working on our DRACUS system, a state of-the-art computational simulation environment, believed to be the first available for academic research in Computational Finance, specifically Financial Economics. As part of the DRACUS project, we work to support simulations in algorithmic trading, systemic risk, and sentiment analysis. ","SELECTED":null}
{"Title":"Impact of position-based market makers to shares of markets\u2019 volumes \u2013 An artificial market approach","keywords (cleaned)":"Multi agent;Balloons;Competition among markets;Financial markets;Multi agent systems;Simulation;Financial data processing;Market-maker;Commerce;Trading volumes;Share of trading volume;High frequency trading;Multi agent-based;Electronic trading;Market maker;Profitability;High-frequency trading","Abstract":"We analyzed the impact of position-based market maker, which tries to maintain its neutral position, to the competition among stock exchanges by an artificial market simulation approach. In the previous study, we built an artificial market model and investigated for the impact of non-position-based market maker\u2019s spread to the markets\u2019 shares of trading volumes. However it had the serious problem that the non-position-based market maker is too simple to manage its own position properly and so we could not judge weather the result of previous study is correct or not. Thus in this study, we made a position-based market maker and explored the competition, in terms of taking markets\u2019 shares of trading volumes, between two artificial financial markets that have exactly the same specifications except existing a market maker, the non-position-based market maker or the position-based market maker. As a result, we found that the position-based market maker can acquire the share of trading volumes from the competitor even though its spread is bigger than bid-offer-spread of the competitor. Moreover, we revealed that position-based market maker can get a profit even in the situation that its spread or tick sizes of the stock exchanges are small. In addition to that, position-based market maker made a profit in almost all experiments which we conducted in this research by changing its spread and tick sizes of markets. At last, we confirmed that position-based market maker can manage its position properly compared to non-position-based market maker. In conclusion, the position-based market maker can not only supply liquidity to stock exchanges and contribute to acquire the share from the competitor as well as the non-position-based market maker does, but also manage its own position properly and make a profit. ","SELECTED":null}
{"Title":"2015 National Conference on Parallel Computing Technologies, PARCOMPTECH 2015","keywords (cleaned)":"implementation|r|MATLAB|models|algorithmic trading system|trading system|Twitter|Parallel Computing|DVFS|UML|web|R|architecture|high performance computing|trading|algorithmic trading|performance analysis|arch|design|applications|algorithm|edi|wire|performance","Abstract":"The proceedings contain 11 papers. The topics discussed include: performance analysis of the parallel code execution for an algorithmic trading system, generated from UML models by end users; DVFS based heterogeneous scheduling for power optimisation and load balancing in HPC clustersaper; security strengthening by combining fingerprints; designing a parallel algorithm for heat conduction using MPI, OpenMP and CUDA; a technical survey on various VDC request embedding techniques in virtual data center; analysis of performance enhancement on graphic processor based heterogeneous architecture: a CUDA and MATLAB experiment; multithreaded implementation of data intensive applications with overlapped I\/O and computation; energy efficient rescheduling algorithm for high performance computing; crawling through web to extract the data from social networking site - Twitter; and MRA: multi-level routing algorithm to balance the traffic load in wireless ad hoc network.","SELECTED":null}
{"Title":"Transnational capital and the technology of domination and desire","keywords (cleaned)":"Finance capital;High frequency trading;Information technology;Mega data;Transnational capitalist class","Abstract":"The revolution in information technologies has brought about a fundamental shift in the productive forces. It has become the organising tool of the transnational capitalist class in their drive to global financialisation and production. In important areas information technology has radically reduced the socially necessary labour time to produce great concentrations of wealth, the results of which have significant impacts on social relations. Additionally, the desire for new information technology products, particularly in the realm of social networking and media, has led to a monetarisation of our private information. The control of mega data servers, by both the state security apparatus and information technology corporations, has led to new forms of coercion. This dialectic, of consent and coercion, is consistent with Gramsci\u2019s theory of cultural and ideological hegemony. ","SELECTED":null}
{"Title":"Ontology-supported design of domain-specific languages: A complex event processing case study","keywords (cleaned)":"Ontology;Problem oriented languages;Security of data;Algorithmic trading;Complex event processing;Domain knowledge;Domain specific languages;Event Processing;Multiple contexts;Multiple domains;Robust monitoring;Digital subscriber lines","Abstract":"This chapter introduces a novel approach for design of Domain-Specific Languages (DSL). It is very common in practice that the same problems emerge in different application domains (e.g. the modeling support for complex event processing is desirable in the domain of algorithmic trading, IT security assessment, robust monitoring, etc.). A DSL operates in one single domain, but the above-mentioned cross-domain challenges raise the question: is it possible to automate the design of DSLs which are so closely related? This approach demonstrates how a family of domain-specific languages can be developed for multiple domains from a single generic language metamodel with generative techniques. The basic idea is to refine the targeted domain with separating the problem domain from the context domain. This allows designing a generic language based on the problem and customizing it with the appropriate extensions for arbitrary contexts, thus defining as many DSLs and as many contexts as one extends the generic language for. The authors also present an ontology-based approach for establishing contextspecific domain knowledge bases. The results are discussed through a case study, where a language for event processing is designed and extended for multiple context domains. ","SELECTED":null}
{"Title":"Single stock dynamics on high-frequency data: From a compressed coding perspective","keywords (cleaned)":"Algorithms;article;coding;commercial phenomena;digital compression;Finance;global market;hierarchical factor segmentation;high frequency return;mathematical parameters;mathematical phenomena;single stock dynamics;Trading volumes;transaction number;Algorithms;information processing;Investments;statistical model;statistics and numerical data;Algorithms;Data Compression;Investments;Models, Economic","Abstract":"High-frequency return, trading volume and transaction number are digitally coded via a nonparametric computing algorithm, called hierarchical factor segmentation (HFS), and then are coupled together to reveal a single stock dynamics without global state-space structural assumptions. The base-8 digital coding sequence, which is capable of revealing contrasting aggregation against sparsity of extreme events, is further compressed into a shortened sequence of state transitions. This compressed digital code sequence vividly demonstrates that the aggregation of large absolute returns is the primary driving force for stimulating both the aggregations of large trading volumes and transaction numbers. The state of system-wise synchrony is manifested with very frequent recurrence in the stock dynamics. And this data-driven dynamic mechanism is seen to correspondingly vary as the global market transiting in and out of contraction-expansion cycles. These results not only elaborate the stock dynamics of interest to a fuller extent, but also contradict some classical theories in finance. Overall this version of stock dynamics is potentially more coherent and realistic, especially when the current financial market is increasingly powered by high-frequency trading via computer algorithms, rather than by individual investors. ","SELECTED":null}
{"Title":"Exploring Irregular Time Series through Non-Uniform Fast Fourier Transform","keywords (cleaned)":"Commerce;Digital storage;Electronic trading;Fast Fourier transforms;Fourier series;Time series;Automated trading;Fourier components;High performance computing;Irregular time series;Natural gas futures;Non-uniform fast Fourier transforms;Real-world time series;Structural feature;Time series analysis","Abstract":"Most popular analysis tools on time series require the data to be taken at uniform time intervals. However, the realworld time series, such as those fromnancial markets, are typically taken at irregular time intervals. It is a common practice to resample or bin the irregular time series into a regular one, but there are significant limitations on this practice. For example, if one is to resample the trading activities of a stock into hourly series, then the time series can only last through the trading day, because there usually is no trading in the night. In this work, we explore the dynamics of irregular time series through a high-performance computing algorithm known as Non-Uniform Fast Fourier Transform (NUFFT).To illustrate its effectiveness, we apply NUFFT on the trading records of natural gas futures contracts for the last seven years. Tests show that NUFFT results accurately capture well-known structural features in the trading records, such as weekly and daily cycles. At the same time the results also reveal unexplored features, such as the presence of multiple power laws. In particular, we observe an emerging power law in the Fourier spectra in recent years. We also detect a strong Fourier component at the precise frequency once per minute, which implies significant automated trading activities might be triggered by clock. ","SELECTED":null}
{"Title":"Dimensionality issues in searching for the best portfolio","keywords (cleaned)":"Sample size problems;Multi agent systems;New approaches;Weight calculation;Mean variance;Commerce;Automation;Large scale data;Cost-sensitive;Sampling problems;Automated trading;Multiagent systems","Abstract":"This work analyses dimensionality issues in automated trading portfolio construction. We suggest the best risk\/return portfolio creation schema for automated trading. Study shows that effectiveness of standard portfolio weight calculation rules depends on the dimensionality and sample size ratio. To solve sample size problem a multistage multi-agent system is suggested. In the first stage simple expert agents are created. Then they are used in mean-variance framework in order to form more complex output fusion agents. We employ cost sensitive set of perceptrons to find the most successful fusion agents. Experiments on large scale real world data confirm the effectiveness of the new approach. ","SELECTED":null}
{"Title":"An empirical study of collaboration methods for CEP based on algorithmic trading","keywords (cleaned)":"Data stream management systems;Algorithmic trading;Data handling;computer interface;Volume-weighted averages;Institutional investors;volume weight average price;Information systems;Data stream management systems;commercial phenomena;Managers;Empirical studies;Information management;architecture;Complex events;Complex event processing;Semantics;Commerce;Algorithmic trading;Complex event processing;Algorithms;Database systems;Data processing;Investments;article;Data processing techniques;Semantics;computer language;empiricism","Abstract":"Algorithmic trading has become more popular with large institutional investors these days. Complex event processing is a typical data processing technique which becomes the new spotlight of researches. In order to obtain semantic data, in this paper, we investigate the definition, detection, and management of events in the architecture of complex event processing based on algorithmic trading. Especially we propose a corresponding event model and develop an algorithm that can efficiently detect complex event over event stream. ","SELECTED":null}
{"Title":"A formal model for investment strategies to enable automated stock portfolio management","keywords (cleaned)":"Executable specifications;Financial data processing;Stock investment;Decision supports;Planning;Decision support systems;Automation;Algorithmic trading system;Automated portfolio management;Investment strategy model;Portfolio managements;Management information systems;Investment strategy;Model components;Investments;Investment decisions;Investment decision support;Information systems;Portfolio managements;Strategic planning;Decision making","Abstract":"In this paper, we develop a formal model to specify a stock investment strategy. Based on an extensive review of investment literature, we identify determinants for portfolio performance - such as risk attitude, rebalancing interval or number of portfolio positions - and formalize them as model components. With this model, we aim to bridge the gap between pure decision support and algorithmic trading systems by enabling the implementation of investment approaches into an executable specification which forms the foundation of an automated portfolio management system. Such a system helps researchers and practitioners to specify, test, compare and execute investment approaches with strong automation support. To ensure the technical applicability of our model, we implement a prototype and use it to show the effectiveness of the model components on portfolio performance by running several investment scenarios.","SELECTED":null}
{"Title":"Holonic Intelligent Multi-Agent Algorithmic Trading System (HIMAATS)","keywords (cleaned)":"Algorithmic trading system;Holonic systems;Multiagent systems;Multi-asset trading;Multi-strategy trading","Abstract":"In the last few years, the financial industry has witnessed a growing demand for an integrated multi-asset, multi-strategy trading system that allows traders to simultaneously trade different types of assets, and provides real-time risk assessments, status, and performance of the diversified portfolio. However, many factors contribute to the complexity of developing such integrated decision-making systems. Some of these factors include: the inherent diversity of financial assets, the heterogeneity of trading and risk assessment strategies, and the highly dynamic nature of financial markets. Moreover, the large volume of data to be analyzed severely affects the system's ability to make timely decisions, especially, for high-frequency trading. This paper proposes a novel Holonic Intelligent Multi-Agent Algorithmic Trading System (HIMAATS) to address the software functional requirements (multi-asset, multi-strategy, real-time risk assessment, etc.), and non-functional requirements (autonomy, high-throughput, low-latency, modularity, scalability, etc.). ISCA Copyright ","SELECTED":null}
{"Title":"Revisiting agent-based models of algorithmic trading strategies","keywords (cleaned)":"Estimation methods;Simulation;Market simulation;Bayesian adaptive agents;Adaptive agents;Algorithmic trading;Computational methods;Economic and social effects;Correlation theory;Agent-based model;Statistical tests;backtesting;Algorithms;Execution strategies;Frequency estimation;Decision making","Abstract":"Algorithmic trading (AT) strategies aim at executing large orders discretely, in order to minimize the order\u2019s impact, whilst also hiding the traders\u2019 intentions. The contribution of this paper is twofold. First we presented a method for identifying the most suitable market simulation type, based on the specific market model to be investigated. Then we proposed an extended model of the Bayesian execution strategy. We implemented and assessed this model using our tool AlTraSimBa (ALgorithmic TRAding SIMulation BAcktesting) against the standard Bayesian execution strategy and na\u00a8\u0131ve execution strategies, for momentum, random and noise markets, as well as against historical data. Our results suggest that: (i) momentum market is the most suitable model for testing AT strategies, since it quickly fills the Limit Order book and produces results comparable to those of a liquid stock; (ii) the priors estimation method proposed in this paper \u2212 within the Bayesian adaptive agent model \u2212 can be advantageous in relatively stable markets, when trading patterns in consecutive days are strongly correlated, and (iii) there exists a trade-off between the frequency of decision making and more complex decision criteria, on one side, and the negative outcome of lost trading on the agents\u2019 side due to them not participating actively in the market for some of the execution steps. ","SELECTED":null}
{"Title":"A trend tracking strategy for gold future: An artificial neutral network analysis","keywords (cleaned)":"Profitability;trend tracking;High frequency data;Costs;Technical analysis;Algorithmic trading;Commerce;Artificial neutral network;Observed data;Algorithmic trading;Price trends;Neural networks;Trading cost;Gold;Tracking strategies;Neural networks;Technical analysis;gold future","Abstract":"In this paper, we construct a simple data-driven trend tracking strategy for gold future in a view of contrarians. The artificial neutral network (ANN) is adopted to determine the price trend signal, by which the degree of tightness could be adjusted based on observed data. We attempt to capture the small profits when the price is deviated from the Bollinger band in the gold future market by intraday trading. High frequency data of gold future is used to train and test the strategy. Despite of the trading cost, the back-tests show that our strategy has delivered positive returns and is adaptive to different price trends. Finally, we evaluate the profitability with the consideration of trading cost, revealing that the strategy is applicable in practice. ","SELECTED":null}
{"Title":"Combining piecewise linear regression and a granular computing framework for financial time series classification","keywords (cleaned)":"Time series classifications;Sequential pattern mining;Evolutionary optimizations;Time series classifications;Financial data processing;Time series;Benchmarking;Artificial intelligence;Algorithmic trading;Evolutionary optimizations;Finance;Optimization;Piecewise linear techniques;Regression analysis;Algorithms;Granular computing;Sequential-pattern mining;Computation theory;Piecewise regression;Linear Piecewise Regression","Abstract":"Finance is a very broad field where the uncertainty plays a central role and every financial operator have to deal with it. In this paper we propose a new method for a trend prediction on financial time series combining a Linear Piecewise Regression with a granular computing framework. A set of parameters control the behavior of the whole system, thus making their fine tuning a critical optimization task. To this aim in this paper we employ an evolutionary optimization algorithm to tackle this crucial phase. We tested our system on both synthetic benchmarking data and on real financial time series. Our tests show very good classification results on benchmarking data. Results on real data, although not completely satisfactory, are encouraging, suggesting further developments.","SELECTED":null}
{"Title":"Algorithmic trading behavior identification using reward learning method","keywords (cleaned)":"Gaussian noise (electronic);Gaussian Processes;Electronic data interchange;Behavioral finance;Algorithmic trading;Algorithmic trading;Virtual reality;Support vector machines;High-frequency trading;Correlation theory;Inverse reinforcement learning;Markov decision process;Inverse reinforcement learning;Gaussian Processes;Financial markets;Behavioral finance;Commerce;Markov processes;Algorithms;Reinforcement learning;Gaussian distribution;Markov decision process;Support vector machines;High frequency trading","Abstract":"Identifying and understanding the impact of algorithmic trading on financial markets has become a critical issue for market operators and regulators. Advanced data feed and audit trail information from market operators now make the full observation of market participants' actions possible. A key question is the extent to which it is possible to understand and characterize the behavior of individual participants from observations of trading actions. In this paper, we consider the basic problems of categorizing and recognizing traders (or, equivalently, trading algorithms) on the basis observed limit orders. Our approach, which is based on inverse reinforcement learning (IRL), is to model trading decisions as a Markov decision process and then use observations of an optimal decision policy to find the reward function. The approach strikes a balance between two desirable features in that it captures key empirical properties of order book dynamics and yet remains computationally tractable. Making use of a real-world data set from the E-Mini futures contract, we compare two principal IRL variants, linear IRL and Gaussian process IRL. Results suggest that IRL-based feature spaces support accurate classification and meaningful clustering. ","SELECTED":null}
{"Title":"Risk-averse reinforcement learning for algorithmic trading","keywords (cleaned)":"Algorithmic trading","Abstract":"We propose a general framework of risk-averse reinforcement learning for algorithmic trading. Our approach is tested in an experiment based on 1.5 years of millisecond time-scale limit order data from NASDAQ, which contain the data around the 2010 flash crash. The results show that our algorithm outperforms the risk-neutral reinforcement learning algorithm by 1) keeping the trading cost at a substantially low level at the spot when the flash crash happened, and 2) significantly reducing the risk over the whole test period. ","SELECTED":null}
{"Title":"High frequency trading an analysis regarding volatility and liquidity starting from a base case of algorithms and a dedicated software architecture","keywords (cleaned)":"High-frequency trading","Abstract":"High-frequency algorithmic trading has had significant success in recent years due to technological advances and innovations around trading activities. The technique involves the use of algorithms to acquire, process and react to market information at high speed. This paper introduces a reference architecture for performing such trading and dissects a example transaction. We show the life-cycle of the connections between the client and the trading platform; the structure and dynamics of the data processed by the algorithm, and the response of the trading platform. The paper helps understand the components involved in this process and acts as a basic use-case that allows the reader to develop further more sophisticated financial applications. ","SELECTED":null}
{"Title":"Quality and consistency assurance of quote data for algorithmic trading strategies","keywords (cleaned)":"Algorithmic trading","Abstract":"Quote data are a vital part of information almost every trading algorithm relies on. While increasing effort is put into making trading algorithms better, the quote data as foundation of the algorithms is often asserted to be perfect. In reality, much work is needed to acquire good quote data. This paper shows methods of collecting and storing quote data and how to measure and improve its quality and completeness. Applying these methods leads to greatly improved results and higher speed of algorithms which rely on this quote data. ","SELECTED":null}
{"Title":"The effect of probability and uncertainty models on hedge fund performance analysis","keywords (cleaned)":"Hedge fund performance analysis;Probability and uncertainty models","Abstract":"This paper implements two types of framework to investigate the outperformance, selectivity, and market timing skills in hedge funds: uncertainty and probability. Using the uncertainty framework, the paper develops an uncertain fuzzy credibility regression model in the form of a linear and quadratic CAPM in order to estimate these performance skills. Using the probability framework the paper implements frequentist and Bayesian CAPMs (linear and quadratic) to estimate the same performance skills. We consider a data set of monthly investment style indices published by Hedge Fund Research group. The data set extends from January 1995 to June 2010. We divide this sample period into four overlapping sub-sample periods that contain different market trends. Using the probability framework, our results show that bounded rationality triggers inefficiencies in the market that fund managers can utilise to outperform the market. This market outperformance is due to selectivity and market timing skill during periods of economic recovery only. We admit that these results contradict the rational expectations model. However, with the uncertainty framework this effect disappears on behalf of the rational expectations model and the efficient market hypothesis. This disappearance may be a result of the increased amount of high frequency trading witnessed recently that has made market inefficiencies, which are the main source of hedge fund performance, rarer.","SELECTED":null}
{"Title":"Fast, low-memory algorithm for construction of nanosecond level snapshots of financial markets","keywords (cleaned)":"Parallel architectures;Parallel processing systems;Memory footprint;New approaches;Parallel computing;Small files;Performance tuning;Financial markets;Solid-state storage;Virtual storage;Nanosecond resolution;Finance;Algorithms;High-frequency trading","Abstract":"We present a fast, low-memory algorithm for constructing an order-by-order level snapshot of financial markets with nanosecond resolution. This new implementation is 20-30x faster than an earlier version of the code. In addition, since message data are retained only for as long as it they are needed, the memory footprint is greatly reduced. We find that even the heaviest days of trading spanning the NASDAQ, NYSE and BATS exchanges can now easily be handled using compute nodes with very modest memory (~ 4 GB). A tradeoff of this new approach is that the ability to efficiently manage large numbers of small files is more critical. We demonstrate how we can accommodate these new I\/O requirements using the solid-state storage devices (SSDs) on SDSC's Gordon system. Copyright 2014 ACM.","SELECTED":null}
{"Title":"Steps towards a high-frequency financial decision support system to pricing options on currency futures with neural networks","keywords (cleaned)":"Design;Economics;High frequency data;Decision support systems;Commerce;Artificial intelligence;Trading systems;Neural networks;Financial decision support systems;High-frequency data;Financial decision support systems;Costs;Option pricing;FDSS;Design science","Abstract":"In this paper, we present steps towards a model-driven financial decision support system (FDSS) to pricing options on currency futures, which can be embedded in a high-frequency trading process. Due to the difficulty of option valuation, we provide an alternative heuristic option pricing approach with neural networks. We show that the use of neural networks is not only suitable in generating accurate trading signals, but also in generating automated fast run-time trading signals for the decision taker. To achieve this, we conduct an experiment with an empirical tick data set of EUR\/USD options on currency futures of four weeks. An essential advantage of our approach is the simultaneous pricing across different strike prices and parsimonious use of input variables. Nevertheless, we also have to take particular limitations into account, which give us useful hints for further research and steps. Copyright ","SELECTED":null}
{"Title":"Accelerating option risk analytics in R using GPUs","keywords (cleaned)":"R;Option models;Calibration;Commerce;Efficient implementation;Economic analysis;Risk assessment;GPGPU computing;Performance requirements;Financial option modeling;Computer architecture;Loading;C++ (programming language);Programming environment;Program processors;Sequential implementation;Heston stochastic volatility","Abstract":"Broadly, a major prerequisite for analytics applications is robustness to modeling idiosyncrasies. As a result, there is a demand for comprehensive model exploration and validation in high level statistical programming environments such as R. Many financial applications require on-demand processing, which in turn requires fast modeling and calibration computations. In this paper we describe our work on speeding up the calibration of a Heston stochastic volatility model, a financial application, on GPUs. The Heston volatility model is used extensively across the capital markets to price and measure the market risk of exchange traded financial options. However, a typical R based implementation of the Heston model calibration on a CPU does not meet the performance requirements for sub-minute level trading, i.e. mid to high frequency trading. The calibration of a Heston model is performed over M option data points which remains fixed during the calibration computation. The most computation intensive part of this computation is the ErrorFunction() which estimates the error between market observed and model option prices. We have implemented the ErrorFunction() using a Map-Reduce design pattern leading to efficient implementation on various architectures including GPUs. In this paper, we describe the implementation of a GPU optimized kernel for this computation that can be called by the R script performing the calibration process. For M = 1024 we demonstrate a factor of 760x improvement in the overall calibration time over the R sequential implementation by off-loading ErrorFunction() on a system with an Intel Core i5 processor and NVIDIA Tesla K20c (Kepler architecture) consisting of 2496 cores. Note that not all the performance gain is due to the GPU- partly it is due to the reduction in the overhead of R for the Heston model calculation. For comparison we also implemented the calibration code using C\/C + +. We observed a speed up of 230x for the GPU based implementation over the C\/C + + indicating that a factor of 3.4x improvement is due to avoiding the R overhead for the Heston model calculation. However, the overall calibration time using R based optimization routines combined with the GPU off-loaded ErrorFunction() is comparable to a C\/C + + GPU based calibration code.","SELECTED":null}
{"Title":"FINPAGE: Generating high performance feed-specific parser circuits","keywords (cleaned)":"Area-Efficient;Dedicated hardware;Hardware platform;Hardware structures;High throughput;High-frequency trading;Low latency;Parser generators;Data processing;Hardware","Abstract":"The low latency and high throughput requirements of high-frequency trading has resulted in increasing adoption of dedicated hardware for processing financial feeds. Development of hardware platforms, however, is plagued with slow design\/verification cycles compared to their software counterparts. In this work, we present FINPAGE, a FINancial PArser GEnerator, to automatically generate hardware structures for parsing financial feeds. Given a high-level feed format description, FINPAGE generates an area-efficient hardware parser capable of processing feeds at line rate. ","SELECTED":null}
{"Title":"High-frequency trading with type-2 fuzzy logic time series forecasting and hilbert transforms","keywords (cleaned)":"implementation|market cycles|r|fuzzy logic|trading system|High-frequency trading|gate|Fuzzy Logic|type-2 fuzzy|high-frequency trading|hilbert transform|forecasting|time series|trading|trading strategies|market|complex variable|strategies|Hilbert transform|Type-2 Fuzzy|High-frequency|trading systems|forecasting problems|market trends|high-frequency","Abstract":"This chapter proposes a hybrid high-frequency trading application based on a Type-2 Fuzzy Logic approach combined with Hilbert transforms for the detection of periodicities in high-frequency FX data. Most trading systems have concentrated on using common analytic waveforms and have not taken into consideration phasors and complex variables. The Hilbert transform, often employed to convert signal variables in analytic waveforms to complex variables, will be used for the measurement of market cycles, rather than measurement of market trends. The main objective is to investigate how the periods of the dominant cycle in a cycle mode can be quantified in order to create an amplitude compensation of the In-Phase and Quadrature components, alongside methods of smoothing and de-trending the analytic signal. The subsequent output can be defuzzified using Type 2 Fuzzy-Logic techniques, which will in turn refine the implementation of trading strategies. Type-2 Fuzzy Logic systems are becoming more popular as they account for uncertainties that are not captured within a Type-1 Fuzzy Logic system. These Type-2 systems outperform traditional Fuzzy Logic systems in forecasting problems since they can account for more variables in a more efficient way. ","SELECTED":null}
{"Title":"Trading races","keywords (cleaned)":"HFT|news|computers|r|hardware|data stream|Trading|colocation|speed|high-frequency trading|Stock|European|trading|algorithmic trading|market|data streams|algorithm|European Parliament|ace|trade|Colocation|fast|high-frequency","Abstract":"Stock traders are pushing IT to its limits in their obsessive pursuit of high-value market advantage. Firms pay a premium to have their own high-speed computers sitting as close as possible to the trading venues in colocation facilities to minimize the communications time putting those using the standard consolidated feed, which first combines the many different data streams provided by exchanges before the updates are relayed, at a disadvantage. Colocation and fast parallel processing hardware lets the HFT (high-frequency trading) systems act rapidly on any incoming news of trades. Across the Atlantic, MEPs in the European Parliament voted for a bill that could curb the more abusive activities of algorithmic trading. The measure restricts the speed with which orders can be withdrawn from the market, they would have to remain on the system for more than 100ms.","SELECTED":null}
{"Title":"An integrated multi-asset, multi-strategy algorithmic trading system","keywords (cleaned)":"Algorithmic trading system;Holonic systems;Multiagent systems","Abstract":"In the last few years, the financial industry has witnessed a growing demand for an integrated multi-asset, multi-strategy trading system that allows traders to simultaneously trade different types of assets, and provides real-time risk assessments, status, and performance of the diversified portfolio. However, many factors contribute to the complexity of developing such integrated decision-making systems. Some of these factors include: the inherent diversity of financial assets, the heterogeneity of trading and risk assessment strategies, and the highly dynamic nature of financial markets. Moreover, the large volume of data to be analyzed severely affects the system's ability to make timely decisions, especially, for high-frequency trading. This paper proposes a novel Holonic Intelligent Multi-Agent Algorithmic Trading System (HIMAATS) to address the software functional requirements (multi-asset, multi-strategy, real-time risk assessment, etc.), and non-functional requirements (autonomy, high-throughput, low-latency, modularity, scalability, etc.).","SELECTED":null}
{"Title":"Architectural considerations for multi-asset, multi-strategy algorithmic trading systems","keywords (cleaned)":"Algorithmic trading system;Architecture paradigms;Holonic systems","Abstract":"Algorithmic trading systems are data intensive software systems that employ precise set of rules and mathematical models to analyze the financial data\/news, generate buy\/sell signals when trading opportunities are discovered, decide on quantities, and finally, submit orders to one or more exchanges. The development of a multi-asset, multi-strategy algorithmic trading system is a complex and multidimensional problem. Therefore, it is imperative that careful decisions are made regarding the best suited system architecture that meets the software functional requirements (multi-asset, multi-strategy, real-time risk assessment, etc.), and non-functional requirements (autonomy, high-throughput, low-latency, modularity, scalability, etc.). This paper discusses architecture paradigms that can serve as candidates for consideration to design a multi-asset, multi-strategy algorithmic trading system. The paper highlights the pros and cons of each one of these architecture paradigms and provides a body of argument that favours the holonic paradigm as the most suitable one.","SELECTED":null}
{"Title":"Prediction based - High Frequency Trading on Financial Time Series","keywords (cleaned)":"Financial data processing;Feedforward neural networks;Commerce;Artificial intelligence;Nonlinear estimator;Forecasting;Prediction-based;Financial time series;Ornstein-Uhlenbeck process;Feedforward neural networks (FFNN);Financial time series predictions;Feedforward neural networks;Financial time series predictions;Trading strategies;High-frequency trading;High frequency trading","Abstract":"In this paper we investigate prediction based trading on financial time series assuming general AR(J) models and mean reverting portfolios. A suitable nonlinear estimator is used for predicting the future values of a financial time series will be provided by a properly trained FeedForward Neural Network (FFNN) which can capture the characteristics of the conditional expected value. In this way, one can implement a simple trading strategy based on the predicted future value of an asset price or a portfolio and comparing it to the current value. The method is tested on FOREX data series and achieved a considerable profit on the mid price. In the presence of the bid-ask spread, the gain is smaller but it still ranges in the interval 2-6 percent in 6 months without using any leverage. FFNNs were also used to predict future values of mean reverting portfolios after identifying them as Ornstein-Uhlenbeck processes. In this way, one can provide fast predictions which can give rise to high frequency trading on intraday data series.","SELECTED":null}
{"Title":"Asymmetric effect of market liquidity demand shocks on price shocks: Empirical studies based on the CSI 300 Index and the Futures","keywords (cleaned)":"HFT;Liquidity demand shocks;Price shocks","Abstract":"Purpose \u2013 With the launch of CSI 300 Index Futures trading on April 16, 2010, China's stock market presents a more diversified trend, such as arbitrage, trends strategy entering the market rapidly. Therefore, the liquidity demand also presents a higher frequency, and the change is more complex than the original situation. In recent years, many literatures are engaged in high-frequency trading (HFT) related research, and an important concern is the impact of HFT on market volatility and liquidity. Is it playing the role of stabilizing the market, or bringing more noise and turmoil? Based on this, the purpose of this study is trying to study what kind of impact the HFT have on market liquidity before and after the launch of the CSI 300 Index Futures. Design\/methodology\/approach \u2013 The paper uses the simultaneous equations model of price and net order flow proposed by Deuskar and Johnson and for the first time introduces an asymmetric identification through heteroskedasticity (ITH) method. The paper applies the method to the high-frequency data of CSI 300 Index and the Futures and classifies the buying and selling orders through volume clock. The price risks are decomposed into a component driven by the impact of liquidity demand shocks (flow-driven risks (FDRs)) and a component driven by external information (information-driven risks (IDRs)). Findings \u2013 The empirical results show that the flow-driven risk of CSI 300 Index Futures is about 20 percent. In addition, before the introduction of the Index Futures, there is no asymmetric effect between liquidity demand shocks and price shocks existing in either CSI 300 Index or CSI 300 Index Futures. While after the introduction of stock Index Futures, the asymmetric effect in the both two markets emerges. The impact of the buying net order flows on the price is less than the impact of the selling net order flows on CSI 300 Index, whereas the impact of the buying net order flows on the price is larger than the impact of the selling net order flows on CSI 300 Index Futures. The paper further analyzes the relationship between liquidity and FDR and gets the conclusion that the reasons for the deterioration of the liquidity level are caused by the impact of the external information shocks, rather than the liquidity demand shocks. And entries of HFTs like arbitrage traders and hedge traders play a positive role in improving the liquidity level in the market. Originality\/value \u2013 The paper introduces an asymmetric ITH method for the first time and finds asymmetric effect of the net order flow on the return in both CSI 300 Index market and the corresponding Index Futures market. ","SELECTED":null}
{"Title":"Advanced Algorithmic Forecasting Techniques","keywords (cleaned)":"Best execution;Day of week effect;Forecasting;Forecasting daily volumes;Forecasting monthly volumes;Liquidity risk;Market impact models;Parameter estimation error;Short-term risk model","Abstract":"Chapter 7 introduces readers to advanced algorithmic forecasting techniques. We provide insight into different market impact modeling formulations (such as in terms of various different trading strategies) in order to be able to generate actionable information sets in a timely manner. These different model structures are compared in order to understand where the approaches work well and where there may be inherent limitations in the approach. The chapter continues with a discussion of the various sources of algorithmic trading risk with special emphasis on liquidity risk. We provide techniques to forecast the different liquidity terms, e.g., daily volumes and monthly ADV. These forecasting advancements provide portfolio managers with essential data to improve portfolio management in times of stressed market conditions (e.g., financial crisis, debt ceiling issues, quant trading, etc.), as well as in times when stocks fall out of favor. The chapter concludes with a discussion of how to develop short-term volume forecasting models. Further, we provide advanced risk management components. ","SELECTED":null}
{"Title":"High Frequency Trading and Black Box Models","keywords (cleaned)":"backtesting;Black box models;High frequency trading (HFT);Index and ETF arbitrage;Liquidity trading;Market-neutral arbitrage;Merger (risk) arbitrage;Statistical analysis;Statistical arbitrage (Stat Arb);Triangular arbitrage","Abstract":"In this chapter, Ayub Hanif provides readers with an overview of the high frequency trading environment. We discuss the necessary mathematical knowledge and skills required to be successful in the high frequency trading environment. The chapter discuss the data needs and research, and provides readers with a description of many of the high frequency strategies such as statistical arbitrage, triangular arbitrage, liquidity trading, market-neutral arbitrage, index and ETF arbitrage, and merger (risk) arbitrage. The chapter continues with discussion of the evaluation and back-testing techniques to critique the model, and an overview of the statistical metrics used by financial professionals. ","SELECTED":null}
{"Title":"Impatient traders' strategies and price formation in order driven markets","keywords (cleaned)":"Order-driven market;Price formation;Commerce;Negative exponential;Trading strategies;Information risk;Informed trader;Costs;Patient trader;High-frequency trading","Abstract":"In this paper, we analyze the effects of patient traders behaviors on price formation and describe the evolution of price formation based on trading strategies of patient traders and information risk. We find that the number of patient traders has negative exponential influence on the spread and testify the result by selecting the high-frequency trading data of 817 securities as our sample. We also testify the process of price formation. Besides, one formula about expect value of asset in EKOP model is corrected.","SELECTED":null}
{"Title":"Computing defines centers","keywords (cleaned)":"Data centers;Hardware solutions;High-frequency trading;Networking devices;Performance requirements;Processing environments;Stock exchange;Storage servers;Data processing;Digital storage;Internet service providers;Hardware","Abstract":"Many different industries and operations require data centers, and each has its own needs, which leads to lots of variability. Data centers that support Internet service providers are significantly different from those serving high frequency trading stock exchanges. A data center design begins with software, then the hardware chassis to support the software the cabinets or racks, followed by the data center infrastructure to support the cabinets or rack space, power, and cooling needs. ASHRAE's Thermal Guidelines for Data Processing Environments apply to all types of electronic equipment and all types of data centers. Data centers are full of a combination of compute-servers, storage servers, and networking devices in varying proportions. Some data centers also have specialized devices for graphic processing and security. The highest performance requirements lead to the installation of customized hardware solutions.","SELECTED":null}
{"Title":"An application of the method of moments to range-based volatility estimation using daily high, low, opening, and closing (HLOC) prices","keywords (cleaned)":"Algorithmic trading;daily high, low;method of moments;opening and closing prices;range of arithmetic Brownian motion;Range-based volatility estimation","Abstract":"We use the expectation of the range of an arithmetic Brownian motion and the method of moments on the daily high, low, opening, and closing prices to estimate the volatility of the stock price. This novel theoretical approach results in an estimator that is genuinely range-based on daily opening, high, low and closing data, unlike current estimators in the literature. The daily price jump at the opening is considered to be the result of the unobserved evolution of an after-hours virtual trading day. In comparison to an existing drift-independent estimator, we find that our estimator is actually more efficient when using a smaller number of data points, while for a larger number of points the efficiency of our estimator stays above 99% of the existing one. A toy example that uses this method to take advantage of mispricing opportunities in the options market illustrates potential applications of this method to algorithmic trading. ","SELECTED":null}
{"Title":"A semiparametric forecasting model for volatility of stock index futures and its MCS test","keywords (cleaned)":"MCS test;Realized volatility;Semiparametric forecasting model;Stock index futures","Abstract":"Stock index futures plays an important role in the process of price discovery and risk prevention of capital market. The prediction of its return volatility is significantly important to achieve the risk aversion function of stock index futures. A semiparametric forecasting model based on the linear nonnegative autoregressive model is proposed to forecast the realized volatility of stock index futures, and the asymptotic properties of estimation method for this model are analyzed. In addition, taking 5 min high-frequency trading data of CSI300 index futures as example, the out-of-sample daily volatility predictions calculated by using rolling predicting method, and a bootstrap MCS test is used to evaluate the predicting accuracy for the proposed model and other 7 models. The empirical results show that, under various robust loss functions, the proposed model is the best model for volatility predictions of stock index futures among the 8 models.","SELECTED":null}
{"Title":"Financial industry ontologies for risk and regulation data (FIORD) - A position paper","keywords (cleaned)":"Risks;Electronic trading;Collaborative networks;Semantic Web;Commerce;Ontology;Big data;Ontology;Risk management;High frequency trading;Semantic Web;Finance;Virtual corporation;Reasoning;Risks;Big data;Financial industry;Regulation;High-frequency trading;Collaborative networks","Abstract":"This paper presents a proposed approach to address risk and regulation management within the highly active and volatile financial domain by employing semantic based technologies within a collaborative networks environment. Firstly the problems and motivation are introduced, with accent on big data and high frequency trading issues that are creating major problems to the current software systems. Secondly the state of the art on Big Data, Regulation and Risk Management are presented. Next the FIORD platform architecture is detailed and the envisioned approach explained. Finally conclusions are presented where benefits for real time monitoring are emphasized so high frequency trading irregularities are detected in real time for the benefit of involved financial institutions. ","SELECTED":null}
{"Title":"Agent-based models simulations for high frequency trading","keywords (cleaned)":"Agent-based model;Double auction;Financial markets;High frequency trading;Zero-intelligence traders","Abstract":"High frequency computer-based trading (HFT) represents a challenging topic nowadays, mainly due to the controversy it creates among investors on the financial market. The hereto paper compares two types of agent-based models, one with zero-intelligence traders and the other with intelligent traders in order to simulate the tick-by-tick high frequency trades on the stock market for the selected U.S. stocks. The simulations of the agent-based models are done with the help of Adaptive Modeler software application which uses the interaction of 2,000 heterogeneous agents to create a virtual stock market for the selected stock with the scope of forecasting the price. Within the intelligent agent-based model the population of agents is continuously adapting and evolving by using genetic programming in forming new agents by using the trading strategies of the best performing agents and replacing the worst performing agents in a process called breeding, while the zero-intelligence agent based model does not evolve, agents do not breed, and they trade in a random manner. After comparing the fitting of the two models with the real data, the results show that in almost all the cases the intelligent agent-based model performed better when compared to the zero-intelligence agent-based model, which could be interpreted as lower market efficiency, allowing for predictions of the stock market price, or even stock market manipulation. Also, the zero-intelligent agent-based model generates more trades and lower wealth for the population, compared to the intelligent agentbased model. The high-frequency data turns out to be very hard to simulate and analyse due to its particularities which differentiate them from daily data, as price changes are discrete, being multiples of the minimum price increment, the price changes not being independent.","SELECTED":null}
{"Title":"Designing loss-aware fitness function for ga-based algorithmic trading","keywords (cleaned)":"foreign exchange|arches|r|research|Stock Exchange|trading rules|management|Machine learning techniques|Design|Stock|trading|algorithmic trading|market|arch|design|machine learning|Algorithm|Machine learning|trading rule|algorithm|trading agent|stock|learning techniques|fitness functions|market trends|profit|forex|Genetic Algorithm","Abstract":"In these days, an algorithmic trading in stock or foreign exchange (henceforth forex) market is in fashion, and needs for automatically performing stable asset management are growing. Machine learning techniques are increasingly used to construct trading rules of the algorithmic trading, as researches on the algorithmic trading advance. Our study aims to build an automatic trading agent, and in this paper, we concentrate in designing a module which determines trading rules by machine learning. We use Genetic Algorithm (henceforth GA), and we build trading rules by learning parameters of technical indices. Our contribution in this paper is that we propose new fitness functions in GA, in order to make them robuster to change of market trends. Although profits were used as a fitness function in the previous study, we propose the fitness functions which pay more attention to not making a loss than to gaining profits. As a result of our experiment using real TSE(Tokyo Stock Exchange) data for eight years, the proposed method has outperformed the previous method in terms of gained profits. ","SELECTED":null}
{"Title":"Study of CEP based on dynamic data management system and algorithmic trading","keywords (cleaned)":"Algorithmic trading;Complex event detection;Complex event processing;Dynamic data management system;volume weight average price","Abstract":"In recent years, dynamic data management systems and algorithmic trading systems have come to account for a majority of volume traded at the major US, European and Asia-Pacific financial markets. Complex event processing is a typical data processing technique which becomes the new spotlight of researches. Complex Event Processing over dynamic data management systems and algorithmic trading systems poses huge challenges with regard to efficient, scalable execution as well as expressive models and languages that account for the dynamics in long-running queries. In this paper we discuss the characteristics that a data event processing service should have in order to support in the best way the complex event pattern detection functionality, and present an assessment of a number of technologies that can be used to dynamic data. Especially we propose a corresponding event model and develop an algorithm that can efficiently detect complex event over event stream. ","SELECTED":null}
{"Title":"Providing soft real-time capabilities to business applications","keywords (cleaned)":"Content filter;QoS (quality of service);Distributed applications;Quick response;Real-time application;QoS parameters;QoS;Business decisions;Eventing;Data distribution services;Publish\/subscribe;Middleware;Quality of service;Financial information;DDS core;Publish\/subscribe;Business applications;Soft real time;Information systems;Behavioral research;Publish\/subscribe middlewares;Automated trading","Abstract":"Modern companies must be able to react in a timely way to changes in production and financial information since quick responses provide advantages against competition. As a matter of example, in automated trading systems a delay of 1 ms may be worth $1M. In addition, nowadays business decisions are made on the basis of increasing volumes of information, complicating the adoption of decisions on time. These requirements are common to those found in other real-time application domains. Consequently, adapting certain tools used in such domains may be a solution for business applications too. This paper draws several scenarios where middleware based solutions are justified, with particular insight on real-time behavior and control of QoS (Quality of Service) needs. More specifically, we provide certain guidelines to use OMG DDS (Data Distribution Service) as communications backbone in soft real-time business applications. DDS is a recent publish\/subscribe middleware specification which provides mechanisms aimed at easing the creation of complex distributed applications such as those allowing the definition of content filters or tuning QoS parameters. Unfortunately, DDS is a complex technology so the programmers of business applications may benefit from certain guidelines that help them in the identification of the topics as well as the values for the QoS parameters provided by DDS. ","SELECTED":null}
{"Title":"High frequency information based portfolio decision for downside risk-return tradeoff using differential evolution algorithm","keywords (cleaned)":"Portfolio decisions;Differential evolution algorithms;Realized semivariance;Portfolio decisions;Performance measure;High frequency information;Differential evolution algorithms;High frequency;Risk perception;Semivariances;Profitability","Abstract":"A performance measure of intraday investment is constructed using high frequency trading data, which takes the trade-off between return and downside risk into account. The principle of the measure consists in partitioning the return on each intraday interval into loss and gain according to a user-specified target. Both the excess return and profit-making opportunity above the target constitute the return term of the new measure, while the intraday price movement below the target is assumed as the proxy of risk. The paper proceeds to demonstrate that the new measure has implication in portfolio decision where the weights could be estimated using the differential evolution algorithm and the results on a market test are promising. 1548-7741\/Copyright ","SELECTED":null}
{"Title":"Description of a straightforward strategy to invest: An experiment in the Spanish stock market","keywords (cleaned)":"Algorithmic trading;arbitrage;Efficient market hypothesis;IBEX 35;Madrid Stock Exchange","Abstract":"In this paper we reintroduce the topic of arbitrage, explaining an easy method to obtain reasonable results for every possible behavior in the stock market. It is based on purchases at low prices and sales at higher prices, and it is closely related to volatility. Obviously, our technique has its limitations, but we study the hypotheses needed to assure a good performance. Our aim is to contribute to the better understanding of the topic by the researchers, giving an idea of the way how algorithmic strategies work in practice. Finally, we exemplify and test the method through the use of a real data set from some shares of Madrid Stock Exchange, for the time period 1994-2011, and express some concluding remarks. ","SELECTED":null}
{"Title":"A scalable architecture for maintaining packet latency measurements","keywords (cleaned)":"Network Monitoring;bloom filter;latency;High-performance computing;Scalable architectures;Packet latencies;Computer software selection and evaluation;Algorithmic trading;Computer systems;Network architecture;Sub-populations;Data structures;Computer architecture;Data structures;Network operator;bloom filter","Abstract":"Latency has become an important metric for network monitoring since the emergence of new latency-sensitive applications (e.g., algorithmic trading and high-performance computing). In this paper, to provide latency measurements at both finer (e.g., packet) as well as flexible (e.g., flow subsets) levels of granularity, we propose an architecture called MAPLE that essentially stores packet-level latencies in routers and allows network operators to query the latency of arbitrary traffic sub-populations. MAPLE is built using a scalable data structure called SVBF with small storage needs. ","SELECTED":null}
{"Title":"Detecting Chinese stock information based on hidden Markov model","keywords (cleaned)":"Shanghai Stock Index;Probability;Chinese stock market;Bayesian inference;Information state;MCMC sampling;Commerce;Information effect clustering;High frequency;Transition probabilities;Information strength;Unobservable;Finance;Hidden Markov models;Inference engines;Bayesian networks;Temporal dimensions;Hidden Markov models","Abstract":"The unobservable state of stock information was modeled based on Hidden Markov Model, and a transition probabilities matrix of information state was built to describe the dynamic association properties in temporal dimension. Based on 5-minutes high frequency trading data and using Bayesian inference and MCMC sampling, the information state and strength of Shanghai Stock Index and sample stocks of SSE 50 in August 2010 were estimated. Empirical results prove the model has effective ability to identify information, and show that the information effects have characteristics of aggregation in Chinese stock market. On the base of the estimated transition probabilities matrix of information state, it is surmised that the probability that an item of information was absorbed by the market after 100 minutes is 99% in Chinese stock market.","SELECTED":null}
{"Title":"Cartesian genetic programming for trading: A preliminary investigation","keywords (cleaned)":"Three dimensions;Financial markets;Cartesian genetic programming;Commerce;Traditional approaches;Algorithmic trading;Genetic algorithms;Algorithms;Dow Jones Industrial averages;Recovery factors;Rule learning;Trading strategies;Data mining;Genetic programming","Abstract":"In this paper, a preliminary investigation of Cartesian Genetic Programming (CGP) for algorithmic intraday trading is conducted. CGP is a recent new variant of genetic programming that differs from traditional approaches in a number of ways, including being able to evolve programs with limited size and with multiple outputs. CGP is used to evolve a predictor for intraday price movements, and trading strategies using the evolved predictors are evaluated along three dimensions (return, maximum drawdown and recovery factor) and against four different financial datasets (the Euro\/US dollar exchange rate and the Dow Jones Industrial Average during periods from 2006 and 2010). We show that CGP is capable in many instances of evolving programs that, when used as trading strategies, lead to modest positive returns. ","SELECTED":null}
{"Title":"Impact of risk management features on performance of automated trading system in GRAINS futures segment","keywords (cleaned)":"Commodity;Exit strategy;Commerce;Automated trading systems;Automation;Grain (agricultural product);Futures;Risk management;Grains;Automated trading;Profitability","Abstract":"Automated trading requires protection against loss in the same manner as manual trading does. Automated trading systems (ATS) have to be acceptable, easy to implement and transparent from the user's perspective. From the perspective of a small speculator, this paper is focused on testing ATS's performance while applying selected exit techniques - maximum loss, profit target, trailing stop and their combination. To allow comparison, performance of ATS without application of any exit strategy will be presented as well. The testing will be conducted using simplified trading system on commodities of GRAINS segment (corn, wheat, oats), using real commodity data. Tests will be conducted on optimized version of simplified trading system, using Stochastics technical analysis indicator.","SELECTED":null}
{"Title":"Rank estimation in cointegrated vector auto-regression models via automated Trans-dimensional Markov chain Monte Carlo","keywords (cleaned)":"Algorithmic trading;Auto regression;Bayes factor;Cointegration;hypothesis testing;Identifiability;Markov chain;Markov chain Monte Carlo;matrix;Model probabilities;Non-trivial;Sub-spaces;Synthetic data;Vector components;Vector estimation;Automation;Covariance matrix;Estimation;Factor analysis;Fisher information matrix;Markov processes;Regression analysis;Sensors;Trace analysis;Vectors;Vector spaces","Abstract":"This paper develops a novel automated Transdimensional Markov chain Monte Carlo sampling methodology for Bayesian Cointegrated Vector Auto Regression (CVAR) models. In automating the rank and cointegration vector estimation in CVAR models we solve an important problem in algorithmic trading of cointegrated price series. The automation of both the within model sub-space sampling for the cointegration vectors directions and the between model rank estimation Markov chain proposal is achieved by developing a global matrix-variate proposal centered on the MLE and with covariance given by the observed Fisher Information matrix. To obtain this in the matrix-variate CVAR setting under an error correction formulation (ECM) involved a non-trivial derivation of the observed Fisher information matrix for each model subspaces unconstrained cointegration vector components, conditional on the components of the long run multiplier matrix which are constrained for identifiability. We study synthetic data and futures data on U.S. treasury notes, bonds and US equity indexes. In each analysis, we compare the estimated rank based on the estimated posterior model probabilities for the rank to simple Bayes Factor estimated posterior rank probabilities and the classical hypothesis test of the rank based on the trace statistic of the long-run multiplier matrix. ","SELECTED":null}
{"Title":"DSL programmable engine for High Frequency Trading acceleration","keywords (cleaned)":"DSL;Stock;FIX;Decoder;FPGA;Low latency;Commerce;FAST;Domain specific languages;Field programmable gate arrays (FPGA);trade;High throughput;Structural optimization;Domain specific languages","Abstract":"In High Frequency Trading systems, a large number of orders needs to be processed with minimal latency at very high data rates. We propose an FPGA based accelerator for High Frequency Trading that is able to decrease latency by an order of magnitude and increase the data rate by the same rate compared to software based CPU approaches. In particular, we focus on the acceleration of FAST, the most commonly used protocol for distributing pricing information of stock and options over the network. As FPGAs are hard to program, we present a novel Domain Specific Language that enables our engine to be programmed via software. The code is compiled by our own compiler into binary microcode that is then executed on a microcode engine. In this paper we provide detailed insights into our hardware structure and the optimizations we applied to increase the data rate and the overall processing performance. ","SELECTED":null}
{"Title":"Hybridizing data stream mining and technical indicators in automated trading systems","keywords (cleaned)":"Automated trading;Data mining techniques;Data sets;Data stream mining;Decision procedure;Financial markets;Hybrid techniques;Price movement;Technical indicator;Trading strategies;Automated trading systems;Data stream mining;Decision procedure;Hybrid techniques;Price movement;Stream mining;Technical indicator;Trading strategies;Artificial intelligence;Data mining;Finance;Forecasting;Artificial intelligence;Commerce;Finance;Financial markets;Forecasting;Motion estimation;Commerce;Data mining","Abstract":"Automated trading systems for financial markets can use data mining techniques for future price movement prediction. However, classifier accuracy is only one important component in such a system: the other is a decision procedure utilizing the prediction in order to be long, short or out of the market. In this paper, we investigate the use of technical indicators as a means of deciding when to trade in the direction of a classifier's prediction. We compare this \"hybrid\" technical\/data stream mining-based system with a naive system that always trades in the direction of predicted price movement. We are able to show via evaluations across five financial market datasets that our novel hybrid technique frequently outperforms the naive system. To strengthen our conclusions, we also include in our evaluation several \"simple\" trading strategies without any data mining component that provide a much stronger baseline for comparison than traditional buy-and-hold or sell-and-hold strategies. ","SELECTED":null}
{"Title":"8th International Conference on Modeling Decisions for Artificial Intelligence, MDAI 2011","keywords (cleaned)":"Spectrum Analysis|Decision Making|Technical Indicator|Linear Programming|r|Data Stream|Value-at-Risk|Automated Trading System|Technical Indicators|Trading|Stream Mining|Artificial Intelligence|Trading System|Automated Trading|cost-Sensitive|R|Risk|Clustering|arch|Data Mining|Regularization|classification|Trading Systems|Portfolio|Framework|Data Privacy|edi|Automated Trading Systems|fast|valuation|Mining|Data Stream Mining","Abstract":"The proceedings contain 23 papers. The special focus in this conference is on Modeling Decisions for Artificial Intelligence. The topics include: Semi-supervised Agglomerative Hierarchical Clustering with Ward Method Using Clusterwise Tolerance; agglomerative Clustering Using Asymmetric Similarities; on Hard c-Means Using Quadratic Penalty-Vector Regularization for Uncertain Data; grey Synthetic Clustering Method for DoS Attack Effectiveness Evaluation; fuzzy-Possibilistic Product Partition: A Novel Robust Approach to c-Means Clustering; a Novel and Effective Approach to Shape Analysis: Nonparametric Representation, De-noising and Change-Point Detection, Based on Singular-Spectrum Analysis; A SSA-Based New Framework Allowing for Smoothing and Automatic Change-Points Detection in the Fuzzy Closed Contours of 2D Fuzzy Objects; possibilistic Linear Programming Using General Necessity Measures Preserves the Linearity; an Efficient Hybrid Approach to Correcting Errors in Short Reads; cost-Sensitive Learning; rule Protection for Indirect Discrimination Prevention in Data Mining; a Comparison of Two Different Types of Online Social Network from a Data Privacy Perspective; on the Declassification of Confidential Documents; uncovering Community Structure in Social Networks by Clique Correlation; evolving Graph Structures for Drug Discovery; fuzzy Measures and Comonotonicity on Multisets; a Parallel Fusion Method for Heterogeneous Multi-sensor Transportation Data; a Dynamic Value-at-Risk Portfolio Model; modelling Heterogeneity among Experts in Multi-criteria Group Decision Making Problems; fast Mining of Non-derivable Episode Rules in Complex Sequences; hybridizing Data Stream Mining and Technical Indicators in Automated Trading Systems.","SELECTED":null}
{"Title":"Assessing and optimizing microarchitectural performance of event processing systems","keywords (cleaned)":"Performance;Tuning;Benchmarking;Aggregation operation;Algorithmic trading;Complex event processing;Memory consumption;Production monitoring;Data handling;Random access storage;Computer architecture;Micro architectures;Complex event processing;Critical applications;Supply chain management","Abstract":"Event Processing (EP) systems are being progressively used in business critical applications in domains such as algorithmic trading, supply chain management, production monitoring, or fraud detection. To deal with high throughput and low response time requirements, these EP systems mainly use the CPU-RAM sub-system for data processing. However, as we show here, collected statistics on CPU usage or on CPU-RAM communication reveal that available systems are poorly optimized and grossly waste resources. In this paper we quantify some of these inefficiencies and propose cache-aware algorithms and changes on internal data structures to overcome them. We test the before and after system both at the microarchitecture and application level and show that: i) the changes improve microarchitecture metrics such as clocks-per-instruction, cache misses or TLB misses; ii) and that some of these improvements result in very high application level improvements such as a 44% improvement on stream-to-table joins with 6-fold reduction on memory consumption, and order-of-magnitude increase on throughput for moving aggregation operations. ","SELECTED":null}
{"Title":"Algorithmic trading strategy optimization based on mutual information entropy based clustering","keywords (cleaned)":"Algorithmic trading;Mutual informations;Optimization models;Strategy optimization;Transaction data;Cluster analysis;Clustering algorithms;Entropy;Lakes;Mathematical models;Optimization;Profitability;Commerce","Abstract":"Algorithmic trading strategies are automated defining a sequence of instructions executed by a computer. A good strategy should be profitable which includes identification of what to trade and how to trade. In this paper, we focus on the study of algorithmic trading strategy optimization and propose a strategy optimization model based on an initialized strategy pool. In order to get a better strategy, a mutual information entropy based clustering algorithm is employed to analyze the correlations among the stocks and a reward and punishment scheme is also set up for updating the latest transaction data in the strategy optimization process. Experimental results on several different groups of stocks showed that in most cases, this optimization model can find a profitable strategy swiftly. ","SELECTED":null}
{"Title":"Reassembling multilingual temporal news datasets with incomplete information","keywords (cleaned)":"Research;Near-duplicate detection;Investments;Trading conditions;Near duplicate detection;Data sets;news;Commerce;Algorithmic trading;Large datasets;Numerical information;Information technology;Incomplete information;News articles;Time stamps","Abstract":"Institutional investors are building increasingly more sophisticated algorithmic trading engines that account for textual as well as numerical information. To train these engines they need large datasets of information with highly accurate timestamps that cover long periods with differing trading conditions. Thus, the demand for temporal news datasets beyond the point where full archives are available is increasing. Rebuilding the actual temporal news dataset that was transmitted to the market relies on merging multiple datasets, each with incomplete information and sometimes questionable quality. Doing so requires near duplicate detection in a very large dataset including news in many languages. This research is novel as in our scenario we are unaware of the language used in any given news article. In this paper we describe a language independent near duplicate detection algorithm and demonstrate its performance on a dataset consisting of tens of millions of news messages in over 20 languages consisting of hundreds of gigabytes of content. ","SELECTED":null}
{"Title":"Extreme databases: The biggest and fastest","keywords (cleaned)":"Algorithmic trading;Application development;Common problems;Digital mammography;Disk memory;Financial services;Global Enterprises;Main memory;Pennsylvania;Petabytes;Vice president;Database systems;Industry;Systems analysis;Flash memory","Abstract":"Work is being conducted in the field of extreme databases that are expected to the biggest and fastest solution for any enterprise. Robert Hollebeek, professor of physics at the University of Pennsylvania, served as technical lead on the National Digital Mammography Archive (NDMA), a system designed to include a database growing by 28 petabytes per year. NDMA had to deal with issues related to siloed data stored on systems that were geographically distributed-a common problem among global enterprises. In an another development, Carl Olofson, IDC research vice president, Application Development and Deployment, highlights that fastest databases have direct access to a piece of information, in memory. Another example of an application that requires extreme speed is algorithmic trading driven by portfolios in the financial services industry. A promising solution is solid-state drives (SSDs) or flash memory, which now is a sandwiched tier between main memory and disk memory.","SELECTED":null}
{"Title":"Building an electronic market system","keywords (cleaned)":"Automated trading;Electronic market;Human agent;Prototype electronics;Trading agent;Electronic commerce;Industrial engineering;Intelligent systems;Intelligent virtual agents;Virtual reality;World Wide Web;Intelligent agents","Abstract":"An electronic market system is predicated on three technologies: data mining, intelligent trading agents and virtual institutions in which informed trading agents can trade securely both with each other and with human agents in a natural way. This paper describes a demonstrable prototype electronic market that integrates these three technologies and is available on the World Wide Web. This is part of a larger project that aims to make informed automated trading a reality. ","SELECTED":null}
{"Title":"Stochastic multi criteria decision analytics and artificial intelligence in continuous automated trading for wealth maximization","keywords (cleaned)":"Cognitive science;Cognitive decision makings;Fundamental characteristics;Multicriteria decision;artificial neural network;Stochastic systems;Commerce;Automation;Neural networks;Radial basis function networks;Elasticity coefficients;Optimization;Automated trading systems;Radial basis function artificial neural networks;Algorithms;Automated trading;Stochastic models","Abstract":"Recent technological and regulatory advances have coalesced to usher in an era where both automated and algorithmic trading routinely characterize a wealth maximization process managed by the continuous trading of equity securities. Under this approach to wealth maximization, automated trading focuses on the process determining directional trades for individual securities based upon the receipt and interpretation of new data. This paper presents a stochastic price formation algorithm that implements a cognitive decision making system modeled by twin radial basis function artificial neural networks to produce a high frequency automated trading system for individual equity securities listed on U.S. exchanges. The overall effectiveness and efficiency of the automated trading system is calibrated by estimating non-parametric quasi elasticity coefficients for individual firm fundamental characteristics. We find that automation driven by cognitive science can effectively auto-trade securities and produce changes to individual wealth that equals or exceeds the performance generated by a simple buy-and-hold strategy. We also identify four fundamental firm factors that explain the ability of the automated trading algorithm to produce a measured level of percent-positive trades. ","SELECTED":null}
{"Title":"9th IFIP WG 6.1 Conference on e-Business, e-Services and e-Society, I3E 2009","keywords (cleaned)":"equity trading|Services|implementation|r|ethics|hardware|e-market|interoperability|models|order routing|auctions|compliance|web|auction|mining|simulation|services|European|trading|algorithmic trading|industry|market|engines|applications|text mining|intelligent agents|liquidity|algorithm|technology|edi|ace|web services","Abstract":"The proceedings contain 33 papers. The special focus in this conference is on E-Business, E-Services and E-Society. The topics include: Electronic voting using identity domain separation and hardware security modules; towards user acceptance of biometric technology in e-government; biometrically based encryption; a model for value-added e-marketplace provisioning; networked virtual organizations; a rule-based approach of creating and executing mashups; implementing a rule-based contract compliance checker; modeling medical ethics through intelligent agents; facilitating business to government interaction using a citizen-centric web 2.0 model; not all adware is badware; proposal and implementation of SSH client system using ajax smart order routing technology in the new European equity trading landscape; algorithmic trading engines and liquidity contribution; anonymous, yet trustworthy auctions; realizing mobile web services for dynamic applications; modifying the balanced scorecard for a network industry; dynamic component selection for SCA applications; transforming collaborative process models into interface process models by applying an MDA approach; the persuasiveness of web-based alcohol interventions; enterprise networks for competences exchange; analyzing strategic business rules through simulation modeling; towards e-society policy interoperability; integrating the European securities settlement; task delegation based access control models for workflow systems; data refining for text mining process in aviation safety data and a 360 vision for virtual organizations characterization and modelling.","SELECTED":null}
{"Title":"The probability of informed trading based on VAR model","keywords (cleaned)":"Asymmetric information;Probability;High frequency HF;Stock exchange;Commerce;Trading volumes;Intraday pattern;Paper research;Probability of informed trading;Announcement;Informed trader;Spread;Value engineering","Abstract":"The paper researches the representative variable of the probability of informed trading, selecting CCER high-frequency trading data of Shanghai Stock Exchange from 2003.7.1 to 2003.12.31, adopting VAR model. Different from previous studies, the paper firstly accounts for the dynamic relationship between trade and price. Then, the content of information in trading volume, duration and trading direction are considered in our model. Finally, it gets the probability of informed trading and analyzes this variable. The results show: the probability of informed trading is about 0.172713; the more asymmetric information is, the larger spread is; the probability of informed trading is the well-known U-shape; it is the biggest before the announcement. ","SELECTED":null}
{"Title":"The liquidity analysis of Chinese a-share market based on the panel data","keywords (cleaned)":"Panel data analysis;Cost benefit analysis;Panel data;Liquidity;Market impact costs;Commerce;Panel data;Market impact costs;Costs;Determinants;High-frequency trading","Abstract":"We construct a market impact cost proxy which reflects market breadth and market depth to measure the liquidity in Chinese A-share market, and based on this proxy, we use high frequency trading data and panel data analysis method to study the relationship between the determinants and the market impact cost.","SELECTED":null}
{"Title":"Research on stock liquidity based on trade size, order imbalance in shanghai A-share market","keywords (cleaned)":"Buy and sell order;Trading datums;Liquidity;Order imbalance;Turbulent flow;Commerce;Trade size;Empirical results;Share markets;Sales;High frequency","Abstract":"This paper considers method to measure investors buy and sell order imbalance and investigates the relationship among trade size, order imbalance and stock liquidity based on high frequency trading data of Shanghai A-share market in 2006. The empirical result shows that there exists significant positive relationship between trade size and stock liquidity for small trade size of mid and small-cap enterprise and for big trade size of big-cap enterprise. And for all level cap stocks, through controlling the trade size, the stock order imbalance based on the ratio of buy minus sell to total volume and the order imbalance of big trade size stock based on tick data are positive to stock liquidity significantly, the investors' preference on buy order results in the higher liquidity. There is no great relationship between order imbalance and trade size, the contemporaneous trade size does not cause the order imbalance changing, and vice versa.","SELECTED":null}
{"Title":"Automating contract negotiation","keywords (cleaned)":"Negotiations;Problem solving;Automatic programming;Electronic commerce;Intelligent agents;Information flows;Argumentation;Automated trading;Data mining;Real time systems","Abstract":"The automation of contract negotiation requires intelligent agents that can assimilate and use real-time information flows wisely. Electronic markets are information-rich with access to the Internet and the World Wide Web. A new breed of \"information-based\" agents are founded on concepts from information theory, and are designed to operate with information flows of varying and questionable integrity. These agents are part of a larger project that aims to make informed automated trading in applications such as eProcurement a reality. ","SELECTED":null}
{"Title":"Real trading volume and price action in the foreign exchange markets","keywords (cleaned)":"foreign exchange|Forex|information|r|transaction cost|technical indicators|foreign exchange markets|exchange markets|market participants|trading volume|high-frequency trading|R|trading|market|costs|liquidity|competition|stock|transaction costs|foreign exchange market|currency markets|trading platform|technical indicator|market trends|trade|high-frequency","Abstract":"Among market practitioners, it is a generally accepted fact that the volume traded is closely tied to important turning points in market trends. This largely explains the numerous technical indicators that rely on both volume and price data. The foreign exchange market is characterized by the liquidity it offers on large trades, the 24-hour access it provides to participants, the great number of traded currencies and the absence of a predetermined contract size. Leverage is readily accessible for the investor\/hedger with margin requirements. Contrary to stocks, transaction costs are very small. Large operators are able to reduce brokerage fees and therefore incur mainly liquidity costs. Both the low transaction cost feature and the high liquidity of Forex allow for high-frequency trading, which would not be possible for other markets. Electronic broking systems are multilateral trading platforms that market participants use to trade between themselves. These systems record all the transactions for a large panel of market participants. They are one of the most accurate sources of data for the transactions occurring in the foreign exchange markets. One of the main objectives of electronic broking system (EBS) was to provide effective competition to the system provided by Reuters. Since then EBS has obtained a large share of the spot FX broking market. It is now considered the world's leading electronic foreign exchange Broker. The EBS screen displays all the necessary trading information that a spot trader requires to trade efficiently in the currency markets. ","SELECTED":null}
{"Title":"Building relationships and negotiating agreements in a network of agents","keywords (cleaned)":"Contracts;Data mining;Identification (control systems);Real time systems;Virtual reality;Data mining systems;Information flows;Supplier brokering;Software agents","Abstract":"Fully automated trading, such as e-procurement, is virtually unheard of today. Trading involves the maintenance of effective business relationships, and is the complete process of: need identification, product brokering, supplier brokering, offer-exchange, contract negotiation, and contract execution. Three core technologies are needed to fully automate the trading process. First, real-time data mining technology to tap information flows and to deliver timely information at the right granularity. Second, intelligent agents that are designed to operate in tandem with the real-time information flows from the data mining systems. Third, virtual institutions in which informed trading agents can trade securely both with each other and with human agents in a natural way. This discussion focusses on the second technology the design of \"information driven\" trading agents. The \"information-based agency\" is being developed principally by John Debenham [UTS, Australia] and Carles Sierra [IIIA, Spain]. Published work to date has considered argumentation frameworks between a set of (often just two) agents, and has not taken into account the relationships that the various agents have with other agents. Drawing ideas from the business research literature we are constructing a model of multi-agent interaction that accommodates the sorts of inter-agent agreements found in the business world. The working hypothesis is that the information-based approach will prove to be a suitable conceptual framework through which networks of agents, interacting under the constraints of various styles of business relationship, may be implemented. ","SELECTED":null}
{"Title":"The trading test","keywords (cleaned)":"Computer software;Information technology;Personnel testing;Strategic planning;Financial industry;Trading systems;Personnel selection","Abstract":"A trading contest was organized by Interactive Brokers Group to find tech-savvy engineers and scientist willing to work for the financial industry. The financial industry has been hiring math whizzes as quantitative analysts or quants who devise pricing models, probe new ways of quantifying risks and mine data. As automated trading systems take over more substantive work, many firms are seeking quants who not only know math but information technology as well. In addition to the $400,000 prize fund, there are marketing costs, including ads on Web sites. Participants are required to elaborate trading strategy and write software to execute it. Brian Eckerly finished first and took home $100,000.","SELECTED":null}
{"Title":"IFIP 19th World Computer Congress, TC 12: IFIP AI 2006 Stream","keywords (cleaned)":"selection algorithm|coding|support vector|Information|news|sales|information|tracking|r|Evolutionary Computation|repair|genetic algorithms|semantic web|engineering|distance|support vector machine|automated trading|Planning|algorithms|Knowledge|Integration|Management|interpolation|Information Management|management|web|genetic algorithm|neural network|statistics|R|neural networks|services|planning|ann|pin|trading|ontology|arch|Data Mining|Acquisition|classification|Applications|artificial intelligence|multi-objective|framework|algorithm|expert systems|training|edi|wire|Expert Systems|ace|exchange rate|feature selection|web services|knowledge|negotiation|valuation|Mining","Abstract":"The proceedings contain 51 papers. The special focus in this conference is on Knowledge and Information Management, Integration of AI with other Technologies, Neural Nets, Knowledge Acquisition and Data Mining, Evolutionary Computation, Speech and Natural Language, Industrial Applications of AI, Machine Vision, Expert Systems and Planning and Scheduling. The topics include: artificial intelligence and knowledge management; formal analysis of the communication of probabilistic knowledge; detecting and repairing anomalous evolutions in noisy environments; adding semantic web services matching and discovery support to the MoviLog platform; learning browsing patterns for context-aware recommendation; applying collaborative filtering to reputation domain; combine vector quantization and support vector machine for imbalanced datasets; ontology support for translating negotiation primitives; statistical method of context evaluation for biological sequence similarity; biological inspired algorithm for storage area networks; radial basis functions versus geostatistics in spatial interpolations; neural networks applied to wireless communications; anomaly detection using prior knowledge; neural plasma; comparison of SVM and some older classification algorithms in text classification tasks; an automatic graph layout procedure to visualize correlated data; knowledge perspectives in data grids; on the class distribution labelling step sensitivity of co-training; two new feature selection algorithms with rough sets theory; global convexity in the bi-criteria traveling salesman problem; evolutionary algorithm for state encoding; hypercube frame work for ACO applied to timetabling; multitree-multiobjective multicast routing for traffic engineering; road segment identification in natural language text; learning discourse-new references in Portuguese texts; analysing definition questions by two machine leaming approaches; fuzzy rule-based hand gesture recognition; comparison of distance measures for historical spelling variants; patterns in temporal series of meteorological variables using SOM and TDIDT; applying genetic algorithms to convoy scheduling; a GRASP algorithm to solve the problem of dependent tasks scheduling in different machines; tactical field force planning in BT; an agent solution to flexible planning and scheduling of passenger trips; facial expression recognition using shape and texture information; limited receptive area neural classifier for texture recognition of metal surfaces; a tracking framework for accurate face localization; a combination of spatiotemporal ICA and Euclidean features for face recognition; three technologies for automated trading; conceptualization maturity metrics for expert systems; toward developing a tele-diagnosis system on fish disease; effective prover for minimal inconsistency logic; identification of important news for exchange rate modeling; autonomous search and rescue rotorcraft mission stochastic planning with generic DBNs and solving multi-objective scheduling problems-an integrated systems approach.","SELECTED":null}
{"Title":"Agents for information-rich environments","keywords (cleaned)":"Administrative data processing;Electronic commerce;Information dissemination;Information services;Portals;Real time systems;Automated trading;Questionable integrities;Real-time informations;Intelligent agents","Abstract":"Information-rich environments, such as electronic markets, or even more generally the World Wide Web, require agents that can assimilate and use real-time information flows wisely. A new breed of \"information-based\" agents aim to meet this requirement. They are founded on concepts from information theory, and are designed to operate with information flows of varying and questionable integrity. These agents are part of a larger project that aims to make informed automated trading, in applications such as eProcurernent, a reality. ","SELECTED":null}
{"Title":"Making informed automated trading a reality","keywords (cleaned)":"Automation;Data mining;Intelligent agents;World Wide Web;Automated trading;Intelligent trading agents;Virtual institutions;Electronic commerce","Abstract":"Three core technologies are needed to fully automate the trading process: data mining, intelligent trading agents and virtual institutions in which informed trading agents can trade securely both with each other and with human agents in a natural way. This paper describes a demonstrable prototype e-trading system that integrates these three technologies and is available on the World Wide Web. This is part of a larger project that aims to make informed automated trading a reality. ","SELECTED":null}
{"Title":"Information, prices, and sensemaking in financial futures trading","keywords (cleaned)":"Schemas;Futures trading;Electronic trading;Economic consequences;Futures contract;Investment bank;Organizational sensemaking;Organizational practices;Trading floors;Futures market;Organizational setting;Physical locations;Costs;Participant observations;Floors;Investments;Sensemaking;Electronic commerce;Decentralized networks;Prices;Futures market;Automated trading","Abstract":"The interaction of organizational setting with cognitive schemas provides the content for analyzing futures prices as outcome of organizational sensemaking instead of a priori inputs of markets. It defines the process by which information is transformed into an ontologically stable category of price. Reconstructing this sensemaking is a crucial task, since the settlement of prices for futures markets has significant organizational and economic consequences. And the recent emergence of electronic trading as an alternative to traditional ways of trading makes this issue more vivid. Futures exchanges have historically relied on a trading technology known as \"open outcry,\" where traders gather in a central physical location to buy and sell futures contracts on a range of physical and financial commodities. The organizational practices of open outcry produces prices deeply embedded in the interactions among these traders. Since the early 1990s, the technology of futures trading has been challenged by the introduction of electronic trading platforms. Automated trading systems shift the point of exchange from a centralized physical location to a decentralized network of terminals, located in investment banks and smaller boutique trading firms. With the elimination of the exchange trading floor and transformation of trading technology, the schemas and organizational settings that supported the creation of prices in open outcry have required renegotiation. This paper uses participant observation and interview data gathered on the trading floors of open outcry and electronic exchanges to investigate how sensemaking leads to the production of futures market prices.","SELECTED":null}
{"Title":"Performance analysis of a counter-intuitive automated stock-trading agent","keywords (cleaned)":"Automated trading;Market simulation;Performance analysis;Stock market;Technical analysis;Trading agent;Trading strategies;Artificial intelligence;Electronic commerce;Automation","Abstract":"Autonomous trading in stock markets is an area of great interest in both academic and commercial circles. A lot of trading strategies have been proposed and practiced from the perspectives of Artificial Intelligence, market making, external data indication, technical analysis, etc. This paper examines some properties of a counter-intuitive automated stock-trading strategy in the context of the Penn-Lehman Automated Trading (PLAT) simulator [1], which is a real-time, real-data market simulator. While it might seem natural to buy when the market is on the rise and sell when it is on the decline, our strategy does exactly the opposite. As a result, we call it the reverse strategy. The reverse strategy was the winner strategy in the first and second PLAT live competitions. In this paper, we analyze the performance of the reverse strategy. Also, we suggest ways to control the risk of using the reverse strategy in certain kinds of markets.","SELECTED":null}
{"Title":"Automated trading in augments markets for communication bandwidth","keywords (cleaned)":"Electronic market;Marketing;Decision theory;Societies and institutions;Communication bandwidth;Statistical decision theory;Electronic commerce;Statistical methods;Electronic data submissions;Data processing;Bandwidth;Automated trading;Internet auctions;Data communication systems","Abstract":"Automated agents are increasingly being used by organisations and individuals trading in electronic markets. Agents are particularly useful in markets where trade might not have been possible otherwise, for example because a lot of information must be processed quickly, or because employing human traders in 24-hour, small transactions markets is not cost-effective. Markets for communication bandwidth, where organisations trade the rights to transmit data over a network, are one such application. Because demand fluctuates considerably every few seconds agent-based spot markets provide extra liquidity. This paper considers the design of agents which automatically trade in a k-double auction market for communication bandwidth. We suggest criteria and a general framework for building adaptive agents based on ideas from statistical decision theory. In particular our agents are designed to differentiate stable from unstable market conditions and to best-respond to these changes.","SELECTED":null}
{"Title":"ACCESS system speeds trading of energy futures","keywords (cleaned)":"Computer workstations;Crude petroleum;Data communication equipment;Fault tolerant computer systems;Performance;Petroleum industry;Program processors;Standards;System program documentation;Systems analysis;Systems engineering;Telecommunication services;Supply and demand;Data communication systems","Abstract":"The NYMEX ACCESS System (American Computerized Commodity Exchange Systems and Services), a fully automated trading system, was developed to meet exacting requirements of global trading in the volatile petroleum market. By keeping traders in touch with world petroleum markets around the clock, ACCESS facilitates performance, availability and manageability in commodities trading. Being fully automated, traders could enter bid and asked prices on petroleum contracts from their workstations. ACCESS complies with stringent regulations for security, equal access to trades, reliability performance, and trader training certification. It has trader monitoring, auditing, and risk-management capabilities. Traders had equal access to any trade within 800 milliseconds. Most importantly, the system can be available end-to-end all the time.","SELECTED":null}